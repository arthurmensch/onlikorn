\documentclass[a4paper, 10pt]{article}
\input{preamble.tex}

\addbibresource{biblio.bib}

\begin{document}

\section{An online expectation minimization algorithm}

Define $\mu = \alpha \exp(f)$, $\nu = \beta \exp(g)$, $x = (\mu, \nu)$. We will change variables without warning in the following.
Define the Bregman divergence
\begin{align}
    D_{\alpha}(\mu, \mu_0) &= \dotp{\alpha}{\exp(f_0 - f) - 1 - (f_0 - f)} \\
    D_{\beta}(\nu, \nu_0) &= \dotp{\beta}{\exp(g_0 - g) - 1 - (g_0 - g)} \\
    D_{\alpha, \beta}(x, x_0) &= D_{\alpha}(\mu, \mu_0) + D_{\beta}(\nu, \nu_0)
\end{align}
We want to solve the objective
\begin{equation}
    \min_x \Ff(x) \triangleq \KL(\alpha, \mu) + \KL(\beta, \nu) + \dotp{\mu \otimes \nu}{\exp(-C)}
\end{equation}
Define the prox objective
\begin{align}
    \Ll(x, x_t) &= 2 F(x_t) + \dotp{\nabla \Ff(x_t)}{x - x_t} + D_{\alpha,\beta}(x, x_t) \\
    &= \EE_{\hat \alpha \sim \alpha, \hat \beta \sim \alpha}
    \Big[2 F(x_t) + \dotp{\nabla \Ff(x_t)}{x - x_t} + D_{\hat \alpha,\hat \beta}(x, x_t) \Big]
\end{align}
The Sinkhorn iterations then rewrites as
\begin{equation}
    x_{t+1} = \argmin_x \EE_{\hat \alpha, \hat \beta} \Ll_{\hat \alpha, \hat \beta}(x, x_t)
\end{equation}
and online Sinkhorn

\begin{equation}
    x_{t+1} = (1 - \eta_t) x_t + \eta_t \argmin_x \Ll_{\hat \alpha_t, \hat \beta_t}(x, x_t)
\end{equation}
Probably useless ?

\section{Variable mirror descent point of view}

Consider the objective
\begin{equation}
    \max_{f, g} \Ff(f, g) = \dotp{\alpha}{f} + \dotp{g, \beta} -
     \dotp{\alpha \otimes \beta}{\exp(f \oplus g - C)} + 1
\end{equation}
The gradient reads
\begin{equation}
    \nabla \Ff(f, g) = \Big(\alpha\big(1 - \exp(f - T_\beta(g))\big), \beta\big(1 - \exp(g - T_\alpha(f))\big)\Big)
     \in \Mm^+(\Xx^2)
\end{equation}
Using the local Bregman divergence
\begin{equation}
    \omega_t(f, g) = \dotp{\alpha}{\exp(f_t - f)} + \dotp{\beta}{\exp(g_t - g)},
\end{equation}
online Sinkhorn iterations rewrites as
\begin{equation}
    \nabla \omega_t(f_{t+1}, g_{t+1}) = \nabla \omega_t(f_t, g_t) + \eta_t \tilde \nabla \Ff(f_t, g_t),
\end{equation}
where 
\begin{equation}
    \tilde \nabla \Ff(f, g) = \Big(\hat \alpha_t \big(1 - \exp(f - T_\beta(g))\big), 
    \hat \beta_t \big(1 - \exp(g - T_\alpha(f))\big)\Big)
    \in \Mm^+(\Xx^2)
\end{equation}
is an unbiased estimate of $\nabla \Ff(f, g)$.
\printbibliography

\end{document}
