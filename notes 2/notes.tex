\documentclass[a4paper, 10pt]{article}
\input{preamble.tex}

\addbibresource{biblio.bib}
\addbibresource{SA.bib}

\begin{document}

\section{An online expectation minimization algorithm}

Define $\mu = \alpha \exp(f)$, $\nu = \beta \exp(g)$, $x = (\mu, \nu)$. We will change variables without warning in the following.
Define the Bregman divergence
\begin{align}
    D_{\alpha}(\mu, \mu_0) &= \dotp{\alpha}{\exp(f_0 - f) - 1 - (f_0 - f)} \\
    D_{\beta}(\nu, \nu_0) &= \dotp{\beta}{\exp(g_0 - g) - 1 - (g_0 - g)} \\
    D_{\alpha, \beta}(x, x_0) &= D_{\alpha}(\mu, \mu_0) + D_{\beta}(\nu, \nu_0)
\end{align}
We want to solve the objective
\begin{equation}
    \min_x \Ff(x) \triangleq \KL(\alpha, \mu) + \KL(\beta, \nu) + \dotp{\mu \otimes \nu}{\exp(-C)} - 1
\end{equation}
Define the prox objective
\begin{align}
    \Ll(x, x_t) &= 2 \Ff(x_t) + \dotp{\nabla \Ff(x_t)}{x - x_t} + D_{\alpha,\beta}(x, x_t) \\
    &= \EE_{\hat \alpha \sim \alpha, \hat \beta \sim \alpha}
    \Big[2 F(x_t) + \dotp{\nabla \Ff(x_t)}{x - x_t} + D_{\hat \alpha,\hat \beta}(x, x_t) \Big]
\end{align}
The Sinkhorn iterations then rewrites as
\begin{equation}
    x_{t+1} = \argmin_x \EE_{\hat \alpha, \hat \beta} \Ll_{\hat \alpha, \hat \beta}(x, x_t)
\end{equation}
and online Sinkhorn

\begin{equation}
    x_{t+1} = (1 - \eta_t) x_t + \eta_t \argmin_x \Ll_{\hat \alpha_t, \hat \beta_t}(x, x_t)
\end{equation}
Probably useless ?

\section{Variable mirror descent point of view}

Consider the objective
\begin{equation}
    \max_{f, g} \Ff(f, g) = \dotp{\alpha}{f} + \dotp{g, \beta} -
     \dotp{\alpha \otimes \beta}{\exp(f \oplus g - C)} + 1
\end{equation}
The gradient reads
\begin{equation}
    \nabla \Ff(f, g) = \Big(\alpha\big(1 - \exp(f - T_\beta(g))\big), \beta\big(1 - \exp(g - T_\alpha(f))\big)\Big)
     \in \Mm^+(\Xx^2)
\end{equation}
Using the local Bregman divergence
\begin{equation}
    \omega_t(f, g) = \dotp{\alpha}{\exp(f_t - f)} + \dotp{\beta}{\exp(g_t - g)},
\end{equation}
online Sinkhorn iterations rewrites as
\begin{equation}
    \nabla \omega_t(f_{t+1}, g_{t+1}) = \nabla \omega_t(f_t, g_t) + \eta_t \tilde \nabla \Ff(f_t, g_t),
\end{equation}
where 
\begin{equation}
    \tilde \nabla \Ff(f, g) = \Big(\hat \alpha_t \big(1 - \exp(f - T_\beta(g))\big), 
    \hat \beta_t \big(1 - \exp(g - T_\alpha(f))\big)\Big)
    \in \Mm^+(\Xx^2)
\end{equation}
is an unbiased estimate of $\nabla \Ff(f, g)$.

\section{An EM point of view}
The simultaneous Sinkhorn updates can be rewritten as
\begin{align}
    f_t, g_t = \argmax_{f, g} Q_t^\star((f, g), (f_t, g_t)) 
    &\triangleq \EE_{Y \sim \beta} \left[ \EE_{X \sim \alpha} \left[
     f(X) - e^{f(X) + g_t(Y) - C(X, Y)} \right] \right] \\
     &+
     \EE_{X \sim \alpha} \left[ \EE_{Y \sim \beta} \left[
     g(Y) - e^{f_t(X) + g(Y) - C(X, Y)} \right] \right].
\end{align}
This is similar to the EM algorithm: the first expectation is on data, the second on hidden random variables.
We now define the approximate functions

\begin{align}
    Q_t((f, g), (f_t, g_t)) &= \EE_{Y \sim \hat \beta_t} \left[ \EE_{X \sim \alpha} \left[
        f(X) - e^{f(X) + g_t(Y) - C(X, Y)} \right] \right] \\
        &+
        \EE_{X \sim \hat \alpha_t} \left[ \EE_{Y \sim \beta} \left[
        g(Y) - e^{f_t(X) + g(Y) - C(X, Y)} \right] \right] \\
        &= \EE_{X \sim \alpha} [f(X)] + \EE_{X \sim \alpha} \left[ \sum_{i=n_t}^{n_{t+1}} 
        b_i e^{f(X) + g_t(y_i) - C(X, y_i)} \right] \\
        &+ \EE_{Y \sim \beta} [g(Y)] + \EE_{Y \sim \beta} \left[ \sum_{i=n_t}^{n_{t+1}} 
        a_i e^{g(Y) + f_t(x_i) - C(x_i, Y)} \right]
\end{align}
Running the iterations
\begin{equation}
    f_t, g_t = \argmax_{f,g} Q_t((f, g), (f_t, g_t))
\end{equation}
amounts to set
\begin{equation}
    f_t(\cdot) = - \log \sum_{i=n_t}^{n_{t+1}} b_i e^{g_t(y_i) - C(\cdot, y_i)} \quad
    g_t(\cdot) = - \log \sum_{i=n_t}^{n_{t+1}} a_i e^{f_t(x_i) - C(x_i, \cdot)},
\end{equation}
which is the randomized Sinkhorn algorithm. Setting
\begin{equation}
    \bar Q_t = (1 - \eta_t) \bar Q_{t-1} + \eta_t Q_{t}
\end{equation}
and running the iterations
\begin{equation}
    f_t, g_t = \argmin_{f,g} \bar Q_t((f, g), (f_t, g_t))
\end{equation}
gives online Sinkhorn:
\begin{equation}
    f_t(\cdot) = - \log \sum_{i=1}^{n_{t+1}} e^{q_i - C(\cdot, y_i)} \quad
    g_t(\cdot) = - \log \sum_{i=1}^{n_{t+1}} e^{p_i - C(x_i, \cdot)},
\end{equation}
with the update rule on $q_i, p_i$ as : see paper.
Every function $Q_t$ is parametrized by $(p_i, q_i, x_i, y_i)_{i=(n_t,
 n_{t+1}]}$, and $\bar Q_t$ by $(p_i, q_i, x_i, y_i)_{i=(0, n_{t+1}]}$. Thus the
 parametrization of $f_t, g_t$ is encoded using an argmax trick, and we recover the structure of a stochastic expectation-maximization algorithm (less the probabilistic point of view).

 \section{Stochastic approximation}

 Online EM: in finite dimension: \fullcite{cappe_online_2009}

 Applications + better explanation: \fullcite{dupuy_online_2017}

 Random fixed point iterations: \fullcite{alber_stochastic_2012}

 Non-asymptotic rates for SGD + Polyak-Ruppert averaging: \fullcite{moulines_non-asymptotic_2011}

\subsection{The Robbins Monroe-Algorithm}
Overall, everything can be rewritten as looking for the zero of some function
\begin{equation}
    \text{Find}\:x^\star\:\text{such that}\:h(x) = 0,
\end{equation}
with access to an oracle 
$\hat h(x)$ s.t. $\EE[\hat h(x)] = h(x)$ for all $x \in \Xx$. Then the algorithm
\begin{equation}
    x_{n+1} = x_n - \eta_n h(x_n)
\end{equation}
gives a sequence converging to $x^\star$, provided that
\begin{equation}
    \sum_n \eta_n = \infty,\qquad \sum_n \eta_n^2 \leq \infty,
    \qquad h\text{ non decreasing}\qquad \EE[h(x_n)^2 | \Ff_{n-1} ] \leq \sigma^2
\end{equation}
When looking for $\min f(x)$, we can use $h(x) = \nabla f(x)$. When looking for a fixed point equation
\begin{equation}
    x = T x,
\end{equation}
we may use $h(x) = x - T(x)$, in which case the algorithm writes
\begin{equation}
    x_{n+1} = (1 - \eta_n) x_n + \eta_n S(x_n),
\end{equation}
where $\EE[S(x_n)] = x - T(x)$, which is our case. In a Hilbert space, assuming $T$ is contracting for the norm, i.e.
\begin{equation}
    \Vert Tx - Ty \Vert \leq \kappa \Vert x - y \Vert,
\end{equation}
it is easy to obtain convergence of $\EE[\Vert x_n  - x^\star \Vert^2]$ + rates on the mean-square convergence rate + almost sure convergence of the iterate \autocite{alber_stochastic_2012}.


\subsection{Proof: basic inequality}

Overall, all these references use at some point exhibits a sequence $(\delta_n)_n$ such that
\begin{equation}
    \delta_{n+1} \leq (1 - \eta_n) \delta_n + C \gamma_n
\end{equation}
with $\sum \eta_n = \infty$ and $\sum \gamma_n \leq \infty$. Typically $\gamma_n = \eta_n^2$.

E.g. from SGD, setting $\delta_n = \EE[|| \theta_n - \theta ||^2]$, we have, if objective is $L$-smooth and $\mu$-strongly convex:
\begin{equation}
    \delta_n \leq (1 - 2 \mu \gamma_n   + 2 L^2 \gamma_n^2) \delta_{n-1} + 2 \sigma^2 \gamma_n^2
\end{equation}


\paragraph{Problem.} We do not have access to such an equality:
\begin{itemize}
    \item The contraction of the Sinkhorn operator is for a non-Euclidean distance
    \item Therefore we need to increase the sampling size with time
\end{itemize}

What we have at hand, $e_t \triangleq \EE ||f_t - f^\star||_{var} + ||g_t - g^\star||_{var}$:
\begin{equation}\label{eq:Et_rec}
    0 \leq e_{t+1} \leq 
    (1 - \tilde \eta_t) e_t
    + \tilde \eta_t
    ({\Vert \epsilon_{\hat \beta_t} \Vert}_{\var} + 
    {\Vert \iota_{\hat \alpha_t} \Vert}_{\var}).
\end{equation}
with $\tilde \eta_t = \eta_t (1 - \kappa)$ and
\begin{equation}
    \epsilon_{\hat \beta}(\cdot) \triangleq
    f^\star - T_{\hat \beta}{g^\star} ,\qquad
    \iota_{\hat \alpha}(\cdot) \triangleq 
    g^\star - T_{\hat \alpha}{f^\star},
\end{equation}
With increasing batch-sizes, we may end up with
\begin{equation}
    e_{t+1} \leq (1 - \eta_t) e_t + C \eta_t w_t.           
\end{equation}

\section{Rates for online Sinkhorn}



\section{Unbalanced algorithm}

Fixed point equation (KL divergence, or aprox from Thibault's paper)
\begin{equation}
    f^\star = \left( 1 + \frac{\varepsilon}{\rho} \right)^{-1} T_\beta ( g^\star), \qquad
    g^\star = \left( 1 + \frac{\varepsilon}{\rho} \right)^{-1} T_\alpha ( f^\star)
\end{equation}
In unbiased space, $\lambda \triangleq \left( 1 + \frac{\varepsilon}{\rho} \right)^{-1}$:
\begin{equation}
    u^\star = \exp(-f^\star) = \exp(-\lambda) \exp(-T_\beta ( g^\star)), \qquad
    v^\star = \exp(-g^\star) = \exp(-\lambda) \exp(-T_\alpha ( f^\star))
\end{equation}
Define
\begin{equation}
    T(u, v) \triangleq \left(\exp(-\lambda) \exp(-T_\beta (\log(v))),
                    \exp(-\lambda) \exp(-T_\alpha (-\log(u)) \right)
\end{equation}
fixed point operator. Online Sinkhorn reads
\begin{equation}
    x_n = (u_n, v_n) = (1 - \eta_n) x_{n-1} + \eta_n T_n(x_{n-1}),
\end{equation}
\begin{equation}
    T_n(u, v) \triangleq \left(\exp(-\lambda) \exp(-T_{\hat \beta_n} (\log(v))),
                    \exp(-\lambda) \exp(-T_{\hat \alpha_n} (-\log(u)) \right)
\end{equation}

\printbibliography

\end{document}
