
@inproceedings{yu_scalable_2012,
  title = {Scalable Coordinate Descent Approaches to Parallel Matrix Factorization for Recommender Systems},
  url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=6413853},
  booktitle = {Proceedings of the {{International Conference}} on {{Data Mining}}},
  publisher = {{IEEE}},
  urldate = {2016-01-13},
  date = {2012},
  pages = {765--774},
  author = {Yu, Hsiang-Fu and Hsieh, Cho-Jui and Dhillon, Inderjit},
  file = {/home/arthur/Dropbox/Zotero/Yu et al_2012_Scalable coordinate descent approaches to parallel matrix factorization for.pdf}
}

@inproceedings{szabo_online_2011,
  title = {Online Group-Structured Dictionary Learning},
  url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5995712},
  booktitle = {Proceedings of the {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  publisher = {{IEEE}},
  urldate = {2015-12-28},
  date = {2011},
  pages = {2865--2872},
  author = {Szabó, Zoltán and Póczos, Barnabás and Lorincz, András},
  file = {/home/arthur/Dropbox/Zotero/Szabó et al_2011_Online group-structured dictionary learning.pdf;/home/arthur/Zotero/storage/F4GAJTQ9/abs_all.html}
}

@article{combettes_proximal_2008,
  title = {A Proximal Decomposition Method for Solving Convex Variational Inverse Problems},
  volume = {24},
  url = {http://iopscience.iop.org/0266-5611/24/6/065014},
  number = {6},
  journaltitle = {Inverse problems},
  urldate = {2015-03-20},
  date = {2008},
  pages = {065014},
  author = {Combettes, Patrick L. and Pesquet, Jean-Christophe},
  file = {/home/arthur/Dropbox/Zotero/Combettes_Pesquet_2008_A proximal decomposition method for solving convex variational inverse problems.pdf;/home/arthur/Zotero/storage/4DMVJ97N/Combettes and Pesquet - 2008 - A proximal decomposition method for solving convex.html}
}

@incollection{hinton_transforming_2011,
  title = {Transforming Auto-Encoders},
  url = {http://link.springer.com/chapter/10.1007/978-3-642-21735-7_6},
  booktitle = {Artificial {{Neural Networks}} and {{Machine Learning}}–{{ICANN}} 2011},
  publisher = {{Springer}},
  urldate = {2015-06-21},
  date = {2011},
  pages = {44--51},
  author = {Hinton, Geoffrey E. and Krizhevsky, Alex and Wang, Sida D.},
  file = {/home/arthur/Dropbox/Zotero/Hinton et al_2011_Transforming auto-encoders.pdf;/home/arthur/Zotero/storage/UHZ5DA8K/978-3-642-21735-7_6.html}
}

@inproceedings{varoquaux_multi-subject_2011,
  langid = {english},
  title = {Multi-Subject Dictionary Learning to Segment an Atlas of Brain Spontaneous Activity},
  volume = {22},
  abstract = {Fluctuations in brain on-going activity can be used to reveal its intrinsic functional organization. To mine this information, we give a new hierarchical probabilistic model for brain activity patterns that does not require an experimental design to be specified. We estimate this model in the dictionary learning framework, learning simultaneously latent spatial maps and the corresponding brain activity time-series. Unlike previous dictionary learning frameworks, we introduce an explicit difference between subject-level spatial maps and their corresponding population-level maps, forming an atlas. We give a novel algorithm using convex optimization techniques to solve efficiently this problem with non-smooth penalties well-suited to image denoising. We show on simulated data that it can recover population-level maps as well as subject specificities. On resting-state fMRI data, we extract the first atlas of spontaneous brain activity and show how it defines a subject-specific functional parcellation of the brain in localized regions.},
  booktitle = {Proceedings of the {{Information Processing}} in {{Medical Imaging Conference}}},
  publisher = {{Springer}},
  date = {2011},
  pages = {562-573},
  keywords = {Action Potentials,Algorithms,Brain,Computer Simulation,Humans,Image Enhancement,Image Interpretation; Computer-Assisted,Magnetic Resonance Imaging,Models; Anatomic,Models; Neurological,Pattern Recognition; Automated,Reproducibility of Results,Sensitivity and Specificity},
  author = {Varoquaux, Gaël and Gramfort, Alexandre and Pedregosa, Fabian and Michel, Vincent and Thirion, Bertrand},
  file = {/home/arthur/Zotero/storage/3ISI2BUK/Varoquaux et al. - 2011 - Multi-subject dictionary learning to segment an at.ps},
  eprinttype = {pmid},
  eprint = {21761686}
}

@inproceedings{hazan_beating_2011,
  title = {Beating Sgd: {{Learning}} Svms in Sublinear Time},
  url = {http://scholar.google.fr/scholar_url?url=http%3A%2F%2Fpapers.nips.cc%2Fpaper%2F4359-beating-sgd-learning-svms-in-sublinear-time&hl=en&sa=T&oi=ggp&ct=res&cd=0&ei=Sl9sVZvCDouUqAGs2ICQDw&scisig=AAGBfm2VachMmSZ_pcQidqEMt87LMjSF0Q&nossl=1&ws=1472x801},
  shorttitle = {Beating Sgd},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  urldate = {2015-06-01},
  date = {2011},
  pages = {1233--1241},
  author = {Hazan, Elad and Koren, Tomer and Srebro, Nati},
  file = {/home/arthur/Dropbox/Zotero/Hazan et al_2011_Beating sgd.pdf;/home/arthur/Zotero/storage/2THVDA8G/scholar_url.html;/home/arthur/Zotero/storage/JNPKFDPS/4359-beating-sgd-learning-svms-in-sublinear-time.html}
}

@book{bauschke_convex_2011,
  location = {{New York, NY}},
  title = {Convex {{Analysis}} and {{Monotone Operator Theory}} in {{Hilbert Spaces}}},
  isbn = {978-1-4419-9466-0 978-1-4419-9467-7},
  url = {http://link.springer.com/10.1007/978-1-4419-9467-7},
  series = {CMS Books in Mathematics},
  publisher = {{Springer New York}},
  urldate = {2015-07-02},
  date = {2011},
  author = {Bauschke, Heinz H. and Combettes, Patrick L.},
  file = {/home/arthur/Dropbox/Zotero/Bauschke_Combettes_2011_Convex Analysis and Monotone Operator Theory in Hilbert Spaces.pdf}
}

@article{halko_finding_2009,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {0909.4061},
  primaryClass = {math},
  title = {Finding Structure with Randomness: {{Probabilistic}} Algorithms for Constructing Approximate Matrix Decompositions},
  url = {http://arxiv.org/abs/0909.4061},
  shorttitle = {Finding Structure with Randomness},
  urldate = {2015-05-13},
  date = {2009},
  keywords = {Mathematics - Numerical Analysis,Mathematics - Probability},
  author = {Halko, Nathan and Martinsson, Per-Gunnar and Tropp, Joel A.},
  file = {/home/arthur/Dropbox/Zotero/Halko et al_2009_Finding structure with randomness.pdf;/home/arthur/Zotero/storage/EBSP2KZE/0909.html}
}

@article{mairal_task-driven_2012,
  title = {Task-{{Driven Dictionary Learning}}},
  volume = {34},
  url = {http://scholar.google.fr/scholar_url?url=https%3A%2F%2Fhal.inria.fr%2Finria-00521534%2F&hl=en&sa=T&ct=res&cd=2&ei=SdtcVZmzLqHl0gHDvIDgDQ&scisig=AAGBfm3DY4B9Qd1dWzZEYHoBoremOhI7iA&nossl=1&ws=1920x925},
  number = {4},
  journaltitle = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  urldate = {2015-05-20},
  date = {2012},
  pages = {30},
  author = {Mairal, Julien and Bach, Francis and Ponce, Jean},
  file = {/home/arthur/Dropbox/Zotero/Mairal et al_2012_Task-Driven Dictionary Learning.pdf;/home/arthur/Zotero/storage/KCE7X8ZE/scholar_url.html}
}

@article{wang_large_2014,
  langid = {english},
  title = {Large Scale Optimization for Machine Learning},
  url = {http://conservancy.umn.edu/handle/11299/172052},
  abstract = {Over the last several decades, tremendous tools have been developed in machine learning, ranging from statistical models to scalable algorithms, from learning strategies to various tasks, having a far-reaching influence in broad applications ranging from image and speech recogni- tions to recommender systems, and from bioinformatics to robotics. In entering the era of big data, large scale machine learning tools become increasingly important in training a big model on big data. Since machine learning problems are fundamentally empirical risk minimization problems, large scale optimization plays a key role in building a large scale machine learning system. However, scaling optimization algorithms like stochastic gradient descent (SGD) in a distributed system raises some issues like synchronization since they were not designed for this purpose. Synchronization is required because consistency should be guaranteed, i.e., the parameters in different machines should be the same. Synchronization leads to blocking com- putation and performance degradation of a distributed system. Without blocking, overwriting may happen and consistency can not be guaranteed. Moreover, SGD may not be suitable for constrained optimization problems.To address the issues of scaling optimization algorithms, we develop several novel opti- mization algorithms suitable for distributed systems from two settings, i.e., unconstrained op- timization and equality-constrained optimization. First, building on SGD in the unconstrained optimization setting, we propose online randomized block coordinate descent which randomly updates some parameters using some samples and thus allows the overwriting in SGD. Second, instead of striving to maintain consistency at each iteration in the unconstrained optimization setting, we turn to the equality-constrained optimization which guarantees eventual consistency , i.e., the parameters in different machines are not the same at each iteration but will be the same eventually. The equality-constrained optimization also includes the cases that SGD can not be applied.The alternating direction method of multipliers (ADMM) provides a suitable framework for equality-constrained optimziation but raises some issues: (1) it does not provide a systematic way to solve subproblems; (2) it requires to solve all subproblems and synchronization; (3) it is a batch method which can not process data online. For the first issue, we propose Bregman ADMM which provides a unified framework to solve subproblems efficiently. For the second issue, we propose parallel direction method of multipliers (PDMM), which randomly picks some subproblems to solve and does asynchronous aggregation. Finally, we introduce online ADMM so that the algorithm can process partial data at each iteration.To validate the effectiveness and scalability of the proposed algorithms, we particularly apply them to a variety of applications, including sparse structure learning and maximum a posterior (MAP) inference in probabilistic graphical models, and online dictionary learning. We also implement the proposed methods on various architectures, including hundreds to thousands CPU cores in clusters and GPUs. Experimental results show that the proposed methods can scale gracefully with the number of cores and perform better than state-of-the-art methods.},
  urldate = {2015-06-08},
  date = {2014-12},
  author = {Wang, Huahua},
  file = {/home/arthur/Zotero/storage/B3CA5B3C/172052.html}
}

@inproceedings{varoquaux_cohort-level_2013,
  title = {Cohort-Level Brain Mapping: Learning Cognitive Atoms to Single out Specialized Regions},
  url = {http://link.springer.com/chapter/10.1007/978-3-642-38868-2_37},
  shorttitle = {Cohort-Level Brain Mapping},
  booktitle = {Proceedings of the {{Information Processing}} in {{Medical Imaging Conference}}},
  publisher = {{Springer}},
  urldate = {2015-03-05},
  date = {2013},
  pages = {438--449},
  author = {Varoquaux, Gaël and Schwartz, Yannick and Pinel, Philippe and Thirion, Bertrand},
  file = {/home/arthur/Dropbox/Zotero/Varoquaux et al_2013_Cohort-level brain mapping.pdf}
}

@article{jaggi_equivalence_2014,
  title = {An Equivalence between the Lasso and Support Vector Machines},
  url = {https://books.google.fr/books?hl=en&lr=&id=5Y_SBQAAQBAJ&oi=fnd&pg=PA1&dq=related:Zt3OmPZHRgcZBM:scholar.google.com/&ots=nwCrsbqDxg&sig=8VM_ME0NQrOefvKJHVYjW4DLvuc},
  journaltitle = {Regularization, Optimization, Kernels, and Support Vector Machines},
  urldate = {2015-06-01},
  date = {2014},
  pages = {1},
  author = {Jaggi, Martin},
  file = {/home/arthur/Dropbox/Zotero/Jaggi_2014_An equivalence between the lasso and support vector machines.pdf;/home/arthur/Zotero/storage/QMK4AA44/books.html}
}

@article{smith_functional_2013,
  langid = {english},
  title = {Functional Connectomics from Resting-State {{fMRI}}},
  volume = {17},
  issn = {13646613},
  url = {http://linkinghub.elsevier.com/retrieve/pii/S1364661313002209},
  number = {12},
  journaltitle = {Trends in Cognitive Sciences},
  urldate = {2015-07-22},
  date = {2013-12},
  pages = {666-682},
  author = {Smith, Stephen M. and Vidaurre, Diego and Beckmann, Christian F. and Glasser, Matthew F. and Jenkinson, Mark and Miller, Karla L. and Nichols, Thomas E. and Robinson, Emma C. and Salimi-Khorshidi, Gholamreza and Woolrich, Mark W. and Barch, Deanna M. and Uğurbil, Kamil and Van Essen, David C.},
  file = {/home/arthur/Dropbox/Zotero/Smith et al_2013_Functional connectomics from resting-state fMRI.pdf}
}

@article{neyshabur_sparse_2013,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1311.3315},
  primaryClass = {cs, stat},
  title = {Sparse {{Matrix Factorization}}},
  url = {http://arxiv.org/abs/1311.3315},
  abstract = {We investigate the problem of factorizing a matrix into several sparse matrices and propose an algorithm for this under randomness and sparsity assumptions. This problem can be viewed as a simplification of the deep learning problem where finding a factorization corresponds to finding edges in different layers and values of hidden units. We prove that under certain assumptions for a sparse linear deep network with \$n\$ nodes in each layer, our algorithm is able to recover the structure of the network and values of top layer hidden units for depths up to \$$\backslash$tilde O(n\^\{1/6\})\$. We further discuss the relation among sparse matrix factorization, deep learning, sparse recovery and dictionary learning.},
  urldate = {2015-06-13},
  date = {2013-11-13},
  keywords = {Computer Science - Learning,Statistics - Machine Learning},
  author = {Neyshabur, Behnam and Panigrahy, Rina},
  file = {/home/arthur/Dropbox/Zotero/Neyshabur_Panigrahy_2013_Sparse Matrix Factorization.pdf;/home/arthur/Zotero/storage/HF998SIA/1311.html}
}

@article{fuchs_recovery_2005,
  title = {Recovery of Exact Sparse Representations in the Presence of Bounded Noise},
  volume = {51},
  url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1512430},
  number = {10},
  journaltitle = {Information Theory, IEEE Transactions on},
  urldate = {2015-06-22},
  date = {2005},
  pages = {3601--3608},
  author = {Fuchs, Jean Jacques},
  file = {/home/arthur/Dropbox/Zotero/Fuchs_2005_Recovery of exact sparse representations in the presence of bounded noise.pdf;/home/arthur/Zotero/storage/INH9X83X/abs_all.html}
}

@article{pedregosa_scikit-learn_2011,
  title = {Scikit-Learn: Machine Learning in {{Python}}},
  volume = {12},
  url = {http://jmlr.csail.mit.edu/papers/v12/pedregosa11a.html},
  shorttitle = {Scikit-Learn},
  abstract = {Scikit-learn is a Python module integrating a wide range of state-of-the-art machine learning algorithms for medium-scale supervised and unsupervised problems. This package focuses on bringing machine learning to non-specialists using a general-purpose high-level language.  Emphasis is put on ease of use, performance, documentation, and API consistency.  It has minimal dependencies and is distributed under the simplified BSD license, encouraging its use in both academic and commercial settings.  Source code, binaries, and documentation can be downloaded from http://scikit-learn.sourceforge.net.},
  journaltitle = {Journal of Machine Learning Research},
  urldate = {2015-06-08},
  date = {2011},
  pages = {2825−2830},
  author = {Pedregosa, Fabian and Varoquaux, Gaël and Gramfort, Alexandre and Michel, Vincent and Thirion, Bertrand and Grisel, Olivier and Blondel, Mathieu and Prettenhofer, Peter and Weiss, Ron and Dubourg, Vincent and Vanderplas, Jake and Passos, Alexandre and Cournapeau, David and Brucher, Matthieu and Perrot, Matthieu and Duchesnay, Édouard},
  file = {/home/arthur/Dropbox/Zotero/Pedregosa et al_2011_Scikit-learn.pdf}
}

@article{rippel_spectral_2015,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1506.03767},
  primaryClass = {cs, stat},
  title = {Spectral {{Representations}} for {{Convolutional Neural Networks}}},
  url = {http://arxiv.org/abs/1506.03767},
  abstract = {Discrete Fourier transforms provide a significant speedup in the computation of convolutions in deep learning. In this work, we demonstrate that, beyond its advantages for efficient computation, the spectral domain also provides a powerful representation in which to model and train convolutional neural networks (CNNs). We employ spectral representations to introduce a number of innovations to CNN design. First, we propose spectral pooling, which performs dimensionality reduction by truncating the representation in the frequency domain. This approach preserves considerably more information per parameter than other pooling strategies and enables flexibility in the choice of pooling output dimensionality. This representation also enables a new form of stochastic regularization by randomized modification of resolution. We show that these methods achieve competitive results on classification and approximation tasks, without using any dropout or max-pooling. Finally, we demonstrate the effectiveness of complex-coefficient spectral parameterization of convolutional filters. While this leaves the underlying model unchanged, it results in a representation that greatly facilitates optimization. We observe on a variety of popular CNN configurations that this leads to significantly faster convergence during training.},
  urldate = {2015-06-13},
  date = {2015-06-11},
  keywords = {Computer Science - Learning,Statistics - Machine Learning},
  author = {Rippel, Oren and Snoek, Jasper and Adams, Ryan P.},
  file = {/home/arthur/Zotero/storage/54FP6P5F/1506.html}
}

@inproceedings{varoquaux_ica-based_2010,
  title = {{{ICA}}-Based Sparse Features Recovery from {{fMRI}} Datasets},
  url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5490204},
  booktitle = {Biomedical {{Imaging}}: {{From Nano}} to {{Macro}}, 2010 {{IEEE International Symposium}} On},
  publisher = {{IEEE}},
  urldate = {2015-03-20},
  date = {2010},
  pages = {1177--1180},
  author = {Varoquaux, Gaël and Keller, Merlin and Poline, J.-B. and Ciuciu, Philippe and Thirion, Bertrand},
  file = {/home/arthur/Dropbox/Zotero/Varoquaux et al_2010_ICA-based sparse features recovery from fMRI datasets.pdf;/home/arthur/Zotero/storage/TSPWZDN5/Varoquaux et al. - 2010 - ICA-based sparse features recovery from fMRI datas.html}
}

@inproceedings{sarlos_improved_2006,
  title = {Improved Approximation Algorithms for Large Matrices via Random Projections},
  url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4031351},
  booktitle = {Foundations of {{Computer Science}}, 2006. {{FOCS}}'06. 47th {{Annual IEEE Symposium}} On},
  publisher = {{IEEE}},
  urldate = {2015-12-30},
  date = {2006},
  pages = {143--152},
  author = {Sarlos, Tamas},
  file = {/home/arthur/Dropbox/Zotero/Sarlos_2006_Improved approximation algorithms for large matrices via random projections.pdf;/home/arthur/Zotero/storage/E493VG7X/abs_all.html}
}

@report{mairal_learning_2007,
  title = {Learning Multiscale Sparse Representations for Image and Video Restoration},
  url = {http://oai.dtic.mil/oai/oai?verb=getRecord&metadataPrefix=html&identifier=ADA519269},
  institution = {{DTIC Document}},
  urldate = {2015-05-31},
  date = {2007},
  author = {Mairal, Julien and Sapiro, Guillermo and Elad, Michael},
  file = {/home/arthur/Dropbox/Zotero/Mairal et al_2007_Learning multiscale sparse representations for image and video restoration.pdf;/home/arthur/Zotero/storage/6AQW7BDA/oai.html}
}

@inproceedings{xiang_learning_2011,
  title = {Learning Sparse Representations of High Dimensional Data on Large Scale Dictionaries},
  url = {http://machinelearning.wustl.edu/mlpapers/paper_files/NIPS2011_0578.pdf},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  urldate = {2015-06-03},
  date = {2011},
  pages = {900--908},
  author = {Xiang, Zhen J. and Xu, Hao and Ramadge, Peter J.},
  file = {/home/arthur/Dropbox/Zotero/Xiang et al_2011_Learning sparse representations of high dimensional data on large scale.pdf}
}

@inproceedings{jiang_learning_2011,
  title = {Learning a Discriminative Dictionary for Sparse Coding via Label Consistent {{K}}-{{SVD}}},
  doi = {10.1109/CVPR.2011.5995354},
  abstract = {A label consistent K-SVD (LC-KSVD) algorithm to learn a discriminative dictionary for sparse coding is presented. In addition to using class labels of training data, we also associate label information with each dictionary item (columns of the dictionary matrix) to enforce discriminability in sparse codes during the dictionary learning process. More specifically, we introduce a new label consistent constraint called `discriminative sparse-code error' and combine it with the reconstruction error and the classification error to form a unified objective function. The optimal solution is efficiently obtained using the K-SVD algorithm. Our algorithm learns a single over-complete dictionary and an optimal linear classifier jointly. It yields dictionaries so that feature points with the same class labels have similar sparse codes. Experimental results demonstrate that our algorithm outperforms many recently proposed sparse coding techniques for face and object category recognition under the same learning conditions.},
  eventtitle = {2011 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  booktitle = {2011 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  date = {2011-06},
  pages = {1697-1704},
  keywords = {Databases,Equations,Face,K-SVD,Testing,Training,classification error,dictionaries,dictionary learning process,discriminative sparse code error,face recognition,image classification,image coding,label consistent,learning (artificial intelligence),object category recognition,object recognition,optimal linear classifier,reconstruction error,singular value decomposition,training data},
  author = {Jiang, Zhuolin and Lin, Zhe and Davis, L.S.},
  file = {/home/arthur/Dropbox/Zotero/Jiang et al_2011_Learning a discriminative dictionary for sparse coding via label consistent.pdf;/home/arthur/Zotero/storage/PPNPGDK3/abs_all.html}
}

@article{smith_group-pca_2014,
  langid = {english},
  title = {Group-{{PCA}} for Very Large {{fMRI}} Datasets},
  volume = {101},
  issn = {10538119},
  url = {http://linkinghub.elsevier.com/retrieve/pii/S105381191400634X},
  journaltitle = {NeuroImage},
  urldate = {2015-09-22},
  date = {2014-11},
  pages = {738-749},
  author = {Smith, Stephen M. and Hyvärinen, Aapo and Varoquaux, Gaël and Miller, Karla L. and Beckmann, Christian F.},
  file = {/home/arthur/Dropbox/Zotero/Smith et al_2014_Group-PCA for very large fMRI datasets.pdf}
}

@inproceedings{clarkson_sublinear_2010,
  title = {Sublinear {{Optimization}} for {{Machine Learning}}},
  doi = {10.1109/FOCS.2010.50},
  abstract = {We give sub linear-time approximation algorithms for some optimization problems arising in machine learning, such as training linear classifiers and finding minimum enclosing balls. Our algorithms can be extended to some kernelized versions of these problems, such as SVDD, hard margin SVM, and L2-SVM, for which sub linear-time algorithms were not known before. These new algorithms use a combination of a novel sampling techniques and a new multiplicative update algorithm. We give lower bounds which show the running times of many of our algorithms to be nearly best possible in the unit-cost RAM model. We also give implementations of our algorithms in the semi-streaming setting, obtaining the first low pass polylogarithmic space and sub linear time algorithms achieving arbitrary approximation factor.},
  eventtitle = {2010 51st {{Annual IEEE Symposium}} on {{Foundations}} of {{Computer Science}} ({{FOCS}})},
  booktitle = {2010 51st {{Annual IEEE Symposium}} on {{Foundations}} of {{Computer Science}} ({{FOCS}})},
  date = {2010-10},
  pages = {449-457},
  keywords = {Approximation algorithms,Approximation methods,Classification algorithms,Machine learning algorithms,Optimization,SVM,Vectors,arbitrary approximation factor,classification,computational complexity,learning (artificial intelligence),linear classifier,machine learning,multiplicative update algorithm,optimisation,pattern classification,polylogarithmic space,polynomial approximation,sampling technique,sublinear algorithms,sublinear optimization,sublinear time approximation,support vector machine,support vector machines},
  author = {Clarkson, K.L. and Hazan, E. and Woodruff, D.P.},
  file = {/home/arthur/Dropbox/Zotero/Clarkson et al_2010_Sublinear Optimization for Machine Learning.pdf;/home/arthur/Zotero/storage/MC5MWCCN/articleDetails.html}
}

@article{cevher_convex_2014,
  title = {Convex Optimization for Big Data: {{Scalable}}, Randomized, and Parallel Algorithms for Big Data Analytics},
  volume = {31},
  url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=6879615},
  shorttitle = {Convex Optimization for Big Data},
  number = {5},
  journaltitle = {Signal Processing Magazine, IEEE},
  urldate = {2015-03-05},
  date = {2014},
  pages = {32--43},
  author = {Cevher, Volkan and Becker, Stephen and Schmidt, Mark},
  file = {/home/arthur/Dropbox/Zotero/Cevher et al_2014_Convex optimization for big data.pdf}
}

@article{mairal_online_2010,
  title = {Online Learning for Matrix Factorization and Sparse Coding},
  volume = {11},
  url = {http://dl.acm.org/citation.cfm?id=1756008},
  journaltitle = {The Journal of Machine Learning Research},
  urldate = {2015-03-05},
  date = {2010},
  pages = {19--60},
  author = {Mairal, Julien and Bach, Francis and Ponce, Jean and Sapiro, Guillermo},
  file = {/home/arthur/Dropbox/Zotero/Mairal et al_2010_Online learning for matrix factorization and sparse coding.pdf}
}

@incollection{bottou_large-scale_2010,
  title = {Large-Scale Machine Learning with Stochastic Gradient Descent},
  booktitle = {Proceedings of {{COMPSTAT}}},
  publisher = {{Springer}},
  date = {2010},
  pages = {177--186},
  author = {Bottou, Léon},
  file = {/home/arthur/Dropbox/Zotero/Bottou_2010_Large-scale machine learning with stochastic gradient descent.pdf;/home/arthur/Zotero/storage/XKFKP7EQ/978-3-7908-2604-3_16.html}
}

@article{kreutz-delgado_dictionary_2003,
  title = {Dictionary Learning Algorithms for Sparse Representation},
  volume = {15},
  url = {http://www.mitpressjournals.org/doi/abs/10.1162/089976603762552951},
  number = {2},
  journaltitle = {Neural computation},
  urldate = {2015-03-20},
  date = {2003},
  pages = {349--396},
  author = {Kreutz-Delgado, Kenneth and Murray, Joseph F. and Rao, Bhaskar D. and Engan, Kjersti and Lee, Te-Won and Sejnowski, Terrence J.},
  file = {/home/arthur/Dropbox/Zotero/Kreutz-Delgado et al_2003_Dictionary learning algorithms for sparse representation.pdf;/home/arthur/Zotero/storage/EQUZJMTZ/Kreutz-Delgado et al. - 2003 - Dictionary learning algorithms for sparse represen.html}
}

@article{mckeown_analysis_1998,
  langid = {english},
  title = {Analysis of {{fMRI Data}} by {{Blind Separation}} into {{Independent Spatial Components}}},
  volume = {6},
  issn = {1065-9471},
  abstract = {Current analytical techniques applied to functional magnetic resonance imaging (fMRI) data require a priori knowledge or specific assumptions about the time courses of processes contributing to the measured signals. Here we describe a new method for analyzing fMRI data based on the independent component analysis (ICA) algorithm of Bell and Sejnowski ([1995]: Neural Comput 7:1129-1159). We decomposed eight fMRI data sets from 4 normal subjects performing Stroop color-naming, the Brown and Peterson work/number task, and control tasks into spatially independent components. Each component consisted of voxel values at fixed three-dimensional locations (a component "map"), and a unique associated time course of activation. Given data from 144 time points collected during a 6-min trial, ICA extracted an equal number of spatially independent components. In all eight trials, ICA derived one and only one component with a time course closely matching the time course of 40-sec alternations between experimental and control tasks. The regions of maximum activity in these consistently task-related components generally overlapped active regions detected by standard correlational analysis, but included frontal regions not detected by correlation. Time courses of other ICA components were transiently task-related, quasiperiodic, or slowly varying. By utilizing higher-order statistics to enforce successively stricter criteria for spatial independence between component maps, both the ICA algorithm and a related fourth-order decomposition technique (Comon [1994]: Signal Processing 36:11-20) were superior to principal component analysis (PCA) in determining the spatial and temporal extent of task-related activation. For each subject, the time courses and active regions of the task-related ICA components were consistent across trials and were robust to the addition of simulated noise. Simulated movement artifact and simulated task-related activations added to actual fMRI data were clearly separated by the algorithm. ICA can be used to distinguish between nontask-related signal components, movements, and other artifacts, as well as consistently or transiently task-related fMRI activations, based on only weak assumptions about their spatial distributions and without a priori assumptions about their time courses. ICA appears to be a highly promising method for the analysis of fMRI data from normal and clinical populations, especially for uncovering unpredictable transient patterns of brain activity associated with performance of psychomotor tasks.},
  number = {3},
  journaltitle = {Human Brain Mapping},
  date = {1998},
  pages = {160-188},
  keywords = {Algorithms,Artifacts,Brain Mapping,Computer Simulation,Head Movements,Humans,Linear Models,Magnetic Resonance Imaging,Reference Values,Reproducibility of Results,Signal Processing; Computer-Assisted,Time Factors},
  author = {McKeown, M. J. and Makeig, S. and Brown, G. G. and Jung, T. P. and Kindermann, S. S. and Bell, A. J. and Sejnowski, T. J.},
  file = {/home/arthur/Dropbox/Zotero/McKeown et al_1998_Analysis of fMRI Data by Blind Separation into Independent Spatial Components.pdf},
  eprinttype = {pmid},
  eprint = {9673671}
}

@inproceedings{gregor_learning_2010,
  title = {Learning Fast Approximations of Sparse Coding},
  url = {http://machinelearning.wustl.edu/mlpapers/paper_files/icml2010_GregorL10.pdf},
  booktitle = {Proceedings of the 27th {{International Conference}} on {{Machine Learning}} ({{ICML}}-10)},
  urldate = {2015-05-21},
  date = {2010},
  pages = {399--406},
  author = {Gregor, Karol and LeCun, Yann},
  file = {/home/arthur/Dropbox/Zotero/Gregor_LeCun_2010_Learning fast approximations of sparse coding.pdf}
}

@article{zhao_supervised_2015,
  title = {Supervised {{Dictionary Learning}} for {{Inferring Concurrent Brain Networks}}},
  issn = {0278-0062, 1558-254X},
  url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=7076593},
  journaltitle = {IEEE Transactions on Medical Imaging},
  urldate = {2015-07-22},
  date = {2015},
  pages = {1-1},
  author = {Zhao, Shijie and Han, Junwei and Lv, Jinglei and Jiang, Xi and Hu, Xintao and Zhao, Yu and Ge, Bob and Guo, Lei and Liu, Tianming},
  file = {/home/arthur/Dropbox/Zotero/Zhao et al_2015_Supervised Dictionary Learning for Inferring Concurrent Brain Networks.pdf}
}

@article{bronstein_learning_2012,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1206.4649},
  primaryClass = {cs, stat},
  title = {Learning {{Efficient Structured Sparse Models}}},
  url = {http://arxiv.org/abs/1206.4649},
  abstract = {We present a comprehensive framework for structured sparse coding and modeling extending the recent ideas of using learnable fast regressors to approximate exact sparse codes. For this purpose, we develop a novel block-coordinate proximal splitting method for the iterative solution of hierarchical sparse coding problems, and show an efficient feed forward architecture derived from its iteration. This architecture faithfully approximates the exact structured sparse codes with a fraction of the complexity of the standard optimization methods. We also show that by using different training objective functions, learnable sparse encoders are no longer restricted to be mere approximants of the exact sparse code for a pre-given dictionary, as in earlier formulations, but can be rather used as full-featured sparse encoders or even modelers. A simple implementation shows several orders of magnitude speedup compared to the state-of-the-art at minimal performance degradation, making the proposed framework suitable for real time and large-scale applications.},
  urldate = {2015-06-03},
  date = {2012-06-18},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Learning,Statistics - Machine Learning},
  author = {Bronstein, Alex and Sprechmann, Pablo and Sapiro, Guillermo},
  file = {/home/arthur/Dropbox/Zotero/Bronstein et al_2012_Learning Efficient Structured Sparse Models.pdf;/home/arthur/Zotero/storage/JZT42TDQ/1206.html}
}

@article{bickel_simultaneous_2009,
  langid = {english},
  title = {Simultaneous Analysis of {{Lasso}} and {{Dantzig}} Selector},
  volume = {37},
  issn = {0090-5364, 2168-8966},
  url = {http://projecteuclid.org/euclid.aos/1245332830},
  abstract = {We show that, under a sparsity scenario, the Lasso estimator and the Dantzig selector exhibit similar behavior. For both methods, we derive, in parallel, oracle inequalities for the prediction risk in the general nonparametric regression model, as well as bounds on the ℓp estimation loss for 1≤p≤2 in the linear model when the number of variables can be much larger than the sample size.},
  number = {4},
  journaltitle = {The Annals of Statistics},
  shortjournal = {Ann. Statist.},
  urldate = {2015-05-06},
  date = {2009-08},
  pages = {1705-1732},
  keywords = {Linear Models,model selection,nonparametric statistics},
  author = {Bickel, Peter J. and Ritov, Ya’acov and Tsybakov, Alexandre B.},
  file = {/home/arthur/Dropbox/Zotero/Bickel et al_2009_Simultaneous analysis of Lasso and Dantzig selector.pdf;/home/arthur/Zotero/storage/27S62HMM/1245332830.html}
}

@inproceedings{duchi_efficient_2008,
  title = {Efficient Projections onto the l 1-Ball for Learning in High Dimensions},
  url = {http://dl.acm.org/citation.cfm?id=1390191},
  booktitle = {Proceedings of  the {{International Conference}} on {{Machine Learning}}},
  publisher = {{ACM}},
  urldate = {2015-06-19},
  date = {2008},
  pages = {272--279},
  author = {Duchi, John and Shalev-Shwartz, Shai and Singer, Yoram and Chandra, Tushar},
  file = {/home/arthur/Dropbox/Zotero/Duchi et al_2008_Efficient projections onto the l 1-ball for learning in high dimensions.pdf;/home/arthur/Zotero/storage/J74GRV84/citation.html}
}

@article{kiviniemi_functional_2009,
  langid = {english},
  title = {Functional Segmentation of the Brain Cortex Using High Model Order Group {{PICA}}},
  volume = {30},
  issn = {1097-0193},
  doi = {10.1002/hbm.20813},
  abstract = {Baseline activity of resting state brain networks (RSN) in a resting subject has become one of the fastest growing research topics in neuroimaging. It has been shown that up to 12 RSNs can be differentiated using an independent component analysis (ICA) of the blood oxygen level dependent (BOLD) resting state data. In this study, we investigate how many RSN signal sources can be separated from the entire brain cortex using high dimension ICA analysis from a group dataset. Group data from 55 subjects was analyzed using temporal concatenation and a probabilistic independent component analysis algorithm. ICA repeatability testing verified that 60 of the 70 computed components were robustly detectable. Forty-two independent signal sources were identifiable as RSN, and 28 were related to artifacts or other noninterest sources (non-RSN). The depicted RSNs bore a closer match to functional neuroanatomy than the previously reported RSN components. The non-RSN sources have significantly lower temporal intersource connectivity than the RSN (P $<$ 0.0003). We conclude that the high model order ICA of the group BOLD data enables functional segmentation of the brain cortex. The method enables new approaches to causality and connectivity analysis with more specific anatomical details.},
  number = {12},
  journaltitle = {Human Brain Mapping},
  shortjournal = {Hum Brain Mapp},
  date = {2009-12},
  pages = {3865-3886},
  keywords = {Adult,Brain,Brain Mapping,Female,Humans,Image Processing; Computer-Assisted,Magnetic Resonance Imaging,Male,Principal Component Analysis},
  author = {Kiviniemi, Vesa and Starck, Tuomo and Remes, Jukka and Long, Xiangyu and Nikkinen, Juha and Haapea, Marianne and Veijola, Juha and Moilanen, Irma and Isohanni, Matti and Zang, Yu-Feng and Tervonen, Osmo},
  file = {/home/arthur/Dropbox/Zotero/Kiviniemi et al_2009_Functional segmentation of the brain cortex using high model order group PICA.pdf},
  eprinttype = {pmid},
  eprint = {19507160}
}

@article{beckmann_probabilistic_2004,
  title = {Probabilistic Independent Component Analysis for Functional Magnetic Resonance Imaging},
  volume = {23},
  url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1263605},
  number = {2},
  journaltitle = {Medical Imaging, IEEE Transactions on},
  urldate = {2015-05-06},
  date = {2004},
  pages = {137--152},
  author = {Beckmann, Christian F. and Smith, Stephen M.},
  file = {/home/arthur/Dropbox/Zotero/Beckmann_Smith_2004_Probabilistic independent component analysis for functional magnetic resonance.pdf;/home/arthur/Zotero/storage/MD7ZWQG4/abs_all.html}
}

@article{vainsencher_sample_2011,
  title = {The {{Sample Complexity}} of {{Dictionary Learning}}},
  volume = {12},
  issn = {1532-4435},
  url = {http://dl.acm.org/citation.cfm?id=1953048.2078210},
  abstract = {A large set of signals can sometimes be described sparsely using a dictionary, that is, every element can be represented as a linear combination of few elements from the dictionary. Algorithms for various signal processing applications, including classification, denoising and signal separation, learn a dictionary from a given set of signals to be represented. Can we expect that the error in representing by such a dictionary a previously unseen signal from the same source will be of similar magnitude as those for the given examples? We assume signals are generated from a fixed distribution, and study these questions from a statistical learning theory perspective. We develop generalization bounds on the quality of the learned dictionary for two types of constraints on the coefficient selection, as measured by the expected L2 error in representation when the dictionary is used. For the case of l1 regularized coefficient selection we provide a generalization bound of the order of O(√np ln(mλ)/m), where n is the dimension, p is the number of elements in the dictionary, λ is a bound on the l1 norm of the coefficient vector and m is the number of samples, which complements existing results. For the case of representing a new signal as a combination of at most k dictionary elements, we provide a bound of the order O(√np ln(mk)/m) under an assumption on the closeness to orthogonality of the dictionary (low Babel function). We further show that this assumption holds for most dictionaries in high dimensions in a strong probabilistic sense. Our results also include bounds that converge as 1/m, not previously known for this problem. We provide similar results in a general setting using kernels with weak smoothness requirements.},
  journaltitle = {J. Mach. Learn. Res.},
  urldate = {2015-09-24},
  date = {2011-11},
  pages = {3259--3281},
  author = {Vainsencher, Daniel and Mannor, Shie and Bruckstein, Alfred M.},
  file = {/home/arthur/Dropbox/Zotero/Vainsencher et al_2011_The Sample Complexity of Dictionary Learning.pdf}
}

@article{jenatton_multiscale_2012,
  title = {Multiscale Mining of {{fMRI}} Data with Hierarchical Structured Sparsity},
  volume = {5},
  url = {http://epubs.siam.org/doi/abs/10.1137/110832380},
  number = {3},
  journaltitle = {SIAM Journal on Imaging Sciences},
  urldate = {2015-03-22},
  date = {2012},
  pages = {835--856},
  author = {Jenatton, Rodolphe and Gramfort, Alexandre and Michel, Vincent and Obozinski, Guillaume and Eger, Evelyn and Bach, Francis and Thirion, Bertrand},
  file = {/home/arthur/Dropbox/Zotero/Jenatton et al_2012_Multiscale mining of fMRI data with hierarchical structured sparsity.pdf}
}

@article{mohr_sparse_2015,
  title = {Sparse Regularization Techniques Provide Novel Insights into Outcome Integration Processes},
  volume = {104},
  issn = {1053-8119},
  url = {http://www.sciencedirect.com/science/article/pii/S1053811914008490},
  abstract = {By exploiting information that is contained in the spatial arrangement of neural activations, multivariate pattern analysis (MVPA) can detect distributed brain activations which are not accessible by standard univariate analysis. Recent methodological advances in MVPA regularization techniques have made it feasible to produce sparse discriminative whole-brain maps with highly specific patterns. Furthermore, the most recent refinement, the Graph Net, explicitly takes the 3D-structure of fMRI data into account. Here, these advanced classification methods were applied to a large fMRI sample (N = 70) in order to gain novel insights into the functional localization of outcome integration processes. While the beneficial effect of differential outcomes is well-studied in trial-and-error learning, outcome integration in the context of instruction-based learning has remained largely unexplored. In order to examine neural processes associated with outcome integration in the context of instruction-based learning, two groups of subjects underwent functional imaging while being presented with either differential or ambiguous outcomes following the execution of varying stimulus–response instructions. While no significant univariate group differences were found in the resulting fMRI dataset, L1-regularized (sparse) classifiers performed significantly above chance and also clearly outperformed the standard L2-regularized (dense) Support Vector Machine on this whole-brain between-subject classification task. Moreover, additional L2-regularization via the Elastic Net and spatial regularization by the Graph Net improved interpretability of discriminative weight maps but were accompanied by reduced classification accuracies. Most importantly, classification based on sparse regularization facilitated the identification of highly specific regions differentially engaged under ambiguous and differential outcome conditions, comprising several prefrontal regions previously associated with probabilistic learning, rule integration and reward processing. Additionally, a detailed post-hoc analysis of these regions revealed that distinct activation dynamics underlay the processing of ambiguous relative to differential outcomes. Together, these results show that L1-regularization can improve classification performance while simultaneously providing highly specific and interpretable discriminative activation patterns.},
  journaltitle = {NeuroImage},
  shortjournal = {NeuroImage},
  urldate = {2015-06-13},
  date = {2015-01-01},
  pages = {163-176},
  keywords = {Graph Net,Instruction-based learning,MVPA,Outcome integration,Regularization,Structured sparsity},
  author = {Mohr, Holger and Wolfensteller, Uta and Frimmel, Steffi and Ruge, Hannes},
  file = {/home/arthur/Dropbox/Zotero/Mohr et al_2015_Sparse regularization techniques provide novel insights into outcome.pdf;/home/arthur/Zotero/storage/HP8DSAVV/S1053811914008490.html}
}

@article{worsley_general_2002,
  title = {A General Statistical Analysis for {{fMRI}} Data},
  volume = {15},
  url = {http://www.sciencedirect.com/science/article/pii/S1053811901909334},
  number = {1},
  journaltitle = {Neuroimage},
  urldate = {2015-05-06},
  date = {2002},
  pages = {1--15},
  author = {Worsley, Keith J. and Liao, C. H. and Aston, J. and Petre, V. and Duncan, G. H. and Morales, F. and Evans, A. C.},
  file = {/home/arthur/Dropbox/Zotero/Worsley et al_2002_A general statistical analysis for fMRI data.pdf;/home/arthur/Zotero/storage/UC8D8MN5/S1053811901909334.html}
}

@article{bradley_parallel_2011,
  title = {Parallel Coordinate Descent for L1-Regularized Loss Minimization},
  url = {http://arxiv.org/abs/1105.5379},
  journaltitle = {arXiv preprint arXiv:1105.5379},
  urldate = {2015-09-06},
  date = {2011},
  author = {Bradley, Joseph K. and Kyrola, Aapo and Bickson, Danny and Guestrin, Carlos},
  file = {/home/arthur/Dropbox/Zotero/Bradley et al_2011_Parallel coordinate descent for l1-regularized loss minimization.pdf}
}

@article{varoquaux_group_2010,
  title = {A Group Model for Stable Multi-Subject {{ICA}} on {{fMRI}} Datasets},
  volume = {51},
  url = {http://www.sciencedirect.com/science/article/pii/S1053811910001618},
  number = {1},
  journaltitle = {Neuroimage},
  urldate = {2015-05-06},
  date = {2010},
  pages = {288--299},
  author = {Varoquaux, Gaël and Sadaghiani, Sepideh and Pinel, Philippe and Kleinschmidt, Andreas and Poline, Jean-Baptiste and Thirion, Bertrand},
  file = {/home/arthur/Dropbox/Zotero/Varoquaux et al_2010_A group model for stable multi-subject ICA on fMRI datasets.pdf;/home/arthur/Zotero/storage/H4AKTTQI/S1053811910001618.html}
}

@article{koren_matrix_2009,
  title = {Matrix Factorization Techniques for Recommender Systems},
  url = {http://www.computer.org/csdl/mags/co/2009/08/mco2009080030.html},
  number = {8},
  journaltitle = {Computer},
  urldate = {2015-11-04},
  date = {2009},
  pages = {30--37},
  author = {Koren, Yehuda and Bell, Robert and Volinsky, Chris},
  file = {/home/arthur/Dropbox/Zotero/Koren et al_2009_Matrix factorization techniques for recommender systems.pdf;/home/arthur/Zotero/storage/VHFQMNGI/mco2009080030.html}
}

@inproceedings{hsieh_fast_2011,
  title = {Fast Coordinate Descent Methods with Variable Selection for Non-Negative Matrix Factorization},
  url = {http://dl.acm.org/citation.cfm?id=2020577},
  booktitle = {Proceedings of the 17th {{ACM SIGKDD}} International Conference on {{Knowledge}} Discovery and Data Mining},
  publisher = {{ACM}},
  urldate = {2015-06-17},
  date = {2011},
  pages = {1064--1072},
  author = {Hsieh, Cho-Jui and Dhillon, Inderjit S.},
  file = {/home/arthur/Dropbox/Zotero/Hsieh_Dhillon_2011_Fast coordinate descent methods with variable selection for non-negative matrix.pdf;/home/arthur/Zotero/storage/MNSJGP34/citation.html}
}

@article{zou_sparse_2006,
  title = {Sparse Principal Component Analysis},
  volume = {15},
  url = {http://www.tandfonline.com/doi/abs/10.1198/106186006X113430},
  number = {2},
  journaltitle = {Journal of computational and graphical statistics},
  urldate = {2015-06-17},
  date = {2006},
  pages = {265--286},
  author = {Zou, Hui and Hastie, Trevor and Tibshirani, Robert},
  file = {/home/arthur/Dropbox/Zotero/Zou et al_2006_Sparse principal component analysis.pdf;/home/arthur/Zotero/storage/6PSWHU9Q/106186006X113430.html}
}

@article{rakotomamonjy_direct_2013,
  title = {Direct Optimization of the Dictionary Learning Problem},
  volume = {61},
  url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=6578208},
  number = {22},
  journaltitle = {Signal Processing, IEEE Transactions on},
  urldate = {2015-05-21},
  date = {2013},
  pages = {5495--5506},
  author = {Rakotomamonjy, Alain},
  file = {/home/arthur/Dropbox/Zotero/Rakotomamonjy_2013_Direct optimization of the dictionary learning problem.pdf;/home/arthur/Zotero/storage/JGPW7CVB/abs_all.html}
}

@article{aharon_k-svd_2006,
  title = {K-{{SVD}}: {{An}} Algorithm for Designing Overcomplete Dictionaries for Sparse Representation},
  volume = {54},
  number = {11},
  journaltitle = {IEEE Transactions on Signal Processing},
  date = {2006},
  pages = {4311--4322},
  author = {Aharon, Michal and Elad, Michael and Bruckstein, Alfred},
  file = {/home/arthur/Dropbox/Zotero/Aharon et al_2006_k-SVD.pdf;/home/arthur/Zotero/storage/2MD977BW/abs_all.html}
}

@article{efron_least_2004,
  title = {Least Angle Regression},
  volume = {32},
  issn = {0090-5364, 2168-8966},
  url = {http://projecteuclid.org/euclid.aos/1083178935},
  abstract = {The purpose of model selection algorithms such as All Subsets, Forward Selection and Backward Elimination is to choose a linear model on the basis of the same set of data to which the model will be applied. Typically we have available a large collection of possible covariates from which we hope to select a parsimonious set for the efficient prediction of a response variable. Least Angle Regression (LARS), a new model selection algorithm, is a useful and less greedy version of traditional forward selection methods. Three main properties are derived: (1) A simple modification of the LARS algorithm implements the Lasso, an attractive version of ordinary least squares that constrains the sum of the absolute regression coefficients; the LARS modification calculates all possible Lasso estimates for a given problem, using an order of magnitude less computer time than previous methods. (2) A different LARS modification efficiently implements Forward Stagewise linear regression, another promising new model selection method; this connection explains the similar numerical results previously observed for the Lasso and Stagewise, and helps us understand the properties of both methods, which are seen as constrained versions of the simpler LARS algorithm. (3) A simple approximation for the degrees of freedom of a LARS estimate is available, from which we derive a Cp estimate of prediction error; this allows a principled choice among the range of possible LARS estimates. LARS and its variants are computationally efficient: the paper describes a publicly available algorithm that requires only the same order of magnitude of computational effort as ordinary least squares applied to the full set of covariates.},
  number = {2},
  journaltitle = {The Annals of Statistics},
  shortjournal = {Ann. Statist.},
  urldate = {2015-05-18},
  date = {2004},
  pages = {407-499},
  keywords = {Lasso,boosting,coefficient paths,linear regression,variable selection},
  author = {Efron, Bradley and Hastie, Trevor and Johnstone, Iain and Tibshirani, Robert},
  file = {/home/arthur/Dropbox/Zotero/Efron et al_2004_Least angle regression.pdf;/home/arthur/Dropbox/Zotero/Efron et al_2004_Least angle regression2.pdf;/home/arthur/Zotero/storage/GWG7CWBX/1083178935.html}
}

@article{slavakis_per-block-convex_2015,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1501.07315},
  primaryClass = {cs},
  title = {Per-{{Block}}-{{Convex Data Modeling}} by {{Accelerated Stochastic Approximation}}},
  url = {http://arxiv.org/abs/1501.07315},
  abstract = {Applications involving dictionary learning, non-negative matrix factorization, subspace clustering, and parallel factor tensor decomposition tasks motivate well algorithms for per-block-convex and non-smooth optimization problems. By leveraging the stochastic approximation paradigm and first-order acceleration schemes, this paper develops an online and modular learning algorithm for a large class of non-convex data models, where convexity is manifested only per-block of variables whenever the rest of them are held fixed. The advocated algorithm incurs computational complexity that scales linearly with the number of unknowns. Under minimal assumptions on the cost functions of the composite optimization task, without bounding constraints on the optimization variables, or any explicit information on bounds of Lipschitz coefficients, the expected cost evaluated online at the resultant iterates is provably convergent with quadratic rate to an accumulation point of the (per-block) minima, while subgradients of the expected cost asymptotically vanish in the mean-squared sense. The merits of the general approach are demonstrated in two online learning setups: (i) Robust linear regression using a sparsity-cognizant total least-squares criterion; and (ii) semi-supervised dictionary learning for network-wide link load tracking and imputation with missing entries. Numerical tests on synthetic and real data highlight the potential of the proposed framework for streaming data analytics by demonstrating superior performance over block coordinate descent, and reduced complexity relative to the popular alternating-direction method of multipliers.},
  urldate = {2015-06-08},
  date = {2015-01-28},
  keywords = {Computer Science - Learning},
  author = {Slavakis, Konstantinos and Giannakis, Georgios B.},
  file = {/home/arthur/Dropbox/Zotero/Slavakis_Giannakis_2015_Per-Block-Convex Data Modeling by Accelerated Stochastic Approximation.pdf;/home/arthur/Zotero/storage/P3TSEZHQ/1501.html}
}

@incollection{mairal_supervised_2009,
  title = {Supervised {{Dictionary Learning}}},
  url = {http://papers.nips.cc/paper/3448-supervised-dictionary-learning.pdf},
  booktitle = {Advances in {{Neural Information Processing Systems}} 21},
  publisher = {{Curran Associates, Inc.}},
  urldate = {2015-03-21},
  date = {2009},
  pages = {1033--1040},
  author = {Mairal, Julien and Ponce, Jean and Sapiro, Guillermo and Zisserman, Andrew and Bach, Francis R.},
  editor = {Koller, D. and Schuurmans, D. and Bengio, Y. and Bottou, L.},
  file = {/home/arthur/Dropbox/Zotero/Mairal et al_2009_Supervised Dictionary Learning.pdf;/home/arthur/Zotero/storage/F6ZQPQ6M/3448-supervised.html}
}

@article{gal_dropout_2015,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1506.02157},
  primaryClass = {stat},
  title = {Dropout as a {{Bayesian Approximation}}: {{Appendix}}},
  url = {http://arxiv.org/abs/1506.02157},
  shorttitle = {Dropout as a {{Bayesian Approximation}}},
  abstract = {We show that a neural network with arbitrary depth and non-linearities, with dropout applied before every weight layer, is mathematically equivalent to an approximation to a well known Bayesian model. This interpretation offers an explanation to some of dropout's key properties, such as its robustness to over-fitting. Our interpretation allows us to reason about uncertainty in deep learning, and allows the introduction of the Bayesian machinery into existing deep learning frameworks in a principled way. This document is an appendix for the main paper "Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning" by Gal and Ghahramani, 2015.},
  urldate = {2016-02-29},
  date = {2015-06-06},
  keywords = {Statistics - Machine Learning},
  author = {Gal, Yarin and Ghahramani, Zoubin},
  file = {/home/arthur/Dropbox/Zotero/Gal_Ghahramani_2015_Dropout as a Bayesian Approximation.pdf;/home/arthur/Zotero/storage/4QRDNUTZ/1506.html}
}

@inproceedings{eavani_sparse_2012,
  title = {Sparse {{Dictionary Learning}} of {{Resting State fMRI Networks}}},
  doi = {10.1109/PRNI.2012.25},
  abstract = {Research in resting state fMRI (rsfMRI) has revealed the presence of stable, anti-correlated functional sub-networks in the brain. Task-positive networks are active during a cognitive process and are anti-correlated with task-negative networks, which are active during rest. In this paper, based on the assumption that the structure of the resting state functional brain connectivity is sparse, we utilize sparse dictionary modeling to identify distinct functional sub-networks. We propose two ways of formulating the sparse functional network learning problem that characterize the underlying functional connectivity from different perspectives. Our results show that the whole-brain functional connectivity can be concisely represented with highly modular, overlapping task-positive/negative pairs of sub-networks.},
  eventtitle = {2012 {{International Workshop}} on {{Pattern Recognition}} in {{NeuroImaging}} ({{PRNI}})},
  booktitle = {2012 {{International Workshop}} on {{Pattern Recognition}} in {{NeuroImaging}} ({{PRNI}})},
  date = {2012-07},
  pages = {73-76},
  keywords = {Brain,Correlation,Independent component analysis,K-SVD,Resting state fMRI,Sparse matrices,Symmetric matrices,Vectors,Xenon,anticorrelated functional subnetworks,biomedical MRI,cognitive process,dictionaries,functional connectivity,functional magnetic resonance imaging,learning (artificial intelligence),medical image processing,resting state fMRI networks,resting state functional brain connectivity,rsfMRI,sparse dictionary learning,sparse dictionary modeling,sparse functional network learning problem,sparse modeling,subnetwork overlapping task-positive-negative pairs,task-negative networks,task-positive networks,whole-brain functional connectivity},
  author = {Eavani, H. and Filipovych, R. and Davatzikos, C. and Satterthwaite, T.D. and Gur, R.E. and Gur, R.C.},
  file = {/home/arthur/Dropbox/Zotero/Eavani et al_2012_Sparse Dictionary Learning of Resting State fMRI Networks.pdf;/home/arthur/Zotero/storage/5MGI4KNC/abs_all.html}
}

@article{olshausen_sparse_1997,
  title = {Sparse Coding with an Overcomplete Basis Set: {{A}} Strategy Employed by {{V1}}?},
  volume = {37},
  issn = {0042-6989},
  url = {http://www.sciencedirect.com/science/article/pii/S0042698997001697},
  shorttitle = {Sparse Coding with an Overcomplete Basis Set},
  abstract = {The spatial receptive fields of simple cells in mammalian striate cortex have been reasonably well described physiologically and can be characterized as being localized, oriented, and bandpass, comparable with the basis functions of wavelet transforms. Previously, we have shown that these receptive field properties may be accounted for in terms of a strategy for producing a sparse distribution of output activity in response to natural images. Here, in addition to describing this work in a more expansive fashion, we examine the neurobiological implications of sparse coding. Of particular interest is the case when the code is overcomplete—i.e., when the number of code elements is greater than the effective dimensionality of the input space. Because the basis functions are non-orthogonal and not linearly independent of each other, sparsifying the code will recruit only those basis functions necessary for representing a given input, and so the input-output function will deviate from being purely linear. These deviations from linearity provide a potential explanation for the weak forms of non-linearity observed in the response properties of cortical simple cells, and they further make predictions about the expected interactions among units in response to naturalistic stimuli.},
  number = {23},
  journaltitle = {Vision Research},
  shortjournal = {Vision Research},
  urldate = {2015-10-08},
  date = {1997-12},
  pages = {3311-3325},
  keywords = {Coding,Gabor-wavelet,Natural images,V1},
  author = {Olshausen, Bruno A. and Field, David J.},
  file = {/home/arthur/Dropbox/Zotero/Olshausen_Field_1997_Sparse coding with an overcomplete basis set.pdf;/home/arthur/Zotero/storage/BWXIZSK7/S0042698997001697.html}
}

@article{tibshirani_regression_1996,
  eprinttype = {jstor},
  eprint = {2346178},
  title = {Regression Shrinkage and Selection via the Lasso},
  journaltitle = {Journal of the Royal Statistical Society. Series B (Methodological)},
  date = {1996},
  pages = {267--288},
  author = {Tibshirani, Robert},
  file = {/home/arthur/Dropbox/Zotero/Tibshirani_1996_Regression shrinkage and selection via the lasso.pdf;/home/arthur/Zotero/storage/P3X84FTS/2346178.html}
}

@book{boyd_convex_2004,
  title = {Convex {{Optimization}}},
  url = {http://stanford.edu/~boyd/cvxbook/bv_cvxbook.pdf},
  urldate = {2015-03-05},
  date = {2004},
  author = {Boyd, Stephen and Vandenberghe, Lieven},
  file = {/home/arthur/Dropbox/Zotero/Boyd_Vandenberghe_2004_Convex Optimization.pdf}
}

@article{thirion_dealing_2006,
  title = {Dealing with the Shortcomings of Spatial Normalization: {{Multi}}-Subject Parcellation of {{fMRI}} Datasets},
  volume = {27},
  url = {http://onlinelibrary.wiley.com/doi/10.1002/hbm.20210/full},
  shorttitle = {Dealing with the Shortcomings of Spatial Normalization},
  number = {8},
  journaltitle = {Human brain mapping},
  urldate = {2015-05-06},
  date = {2006},
  pages = {678--693},
  author = {Thirion, Bertrand and Flandin, Guillaume and Pinel, Philippe and Roche, Alexis and Ciuciu, Philippe and Poline, Jean-Baptiste},
  file = {/home/arthur/Zotero/storage/DX3GBXDI/full.html}
}

@article{richtarik_iteration_2014,
  title = {Iteration Complexity of Randomized Block-Coordinate Descent Methods for Minimizing a Composite Function},
  volume = {144},
  url = {http://link.springer.com/article/10.1007/s10107-012-0614-z},
  number = {1-2},
  journaltitle = {Mathematical Programming},
  urldate = {2015-06-01},
  date = {2014},
  pages = {1--38},
  author = {Richtárik, Peter and Takáč, Martin},
  file = {/home/arthur/Dropbox/Zotero/Richtárik_Takáč_2014_Iteration complexity of randomized block-coordinate descent methods for.pdf;/home/arthur/Zotero/storage/3M9ZGWWX/s10107-012-0614-z.html}
}

@article{shalev-shwartz_online_2011,
  title = {Online Learning and Online Convex Optimization},
  volume = {4},
  url = {http://ria.sweetandmaxwell.joim.inderscience.nowpublishers.com/article/Download/MAL-018},
  number = {2},
  journaltitle = {Foundations and Trends in Machine Learning},
  urldate = {2015-06-18},
  date = {2011},
  pages = {107--194},
  author = {Shalev-Shwartz, Shai},
  file = {/home/arthur/Dropbox/Zotero/Shalev-Shwartz_2011_Online learning and online convex optimization.pdf;/home/arthur/Zotero/storage/GBWID9H7/MAL-018.html}
}

@article{correa_performance_2007,
  title = {Performance of {{Blind Source Separation Algorithms}} for {{FMRI Analysis}} Using a {{Group ICA Method}}},
  volume = {25},
  issn = {0730-725X},
  url = {http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2358930/},
  abstract = {Independent component analysis (ICA) is a popular blind source separation (BSS) technique that has proven to be promising for the analysis of functional magnetic resonance imaging (fMRI) data. A number of ICA approaches have been used for fMRI data analysis, and even more ICA algorithms exist, however the impact of using different algorithms on the results is largely unexplored. In this paper, we study the performance of four major classes of algorithms for spatial ICA, namely information maximization, maximization of non-gaussianity, joint diagonalization of cross-cumulant matrices, and second-order correlation based methods when they are applied to fMRI data from subjects performing a visuo-motor task. We use a group ICA method to study the variability among different ICA algorithms and propose several analysis techniques to evaluate their performance. We compare how different ICA algorithms estimate activations in expected neuronal areas. The results demonstrate that the ICA algorithms using higher-order statistical information prove to be quite consistent for fMRI data analysis. Infomax, FastICA, and JADE all yield reliable results; each having their strengths in specific areas. EVD, an algorithm using second-order statistics, does not perform reliably for fMRI data. Additionally, for the iterative ICA algorithms, it is important to investigate the variability of the estimates from different runs. We test the consistency of the iterative algorithms, Infomax and FastICA, by running the algorithm a number of times with different initializations and note that they yield consistent results over these multiple runs. Our results greatly improve our confidence in the consistency of ICA for fMRI data analysis.},
  number = {5},
  journaltitle = {Magnetic resonance imaging},
  shortjournal = {Magn Reson Imaging},
  urldate = {2015-09-29},
  date = {2007-06},
  pages = {684-694},
  author = {Correa, Nicolle and Adali, Tülay and Calhoun, Vince D.},
  file = {/home/arthur/Dropbox/Zotero/Correa et al_2007_Performance of Blind Source Separation Algorithms for FMRI Analysis using a.pdf},
  eprinttype = {pmid},
  eprint = {17540281},
  pmcid = {PMC2358930}
}

@article{friedman_regularization_2010,
  title = {Regularization Paths for Generalized Linear Models via Coordinate Descent},
  volume = {33},
  url = {http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2929880/},
  number = {1},
  journaltitle = {Journal of statistical software},
  urldate = {2015-11-04},
  date = {2010},
  pages = {1},
  author = {Friedman, Jerome and Hastie, Trevor and Tibshirani, Rob},
  file = {/home/arthur/Dropbox/Zotero/Friedman et al_2010_Regularization paths for generalized linear models via coordinate descent.pdf}
}

@article{smith_positive-negative_2015,
  langid = {english},
  title = {A Positive-Negative Mode of Population Covariation Links Brain Connectivity, Demographics and Behavior},
  volume = {18},
  issn = {1097-6256},
  url = {http://www.nature.com/neuro/journal/v18/n11/full/nn.4125.html},
  abstract = {We investigated the relationship between individual subjects' functional connectomes and 280 behavioral and demographic measures in a single holistic multivariate analysis relating imaging to non-imaging data from 461 subjects in the Human Connectome Project. We identified one strong mode of population co-variation: subjects were predominantly spread along a single 'positive-negative' axis linking lifestyle, demographic and psychometric measures to each other and to a specific pattern of brain connectivity.},
  number = {11},
  journaltitle = {Nature Neuroscience},
  shortjournal = {Nat Neurosci},
  urldate = {2016-02-29},
  date = {2015-11},
  pages = {1565-1567},
  keywords = {functional magnetic resonance imaging,Intelligence,Network models},
  author = {Smith, Stephen M. and Nichols, Thomas E. and Vidaurre, Diego and Winkler, Anderson M. and Behrens, Timothy E. J. and Glasser, Matthew F. and Ugurbil, Kamil and Barch, Deanna M. and Van Essen, David C. and Miller, Karla L.},
  file = {/home/arthur/Dropbox/Zotero/Smith et al_2015_A positive-negative mode of population covariation links brain connectivity,.pdf;/home/arthur/Zotero/storage/NCWAQXG8/nn.4125.html}
}

@inproceedings{jenatton_proximal_2010,
  title = {Proximal Methods for Sparse Hierarchical Dictionary Learning},
  url = {http://machinelearning.wustl.edu/mlpapers/paper_files/icml2010_JenattonMOB10.pdf},
  booktitle = {Proceedings of the 27th {{International Conference}} on {{Machine Learning}} ({{ICML}}-10)},
  urldate = {2015-03-22},
  date = {2010},
  pages = {487--494},
  author = {Jenatton, Rodolphe and Mairal, Julien and Bach, Francis R. and Obozinski, Guillaume R.},
  file = {/home/arthur/Dropbox/Zotero/Jenatton et al_2010_Proximal methods for sparse hierarchical dictionary learning.pdf}
}

@article{guyon_introduction_2003,
  title = {An Introduction to Variable and Feature Selection},
  volume = {3},
  url = {http://dl.acm.org/citation.cfm?id=944968},
  journaltitle = {The Journal of Machine Learning Research},
  urldate = {2015-06-19},
  date = {2003},
  pages = {1157--1182},
  author = {Guyon, Isabelle and Elisseeff, André},
  file = {/home/arthur/Dropbox/Zotero/Guyon_Elisseeff_2003_An introduction to variable and feature selection.pdf;/home/arthur/Zotero/storage/5228V86B/citation.html}
}

@article{maggioni_multiscale_2014,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1401.5833},
  primaryClass = {math, stat},
  title = {Multiscale {{Dictionary Learning}}: {{Non}}-{{Asymptotic Bounds}} and {{Robustness}}},
  url = {http://arxiv.org/abs/1401.5833},
  shorttitle = {Multiscale {{Dictionary Learning}}},
  abstract = {High-dimensional data sets often exhibit inherently low-dimensional structure. Over the past decade, this empirical fact has motivated researchers to study the detection, measurement, and exploitation of such low-dimensional structure, as well as numerous implications for high-dimensional statistics, machine learning, and signal processing. Manifold learning (where the low-dimensional structure is a manifold) and dictionary learning (where the low-dimensional structure is the set of sparse linear combinations of vectors from a finite dictionary) are two prominent theoretical and computational frameworks in this area and, despite their ostensible distinction, the recently-introduced Geometric Multi-Resolution Analysis (GMRA) provides a robust, computationally efficient, multiscale procedure for simultaneously learning a manifold and a dictionary. In this work, we prove non-asymptotic probabilistic bounds on the approximation error of GMRA for a rich class of underlying models that includes "noisy" manifolds, thus theoretically establishing the robustness of the procedure and confirming empirical observations. In particular, if the data aggregates near a low-dimensional manifold, our results show that the approximation error primarily depends on the intrinsic dimension of the manifold, and is independent of the ambient dimension. Our work thus establishes GMRA as a provably fast algorithm for dictionary learning with approximation and sparsity guarantees. We perform numerical experiments that further confirm our theoretical results.},
  urldate = {2015-05-22},
  date = {2014-01-22},
  keywords = {Mathematics - Statistics Theory},
  author = {Maggioni, Mauro and Minsker, Stanislav and Strawn, Nate},
  file = {/home/arthur/Dropbox/Zotero/Maggioni et al_2014_Multiscale Dictionary Learning.pdf;/home/arthur/Zotero/storage/IZN4A5PN/1401.html}
}

@article{ross_incremental_2008,
  langid = {english},
  title = {Incremental {{Learning}} for {{Robust Visual Tracking}}},
  volume = {77},
  issn = {0920-5691, 1573-1405},
  url = {http://link.springer.com/10.1007/s11263-007-0075-7},
  number = {1-3},
  journaltitle = {International Journal of Computer Vision},
  urldate = {2015-07-03},
  date = {2008-05},
  pages = {125-141},
  author = {Ross, David A. and Lim, Jongwoo and Lin, Ruei-Sung and Yang, Ming-Hsuan},
  file = {/home/arthur/Dropbox/Zotero/Ross et al_2008_Incremental Learning for Robust Visual Tracking.pdf}
}

@article{gangeh_supervised_2015,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1502.05928},
  primaryClass = {cs},
  title = {Supervised {{Dictionary Learning}} and {{Sparse Representation}}-{{A Review}}},
  url = {http://arxiv.org/abs/1502.05928},
  abstract = {Dictionary learning and sparse representation (DLSR) is a recent and successful mathematical model for data representation that achieves state-of-the-art performance in various fields such as pattern recognition, machine learning, computer vision, and medical imaging. The original formulation for DLSR is based on the minimization of the reconstruction error between the original signal and its sparse representation in the space of the learned dictionary. Although this formulation is optimal for solving problems such as denoising, inpainting, and coding, it may not lead to optimal solution in classification tasks, where the ultimate goal is to make the learned dictionary and corresponding sparse representation as discriminative as possible. This motivated the emergence of a new category of techniques, which is appropriately called supervised dictionary learning and sparse representation (S-DLSR), leading to more optimal dictionary and sparse representation in classification tasks. Despite many research efforts for S-DLSR, the literature lacks a comprehensive view of these techniques, their connections, advantages and shortcomings. In this paper, we address this gap and provide a review of the recently proposed algorithms for S-DLSR. We first present a taxonomy of these algorithms into six categories based on the approach taken to include label information into the learning of the dictionary and/or sparse representation. For each category, we draw connections between the algorithms in this category and present a unified framework for them. We then provide guidelines for applied researchers on how to represent and learn the building blocks of an S-DLSR solution based on the problem at hand. This review provides a broad, yet deep, view of the state-of-the-art methods for S-DLSR and allows for the advancement of research and development in this emerging area of research.},
  urldate = {2015-05-21},
  date = {2015-02-20},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  author = {Gangeh, Mehrdad J. and Farahat, Ahmed K. and Ghodsi, Ali and Kamel, Mohamed S.},
  file = {/home/arthur/Dropbox/Zotero/Gangeh et al_2015_Supervised Dictionary Learning and Sparse Representation-A Review.pdf;/home/arthur/Zotero/storage/NZ8D2QXH/1502.html}
}

@article{tibshirani_degrees_2012,
  title = {Degrees of Freedom in Lasso Problems},
  volume = {40},
  url = {http://projecteuclid.org/euclid.aos/1342625466},
  number = {2},
  journaltitle = {The Annals of Statistics},
  urldate = {2015-05-21},
  date = {2012},
  pages = {1198--1232},
  author = {Tibshirani, Ryan J. and Taylor, Jonathan and {others}},
  file = {/home/arthur/Dropbox/Zotero/Tibshirani et al_2012_Degrees of freedom in lasso problems.pdf;/home/arthur/Zotero/storage/XU3EED84/1342625466.html}
}

@article{xiang_screening_2014,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1405.4897},
  primaryClass = {cs, stat},
  title = {Screening {{Tests}} for {{Lasso Problems}}},
  url = {http://arxiv.org/abs/1405.4897},
  abstract = {This paper is a survey of dictionary screening for the lasso problem. The lasso problem seeks a sparse linear combination of the columns of a dictionary to best match a given target vector. This sparse representation has proven useful in a variety of subsequent processing and decision tasks. For a given target vector, dictionary screening quickly identifies a subset of dictionary columns that will receive zero weight in a solution of the corresponding lasso problem. These columns can be removed from the dictionary, prior to solving the lasso problem, without impacting the optimality of the solution obtained. This has two potential advantages: it reduces the size of the dictionary, allowing the lasso problem to be solved with less resources, and it may speed up obtaining a solution. Using a geometrically intuitive framework, we provide basic insights for understanding useful lasso screening tests and their limitations. We also provide illustrative numerical studies on several datasets.},
  urldate = {2015-06-30},
  date = {2014-05-19},
  keywords = {Computer Science - Learning,Statistics - Machine Learning},
  author = {Xiang, Zhen James and Wang, Yun and Ramadge, Peter J.},
  file = {/home/arthur/Dropbox/Zotero/Xiang et al_2014_Screening Tests for Lasso Problems.pdf;/home/arthur/Zotero/storage/7Z7HAAIR/1405.html}
}

@article{levine_end--end_2015,
  title = {End-to-{{End Training}} of {{Deep Visuomotor Policies}}},
  url = {http://arxiv.org/abs/1504.00702},
  journaltitle = {arXiv preprint arXiv:1504.00702},
  urldate = {2015-05-22},
  date = {2015},
  author = {Levine, Sergey and Finn, Chelsea and Darrell, Trevor and Abbeel, Pieter},
  file = {/home/arthur/Dropbox/Zotero/Levine et al_2015_End-to-End Training of Deep Visuomotor Policies.pdf}
}

@article{himberg_validating_2004,
  title = {Validating the Independent Components of Neuroimaging Time Series via Clustering and Visualization},
  volume = {22},
  url = {http://www.sciencedirect.com/science/article/pii/S1053811904001661},
  number = {3},
  journaltitle = {Neuroimage},
  urldate = {2015-10-12},
  date = {2004},
  pages = {1214--1222},
  keywords = {ICASSO},
  author = {Himberg, Johan and Hyvärinen, Aapo and Esposito, Fabrizio},
  file = {/home/arthur/Dropbox/Zotero/Himberg et al_2004_Validating the independent components of neuroimaging time series via.pdf}
}

@article{zhao_block_2015,
  title = {A Block Coordinate Descent Approach for Sparse Principal Component Analysis},
  volume = {153},
  issn = {0925-2312},
  url = {http://www.sciencedirect.com/science/article/pii/S0925231214015719},
  abstract = {There are mainly two methodologies utilized in current sparse PCA calculation, the greedy approach and the block approach. While the greedy approach tends to be incrementally invalidated in sequentially generating sparse PCs due to the cumulation of computational errors, the block approach is difficult to elaborately rectify individual sparse PCs under certain practical sparsity or nonnegative constraints. In this paper, a simple while effective block coordinate descent (BCD) method is proposed for solving the sparse PCA problem. The main idea is to separate the original sparse PCA problem into a series of simple sub-problems, each having a closed-form solution. By cyclically solving these sub-problems in an analytical way, the BCD algorithm can be easily constructed. Despite its simplicity, the proposed method performs surprisingly well in extensive experiments implemented on a series of synthetic and real data. In specific, as compared to the greedy approach, the proposed method can iteratively ameliorate the deviation errors of all computed sparse PCs and avoid the problem of accumulating errors; as compared to the block approach, the proposed method can easily handle the constraints imposed on each individual sparse PC, such as certain sparsity and/or nonnegativity constraints. Besides, the proposed method converges to a stationary point of the problem, and its computational complexity is approximately linear in both data size and dimensionality, which makes it well suited to handle large-scale problems of sparse PCA.},
  journaltitle = {Neurocomputing},
  shortjournal = {Neurocomputing},
  urldate = {2015-06-17},
  date = {2015-04-04},
  pages = {180-190},
  keywords = {Block coordinate descent,Nonnegativity,Principal Component Analysis,Sparsity},
  author = {Zhao, Qian and Meng, Deyu and Xu, Zongben and Gao, Chenqiang},
  file = {/home/arthur/Dropbox/Zotero/Zhao et al_2015_A block coordinate descent approach for sparse principal component analysis.pdf;/home/arthur/Zotero/storage/3WVIPK98/S0925231214015719.html}
}

@article{fercoq_mind_2015,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1505.03410},
  primaryClass = {cs, math, stat},
  title = {Mind the Duality Gap: Safer Rules for the {{Lasso}}},
  url = {http://arxiv.org/abs/1505.03410},
  shorttitle = {Mind the Duality Gap},
  abstract = {Screening rules allow to early discard irrelevant variables from the optimization in Lasso problems, or its derivatives, making solvers faster. In this paper, we propose new versions of the so-called \$$\backslash$textit\{safe rules\}\$ for the Lasso. Based on duality gap considerations, our new rules create safe test regions whose diameters converge to zero, provided that one relies on a converging solver. This property helps screening out more variables, for a wider range of regularization parameter values. In addition to faster convergence, we prove that we correctly identify the active sets (supports) of the solutions in finite time. While our proposed strategy can cope with any solver, its performance is demonstrated using a coordinate descent algorithm particularly adapted to machine learning use cases. Significant computing time reductions are obtained with respect to previous safe rules.},
  urldate = {2015-06-30},
  date = {2015-05-13},
  keywords = {68Uxx; 49N15; 62Jxx; 68Q32; 62-04,Computer Science - Learning,Mathematics - Optimization and Control,Statistics - Computation,Statistics - Machine Learning},
  author = {Fercoq, Olivier and Gramfort, Alexandre and Salmon, Joseph},
  file = {/home/arthur/Dropbox/Zotero/Fercoq et al_2015_Mind the duality gap.pdf;/home/arthur/Zotero/storage/NERII84U/1505.html}
}

@incollection{dhillon_new_2013,
  title = {New {{Subsampling Algorithms}} for {{Fast Least Squares Regression}}},
  url = {http://papers.nips.cc/paper/5105-new-subsampling-algorithms-for-fast-least-squares-regression.pdf},
  booktitle = {Advances in {{Neural Information Processing Systems}} 26},
  publisher = {{Curran Associates, Inc.}},
  urldate = {2016-01-21},
  date = {2013},
  pages = {360--368},
  author = {Dhillon, Paramveer and Lu, Yichao and Foster, Dean P and Ungar, Lyle},
  editor = {Burges, C. J. C. and Bottou, L. and Welling, M. and Ghahramani, Z. and Weinberger, K. Q.},
  file = {/home/arthur/Dropbox/Zotero/Dhillon et al_2013_New Subsampling Algorithms for Fast Least Squares Regression.pdf;/home/arthur/Zotero/storage/JSPG832Q/5105-new-subsampling-algorithms-for-fast-least-squares-regression.html}
}

@article{vershynin_introduction_2010,
  title = {Introduction to the Non-Asymptotic Analysis of Random Matrices},
  url = {http://arxiv.org/abs/1011.3027},
  journaltitle = {arXiv preprint arXiv:1011.3027},
  urldate = {2016-01-21},
  date = {2010},
  author = {Vershynin, Roman},
  file = {/home/arthur/Dropbox/Zotero/Vershynin_2010_Introduction to the non-asymptotic analysis of random matrices.pdf}
}

@inproceedings{rendle_online-updating_2008,
  title = {Online-Updating Regularized Kernel Matrix Factorization Models for Large-Scale Recommender Systems},
  url = {t},
  booktitle = {Proceedings of the {{ACM Conference}} on {{Recommender}} Systems},
  publisher = {{ACM}},
  urldate = {2016-01-21},
  date = {2008},
  pages = {251--258},
  author = {Rendle, Steffen and Schmidt-Thieme, Lars},
  file = {/home/arthur/Dropbox/Zotero/Rendle_Schmidt-Thieme_2008_Online-updating regularized kernel matrix factorization models for large-scale.pdf}
}

@article{slavakis_stochastic_2014,
  title = {Stochastic {{Approximation}} Vis-a-Vis {{Online Learning}} for {{Big Data Analytics}} [{{Lecture Notes}}]},
  volume = {31},
  issn = {1053-5888},
  doi = {10.1109/MSP.2014.2345536},
  abstract = {We live in an era of data deluge, where data translate to knowledge and can thus contribute in various directions if harnessed and processed intelligently. There is no doubt that signal processing (SP) is of uttermost relevance to timely big data applications such as real-time medical imaging, smart cities, network state visualization and anomaly detection (e.g., in the power grid and the Internet), health informatics for personalized treatment, sentiment analysis from online social media, Web-based advertising, recommendation systems, sensor-empowered structural health monitoring, and e-commerce fraud detection, just to name a few. Accordingly, abundant chances unfold to SP researchers and practitioners for fundamental contributions in big data theory and practice.},
  number = {6},
  journaltitle = {IEEE Signal Processing Magazine},
  date = {2014-11},
  pages = {124-129},
  keywords = {Approximation methods,learning (artificial intelligence),machine learning,approximation theory,stochastic processes,stochastic approximation,Big Data,data analysis,signal processing,stochastic programming,SP researchers,big data analytics,big data theory,data deluge,online learning,Algorithm design and analysis,Knowledge management,Learning systems,Online services,Signal processing algorithms,Stochastic methods},
  author = {Slavakis, K. and Kim, S. J. and Mateos, G. and Giannakis, G. B.},
  file = {/home/arthur/Dropbox/Zotero/Slavakis et al_2014_Stochastic Approximation vis-a-vis Online Learning for Big Data Analytics.pdf;/home/arthur/Zotero/storage/SUAA82ZX/abs_all.html}
}

@inproceedings{slavakis_online_2014,
  title = {Online Dictionary Learning from Big Data Using Accelerated Stochastic Approximation Algorithms},
  doi = {10.1109/ICASSP.2014.6853549},
  abstract = {Applications involving large-scale dictionary learning tasks motivate well online optimization algorithms for generally non-convex and non-smooth problems. In this big data context, the present paper develops an online learning framework by jointly leveraging the stochastic approximation paradigm with first-order acceleration schemes. The generally non-convex objective evaluated online at the resultant iterates enjoys quadratic rate of convergence. The generality of the novel approach is demonstrated in two online learning applications: (i) Online linear regression using the total least-squares approach; and, (ii) a semi-supervised dictionary learning approach to network-wide link load tracking and imputation of real data with missing entries. In both cases, numerical tests highlight the potential of the proposed online framework for big data network analytics.},
  eventtitle = {2014 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  booktitle = {2014 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  date = {2014-05},
  pages = {16-20},
  keywords = {Optimization,Vectors,dictionaries,learning (artificial intelligence),Convergence,Big Data,Signal processing algorithms,Internet,big data network analytics,first-order acceleration schemes,large-scale dictionary learning tasks,network-wide link load tracking,nonconvex objective,nonconvex problems,nonsmooth problems,numerical tests,online dictionary learning framework,online linear regression,online optimization algorithms,semisupervised dictionary learning,stochastic approximation algorithms,stochastic approximation paradigm,Acceleration},
  author = {Slavakis, K. and Giannakis, G.},
  file = {/home/arthur/Dropbox/Zotero/Slavakis_Giannakis_2014_Online dictionary learning from big data using accelerated stochastic.pdf;/home/arthur/Zotero/storage/BE9DR7ZN/abs_all.html}
}

@book{bertsekas_nonlinear_1999,
  title = {Nonlinear {{Programming}}},
  isbn = {978-1-886529-00-7},
  url = {http://www.amazon.ca/exec/obidos/redirect?tag=citeulike09-20&path=ASIN/1886529000},
  abstract = {\{This extensive rigorous texbook, developed through instruction at MIT, focuses on nonlinear and other types of optimization: iterative algorithms for constrained and unconstrained optimization, Lagrange multipliers and duality, large scale problems, and the interface between continuous and discrete optimization.   Among its special features, the book:  1) provides extensive coverage of iterative optimization methods within a unifying framework  2) provides a detailed treatment of interior point methods for linear programming  3) covers in depth duality theory from both a variational and a geometrical/convex analysis point of view  4) includes much new material on a number of topics, such as neural network training, discrete-time optimal control, and large-scale optimization  5) includes a large number of examples and exercises detailed solutions of many of which are posted on the internet  Much supplementary/support material can be found at the book's web page  http://www.athenasc.com/nonlinbook.html\}},
  publisher = {{Athena Scientific}},
  urldate = {2016-01-27},
  date = {1999-09-01},
  keywords = {Optimization,nonlinear-programming},
  author = {Bertsekas, Dimitri},
  file = {/home/arthur/Dropbox/Zotero/Bertsekas_1999_Nonlinear Programming.pdf}
}

@article{takacs_scalable_2009,
  title = {Scalable Collaborative Filtering Approaches for Large Recommender Systems},
  volume = {10},
  url = {http://dl.acm.org/citation.cfm?id=1577091},
  journaltitle = {The Journal of Machine Learning Research},
  urldate = {2016-01-30},
  date = {2009},
  pages = {623--656},
  author = {Takács, Gábor and Pilászy, István and Németh, Bottyán and Tikk, Domonkos},
  file = {/home/arthur/Dropbox/Zotero/Takács et al_2009_Scalable collaborative filtering approaches for large recommender systems.pdf;/home/arthur/Zotero/storage/9EX6N5ZF/citation.html}
}

@article{schmidt_minimizing_2013,
  title = {Minimizing Finite Sums with the Stochastic Average Gradient},
  url = {http://arxiv.org/abs/1309.2388},
  journaltitle = {arXiv preprint arXiv:1309.2388},
  urldate = {2015-11-16},
  date = {2013},
  author = {Schmidt, Mark and Roux, Nicolas Le and Bach, Francis},
  file = {/home/arthur/Dropbox/Zotero/Schmidt et al_2013_Minimizing finite sums with the stochastic average gradient.pdf;/home/arthur/Zotero/storage/IZ9TFRHZ/1309.html}
}

@article{drineas_nystrom_2005,
  title = {On the {{Nyström}} Method for Approximating a {{Gram}} Matrix for Improved Kernel-Based Learning},
  volume = {6},
  url = {http://dl.acm.org/citation.cfm?id=1194916},
  journaltitle = {The Journal of Machine Learning Research},
  urldate = {2015-11-23},
  date = {2005},
  pages = {2153--2175},
  author = {Drineas, Petros and Mahoney, Michael W.},
  file = {/home/arthur/Dropbox/Zotero/Drineas_Mahoney_2005_On the Nyström method for approximating a Gram matrix for improved kernel-based.pdf;/home/arthur/Zotero/storage/2Z3IB9FN/citation.html}
}

@inproceedings{rendle_factorization_2010,
  title = {Factorization Machines},
  url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5694074},
  booktitle = {Proceedings of the {{IEEE International Conference}} on {{Data Mining}}},
  publisher = {{IEEE}},
  urldate = {2015-11-25},
  date = {2010},
  pages = {995--1000},
  author = {Rendle, Steffen},
  file = {/home/arthur/Dropbox/Zotero/Rendle_2010_Factorization machines.pdf;/home/arthur/Zotero/storage/973A94FJ/login.html}
}

@article{rendle_factorization_2012,
  title = {Factorization {{Machines}} with {{libFM}}},
  volume = {3},
  issn = {2157-6904},
  url = {http://doi.acm.org/10.1145/2168752.2168771},
  abstract = {Factorization approaches provide high accuracy in several important prediction problems, for example, recommender systems. However, applying factorization approaches to a new prediction problem is a nontrivial task and requires a lot of expert knowledge. Typically, a new model is developed, a learning algorithm is derived, and the approach has to be implemented. Factorization machines (FM) are a generic approach since they can mimic most factorization models just by feature engineering. This way, factorization machines combine the generality of feature engineering with the superiority of factorization models in estimating interactions between categorical variables of large domain. libFM is a software implementation for factorization machines that features stochastic gradient descent (SGD) and alternating least-squares (ALS) optimization, as well as Bayesian inference using Markov Chain Monto Carlo (MCMC). This article summarizes the recent research on factorization machines both in terms of modeling and learning, provides extensions for the ALS and MCMC algorithms, and describes the software tool libFM.},
  number = {3},
  journaltitle = {ACM Trans. Intell. Syst. Technol.},
  urldate = {2015-11-26},
  date = {2012-05},
  pages = {57:1--57:22},
  keywords = {Factorization model,collaborative filtering,factorization machine,matrix factorization,recommender system,tensor factorization},
  author = {Rendle, Steffen},
  file = {/home/arthur/Dropbox/Zotero/Rendle_2012_Factorization Machines with libFM.pdf}
}

@article{arlot_survey_2010,
  langid = {english},
  title = {A Survey of Cross-Validation Procedures for Model Selection},
  volume = {4},
  issn = {1935-7516},
  url = {http://projecteuclid.org/euclid.ssu/1268143839},
  abstract = {Used to estimate the risk of an estimator or to perform model selection, cross-validation is a widespread strategy because of its simplicity and its (apparent) universality. Many results exist on model selection performances of cross-validation procedures. This survey intends to relate these results to the most recent advances of model selection theory, with a particular emphasis on distinguishing empirical statements from rigorous theoretical results. As a conclusion, guidelines are provided for choosing the best cross-validation procedure according to the particular features of the problem in hand.},
  journaltitle = {Statistics Surveys},
  shortjournal = {Statist. Surv.},
  urldate = {2015-12-01},
  date = {2010},
  pages = {40-79},
  keywords = {model selection,cross-validation,leave-one-out},
  author = {Arlot, Sylvain and Celisse, Alain},
  file = {/home/arthur/Zotero/storage/F99PCG4W/1268143839.html}
}

@article{choromanska_loss_2014,
  title = {The Loss Surface of Multilayer Networks},
  url = {http://arxiv.org/abs/1412.0233},
  journaltitle = {arXiv preprint arXiv:1412.0233},
  urldate = {2015-12-03},
  date = {2014},
  author = {Choromanska, Anna and Henaff, Mikael and Mathieu, Michael and Arous, Gérard Ben and LeCun, Yann},
  file = {/home/arthur/Dropbox/Zotero/Choromanska et al_2014_The loss surface of multilayer networks.pdf;/home/arthur/Zotero/storage/54DX8PGI/1412.html}
}

@incollection{blondel_convex_2015,
  title = {Convex Factorization Machines},
  url = {http://link.springer.com/chapter/10.1007/978-3-319-23525-7_2},
  booktitle = {Machine {{Learning}} and {{Knowledge Discovery}} in {{Databases}}},
  publisher = {{Springer}},
  urldate = {2015-12-01},
  date = {2015},
  pages = {19--35},
  author = {Blondel, Mathieu and Fujino, Akinori and Ueda, Naonori},
  file = {/home/arthur/Dropbox/Zotero/Blondel et al_2015_Convex factorization machines.pdf}
}

@article{johnson_extensions_1984,
  title = {Extensions of {{Lipschitz}} Mappings into a {{Hilbert}} Space},
  volume = {26},
  url = {https://www.researchgate.net/profile/William_Johnson16/publication/235008656_Extensions_of_Lipschitz_maps_into_a_Hilbert_space/links/55e9abf908aeb65162649527.pdf},
  number = {189-206},
  journaltitle = {Contemporary mathematics},
  urldate = {2016-02-01},
  date = {1984},
  pages = {1},
  author = {Johnson, William B. and Lindenstrauss, Joram},
  file = {/home/arthur/Dropbox/Zotero/Johnson_Lindenstrauss_1984_Extensions of Lipschitz mappings into a Hilbert space.pdf}
}

@article{bolte_proximal_2014,
  title = {Proximal Alternating Linearized Minimization for Nonconvex and Nonsmooth Problems},
  volume = {146},
  url = {http://link.springer.com/article/10.1007/s10107-013-0701-9},
  number = {1-2},
  journaltitle = {Mathematical Programming},
  urldate = {2015-12-07},
  date = {2014},
  pages = {459--494},
  author = {Bolte, Jérôme and Sabach, Shoham and Teboulle, Marc},
  file = {/home/arthur/Dropbox/Zotero/Bolte et al_2014_Proximal alternating linearized minimization for nonconvex and nonsmooth.pdf;/home/arthur/Zotero/storage/USUWXAM2/s10107-013-0701-9.html}
}

@article{hesse_proximal_2014,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1408.1887},
  primaryClass = {math},
  title = {Proximal {{Heterogeneous Block Input}}-{{Output Method}} and Application to {{Blind Ptychographic Diffraction Imaging}}},
  url = {http://arxiv.org/abs/1408.1887},
  abstract = {We propose a general alternating minimization algorithm for nonconvex optimization problems with separable structure and nonconvex coupling between blocks of variables. To fix our ideas, we apply the methodology to the problem of blind ptychographic imaging. Compared to other schemes in the literature, our approach differs in two ways: (i) it is posed within a clear mathematical framework with practically verifiable assumptions, and (ii) under the given assumptions, it is provably convergent to critical points. A numerical comparison of our proposed algorithm with the current state-of-the-art on simulated and experimental data validates our approach and points toward directions for further improvement.},
  urldate = {2015-12-07},
  date = {2014-08-08},
  keywords = {Mathematics - Numerical Analysis,Mathematics - Optimization and Control,90C05; 90C25; 90C30; 90C52; 65K05},
  author = {Hesse, Robert and Luke, D. Russell and Sabach, Shoham and Tam, Matthew K.},
  file = {/home/arthur/Dropbox/Zotero/Hesse et al_2014_Proximal Heterogeneous Block Input-Output Method and application to Blind.pdf;/home/arthur/Zotero/storage/7Z8ZZ8RP/1408.html}
}

@article{parikh_proximal_2013,
  title = {Proximal Algorithms},
  volume = {1},
  url = {http://www.ebookcs.com/download/zdm9Z3/proximal-algorithms-stanford-university.pdf},
  number = {3},
  journaltitle = {Foundations and Trends in optimization},
  urldate = {2015-12-07},
  date = {2013},
  pages = {123--231},
  author = {Parikh, Neal and Boyd, Stephen},
  file = {/home/arthur/Dropbox/Zotero/Parikh_Boyd_2013_Proximal algorithms.pdf}
}

@incollection{schmidt_convergence_2011,
  title = {Convergence {{Rates}} of {{Inexact Proximal}}-{{Gradient Methods}} for {{Convex Optimization}}},
  url = {http://papers.nips.cc/paper/4452-convergence-rates-of-inexact-proximal-gradient-methods-for-convex-optimization.pdf},
  booktitle = {Advances in {{Neural Information Processing Systems}} 24},
  publisher = {{Curran Associates, Inc.}},
  urldate = {2015-12-07},
  date = {2011},
  pages = {1458--1466},
  author = {Schmidt, Mark and Roux, Nicolas L. and Bach, Francis R.},
  editor = {Shawe-Taylor, J. and Zemel, R. S. and Bartlett, P. L. and Pereira, F. and Weinberger, K. Q.},
  file = {/home/arthur/Dropbox/Zotero/Schmidt et al_2011_Convergence Rates of Inexact Proximal-Gradient Methods for Convex Optimization.pdf;/home/arthur/Zotero/storage/N2D8HX33/4452-convergence-rates-of-inexact-proximal-gradient-methods-for-convex-optimization.html}
}

@inproceedings{bingham_random_2001,
  title = {Random Projection in Dimensionality Reduction: Applications to Image and Text Data},
  booktitle = {Proceedings of {{ACM SIGKDD International Conference}} on {{Knowledge Discovery}} and {{Data Mining}}},
  publisher = {{ACM}},
  date = {2001},
  pages = {245--250},
  author = {Bingham, Ella and Mannila, Heikki},
  file = {/home/arthur/Dropbox/Zotero/Bingham_Mannila_2001_Random projection in dimensionality reduction.pdf}
}

@inproceedings{pourkamali-anaraki_efficient_2015,
  title = {Efficient Dictionary Learning via Very Sparse Random Projections},
  url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=7148937},
  booktitle = {Proceedings of the {{IEEE International Conference}} on {{Sampling Theory}} and {{Applications}}},
  publisher = {{IEEE}},
  urldate = {2016-02-01},
  date = {2015},
  pages = {478--482},
  author = {Pourkamali-Anaraki, Farhad and Becker, Stephen and Hughes, Shannon M.},
  file = {/home/arthur/Dropbox/Zotero/Pourkamali-Anaraki et al_2015_Efficient dictionary learning via very sparse random projections.pdf}
}

@article{bach_convex_2008,
  title = {Convex Sparse Matrix Factorizations},
  url = {http://arxiv.org/abs/0812.1869},
  journaltitle = {arXiv preprint arXiv:0812.1869},
  urldate = {2015-12-09},
  date = {2008},
  author = {Bach, Francis and Mairal, Julien and Ponce, Jean},
  file = {/home/arthur/Dropbox/Zotero/Bach et al_2008_Convex sparse matrix factorizations.pdf}
}

@inproceedings{mairal_stochastic_2013,
  title = {Stochastic Majorization-Minimization Algorithms for Large-Scale Optimization},
  url = {http://papers.nips.cc/paper/5129-stochastic-majorization-minimization-algorithms-for-large-scale-optimization},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  urldate = {2015-12-09},
  date = {2013},
  pages = {2283--2291},
  author = {Mairal, Julien},
  file = {/home/arthur/Dropbox/Zotero/Mairal_2013_Stochastic majorization-minimization algorithms for large-scale optimization.pdf}
}

@article{candes_exact_2009,
  langid = {english},
  title = {Exact Matrix Completion via Convex Optimization},
  volume = {9},
  number = {6},
  journaltitle = {Foundations of Computational Mathematics},
  date = {2009},
  pages = {717-772},
  author = {Candès, Emmanuel J. and Recht, Benjamin},
  file = {/home/arthur/Dropbox/Zotero/Candès_Recht_2009_Exact matrix completion via convex optimization.pdf}
}

@article{haitao_zhao_novel_2006,
  title = {A Novel Incremental Principal Component Analysis and Its Application for Face Recognition},
  volume = {36},
  issn = {1083-4419},
  url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=1658299},
  number = {4},
  journaltitle = {IEEE Transactions on Systems, Man and Cybernetics, Part B (Cybernetics)},
  urldate = {2016-02-02},
  date = {2006-08},
  pages = {873-886},
  author = {{Haitao Zhao} and {Pong Chi Yuen} and Kwok, J.T.},
  file = {/home/arthur/Dropbox/Zotero/Haitao Zhao et al_2006_A novel incremental principal component analysis and its application for face.pdf}
}

@article{cands_stable_2006,
  title = {Stable Signal Recovery from Incomplete and Inaccurate Measurements},
  volume = {59},
  url = {http://onlinelibrary.wiley.com/doi/10.1002/cpa.20124/full},
  number = {8},
  journaltitle = {Communications on pure and applied mathematics},
  urldate = {2016-02-02},
  date = {2006},
  pages = {1207--1223},
  author = {Cands, Emmanuel J. and Romberg, Justin K. and Tao, Terence},
  file = {/home/arthur/Dropbox/Zotero/Cands et al_2006_Stable signal recovery from incomplete and inaccurate measurements.pdf;/home/arthur/Zotero/storage/UE8H9F3E/full.html}
}

@article{bianchi_performance_2013,
  title = {Performance of a {{Distributed Stochastic Approximation Algorithm}}},
  volume = {59},
  issn = {0018-9448},
  doi = {10.1109/TIT.2013.2275131},
  abstract = {In this paper, a distributed stochastic approximation algorithm is studied. Applications of such algorithms include decentralized estimation, optimization, control or computing. The algorithm consists in two steps: a local step, where each node in a network updates a local estimate using a stochastic approximation algorithm with decreasing step size, and a gossip step, where a node computes a local weighted average between its estimates and those of its neighbors. Convergence of the estimates toward a consensus is established under weak assumptions. The approach relies on two main ingredients: the existence of a Lyapunov function for the mean field in the agreement subspace, and a contraction property of the random matrices of weights in the subspace orthogonal to the agreement subspace. A second-order analysis of the algorithm is also performed under the form of a central limit Theorem. The Polyak-averaged version of the algorithm is also considered.},
  number = {11},
  journaltitle = {IEEE Transactions on Information Theory},
  date = {2013-11},
  pages = {7405-7418},
  keywords = {Approximation algorithms,Approximation methods,Optimization,Lyapunov methods,approximation theory,convergence of numerical methods,distributed algorithms,matrix algebra,stochastic processes,Lyapunov function,Polyak-averaged version,central limit theorem,contraction property,decentralized estimation,distributed stochastic approximation algorithm,gossip step,local weighted average,random weights matrices,second-order analysis,weak assumptions,Context,Convergence,Estimation,decentralized optimization,gossip algorithms,stochastic approximation},
  author = {Bianchi, P. and Fort, G. and Hachem, W.},
  file = {/home/arthur/Dropbox/Zotero/Bianchi et al_2013_Performance of a Distributed Stochastic Approximation Algorithm.pdf;/home/arthur/Zotero/storage/3KFUV8HF/abs_all.html}
}

@article{roulet_renegars_2015,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1506.03295},
  primaryClass = {math},
  title = {Renegar's {{Condition Number}} and {{Compressed Sensing Performance}}},
  url = {http://arxiv.org/abs/1506.03295},
  abstract = {Renegar's condition number is a data-driven computational complexity measure for convex programs, generalizing classical condition numbers in linear systems. We provide evidence that for a broad class of compressed sensing problems, the worst case value of this algorithmic complexity measure taken over all signals matches the restricted eigenvalue of the observation matrix, which controls compressed sensing performance. This means that, in these problems, a single parameter directly controls computational complexity and recovery performance.},
  urldate = {2016-03-17},
  date = {2015-06-10},
  keywords = {Mathematics - Optimization and Control,68U10; 49K40; 90C25},
  author = {Roulet, Vincent and Boumal, Nicolas and d' Aspremont, Alexandre},
  options = {useprefix=true},
  file = {/home/arthur/Dropbox/Zotero/Roulet et al_2015_Renegar's Condition Number and Compressed Sensing Performance.pdf;/home/arthur/Zotero/storage/RR35GCA9/1506.html}
}

@article{bonettini_cyclic_2015,
  title = {A Cyclic Block Coordinate Descent Method with Generalized Gradient Projections},
  url = {http://arxiv.org/abs/1502.06737},
  journaltitle = {arXiv preprint arXiv:1502.06737},
  urldate = {2016-02-09},
  date = {2015},
  author = {Bonettini, Silvia and Prato, Marco and Rebegoldi, Simone},
  file = {/home/arthur/Dropbox/Zotero/Bonettini et al_2015_A cyclic block coordinate descent method with generalized gradient projections.pdf}
}

@unpublished{bach_stochastic_2012,
  title = {Stochastic Gradient Methods for Machine Learning},
  url = {http://www.ann.jussieu.fr/~plc/bach2012.pdf},
  urldate = {2016-01-20},
  date = {2012},
  author = {Bach, Francis},
  file = {/home/arthur/Dropbox/Zotero/Bach_2012_Stochastic gradient methods for machine learning.pdf}
}

@article{bianchi_coordinate_2014,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1407.0898},
  primaryClass = {cs, math},
  title = {A {{Coordinate Descent Primal}}-{{Dual Algorithm}} and {{Application}} to {{Distributed Asynchronous Optimization}}},
  url = {http://arxiv.org/abs/1407.0898},
  abstract = {Based on the idea of randomized coordinate descent of \$$\backslash$alpha\$-averaged operators, a randomized primal-dual optimization algorithm is introduced, where a random subset of coordinates is updated at each iteration. The algorithm builds upon a variant of a recent (deterministic) algorithm proposed by V$\backslash$\textasciitilde{}u and Condat that includes the well known ADMM as a particular case. The obtained algorithm is used to solve asynchronously a distributed optimization problem. A network of agents, each having a separate cost function containing a differentiable term, seek to find a consensus on the minimum of the aggregate objective. The method yields an algorithm where at each iteration, a random subset of agents wake up, update their local estimates, exchange some data with their neighbors, and go idle. Numerical results demonstrate the attractive performance of the method. The general approach can be naturally adapted to other situations where coordinate descent convex optimization algorithms are used with a random choice of the coordinates.},
  urldate = {2016-02-09},
  date = {2014-07-03},
  keywords = {Mathematics - Optimization and Control,Computer Science - Distributed; Parallel; and Cluster Computing,Computer Science - Numerical Analysis,Computer Science - Systems and Control},
  author = {Bianchi, Pascal and Hachem, Walid and Iutzeler, Franck},
  file = {/home/arthur/Dropbox/Zotero/Bianchi et al_2014_A Coordinate Descent Primal-Dual Algorithm and Application to Distributed.pdf;/home/arthur/Zotero/storage/5ECK5T6U/1407.html}
}

@article{bianchi_dynamical_2015,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1508.02845},
  primaryClass = {math},
  title = {Dynamical Behavior of a Stochastic Forward-Backward Algorithm Using Random Monotone Operators},
  url = {http://arxiv.org/abs/1508.02845},
  abstract = {The purpose of this paper is to study the dynamical behavior of the sequence \$(x\_n)\$ produced by the forward-backward algorithm \$y\_\{n+1\} $\backslash$in B(u\_\{n+1\}, x\_n)\$, \$x\_\{n+1\} = ( I + $\backslash$gamma\_\{n+1\} A(u\_\{n+1\}, $\backslash$cdot))\^\{-1\}( x\_n - $\backslash$gamma\_\{n+1\} y\_\{n+1\} )\$ where \$A($\backslash$xi) = A($\backslash$xi, $\backslash$cdot)\$ and \$B($\backslash$xi) = B($\backslash$xi, $\backslash$cdot)\$ are two functions valued in the set of maximal monotone operators on \$$\backslash$mathbb\{R\}\^N\$, \$(u\_n)\$ is a sequence of independent and identically distributed random variables, and \$($\backslash$gamma\_n)\$ is a sequence of vanishing step sizes. Following the approach of the recent paper\textasciitilde$\backslash$cite\{bia-(arxiv)15\}, we define the operators \$\{$\backslash$mathcal A\}(x) = \{$\backslash$mathbb E\}[ A(u\_1, x) ]\$ and \$\{$\backslash$mathcal B\}(x) = \{$\backslash$mathbb E\} [ B(u\_1, x)]\$, where the expectations are the set-valued Aumann integrals with respect to the law of \$u\_1\$, and assume that the monotone operator \$\{$\backslash$mathcal A\} + \{$\backslash$mathcal B\}\$ is maximal (sufficient conditions for maximality are provided). It is shown that with probability one, the interpolated process obtained from the iterates \$x\_n\$ is an asymptotic pseudo trajectory in the sense of Bena$\backslash$"\{$\backslash$i\}m and Hirsch of the differential inclusion \$$\backslash$dot z(t) $\backslash$in - (\{$\backslash$mathcal A\} + \{$\backslash$mathcal B\})(z(t))\$. The convergence of the empirical means of the \$x\_n\$'s towards a zero of \$\{$\backslash$mathcal A\} + \{$\backslash$mathcal B\}\$ follows, as well as the convergence of the sequence \$(x\_n)\$ itself to such a zero under a demipositivity assumption. These results find applications in a wide range of optimization or variational inequality problems in random environments.},
  urldate = {2016-02-09},
  date = {2015-08-12},
  keywords = {Mathematics - Optimization and Control,Mathematics - Dynamical Systems,47H05; 47N10; 62L20; 34A60},
  author = {Bianchi, Pascal and Hachem, Walid},
  file = {/home/arthur/Dropbox/Zotero/Bianchi_Hachem_2015_Dynamical behavior of a stochastic forward-backward algorithm using random.pdf;/home/arthur/Zotero/storage/WSZTBA4V/1508.html}
}

@article{candes_near-optimal_2006,
  title = {Near-Optimal Signal Recovery from Random Projections: {{Universal}} Encoding Strategies?},
  volume = {52},
  url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4016283},
  number = {12},
  journaltitle = {Information Theory, IEEE Transactions on},
  urldate = {2016-02-10},
  date = {2006},
  pages = {5406--5425},
  author = {Candès, Emmanuel J. and Tao, Terence},
  file = {/home/arthur/Dropbox/Zotero/Candès_Tao_2006_Near-optimal signal recovery from random projections.pdf;/home/arthur/Zotero/storage/NQGXIK3F/abs_all.html}
}

@article{bouthillier_dropout_2015,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1506.08700},
  primaryClass = {cs, stat},
  title = {Dropout as Data Augmentation},
  url = {http://arxiv.org/abs/1506.08700},
  abstract = {Dropout is typically interpreted as bagging a large number of models sharing parameters. We show that using dropout in a network can also be interpreted as a kind of data augmentation in the input space without domain knowledge. We present an approach to projecting the dropout noise within a network back into the input space, thereby generating augmented versions of the training data, and we show that training a deterministic network on the augmented samples yields similar results. Finally, we propose a new dropout noise scheme based on our observations and show that it improves dropout results without adding significant computational cost.},
  urldate = {2016-02-29},
  date = {2015-06-29},
  keywords = {Computer Science - Learning,Statistics - Machine Learning},
  author = {Bouthillier, Xavier and Konda, Kishore and Vincent, Pascal and Memisevic, Roland},
  file = {/home/arthur/Dropbox/Zotero/Bouthillier et al_2015_Dropout as data augmentation.pdf;/home/arthur/Zotero/storage/H2I55ZWU/1506.html}
}

@article{rudi_less_2015,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1507.04717},
  primaryClass = {cs, stat},
  title = {Less Is {{More}}: {{Nystr}}$\backslash$"om {{Computational Regularization}}},
  url = {http://arxiv.org/abs/1507.04717},
  shorttitle = {Less Is {{More}}},
  abstract = {We study Nystr$\backslash$"om type subsampling approaches to large scale kernel methods, and prove learning bounds in the statistical learning setting, where random sampling and high probability estimates are considered. In particular, we prove that these approaches can achieve optimal learning bounds, provided the subsampling level is suitably chosen. These results suggest a simple incremental variant of Nystr$\backslash$"om Kernel Regularized Least Squares, where the subsampling level implements a form of computational regularization, in the sense that it controls at the same time regularization and computations. Extensive experimental analysis shows that the considered approach achieves state of the art performances on benchmark large scale datasets.},
  urldate = {2016-03-01},
  date = {2015-07-16},
  keywords = {Computer Science - Learning,Statistics - Machine Learning},
  author = {Rudi, Alessandro and Camoriano, Raffaello and Rosasco, Lorenzo},
  file = {/home/arthur/Dropbox/Zotero/Rudi et al_2015_Less is More.pdf;/home/arthur/Zotero/storage/6JSHFHQQ/1507.html}
}

@article{mairal_incremental_2015,
  langid = {english},
  title = {Incremental {{Majorization}}-{{Minimization Optimization}} with {{Application}} to {{Large}}-{{Scale Machine Learning}}},
  volume = {25},
  issn = {1052-6234, 1095-7189},
  url = {http://epubs.siam.org/doi/10.1137/140957639},
  number = {2},
  journaltitle = {SIAM Journal on Optimization},
  urldate = {2016-03-18},
  date = {2015-01},
  pages = {829-855},
  author = {Mairal, Julien},
  file = {/home/arthur/Dropbox/Zotero/Mairal_2015_Incremental Majorization-Minimization Optimization with Application to.pdf}
}

@article{lu_supplementary_nodate,
  title = {Supplementary {{Material}} for ‘{{Faster Ridge Regression}} via the {{Subsampled Randomized Hadamard Transform}}’},
  url = {http://pdhillon.com/ridge2013appendix.pdf},
  urldate = {2016-03-18},
  author = {Lu, Yichao and Dhillon, Paramveer S. and Foster, Dean and Ungar, Lyle},
  file = {/home/arthur/Dropbox/Zotero/Lu et al_Supplementary Material for ‘Faster Ridge Regression via the Subsampled.pdf}
}

@article{bach_kernel_2003,
  title = {Kernel Independent Component Analysis},
  volume = {3},
  url = {http://dl.acm.org/citation.cfm?id=944920},
  journaltitle = {The Journal of Machine Learning Research},
  urldate = {2016-03-18},
  date = {2003},
  pages = {1--48},
  author = {Bach, Francis R. and Jordan, Michael I.},
  file = {/home/arthur/Dropbox/Zotero/Bach_Jordan_2003_Kernel independent component analysis.pdf}
}

@incollection{stober_using_2014,
  title = {Using {{Convolutional Neural Networks}} to {{Recognize Rhythm}} ￼{{Stimuli}} from {{Electroencephalography Recordings}}},
  url = {http://papers.nips.cc/paper/5272-using-convolutional-neural-networks-to-recognize-rhythm-stimuli-from-electroencephalography-recordings.pdf},
  booktitle = {Advances in {{Neural Information Processing Systems}} 27},
  publisher = {{Curran Associates, Inc.}},
  urldate = {2016-03-18},
  date = {2014},
  pages = {1449--1457},
  author = {Stober, Sebastian and Cameron, Daniel J and Grahn, Jessica A},
  editor = {Ghahramani, Z. and Welling, M. and Cortes, C. and Lawrence, N. D. and Weinberger, K. Q.},
  file = {/home/arthur/Dropbox/Zotero/Stober et al_2014_Using Convolutional Neural Networks to Recognize Rhythm ￼Stimuli from.pdf;/home/arthur/Zotero/storage/HD27MIMQ/5272-using-convolutional-neural-networks-to-recognize-rhythm-stimuli-from-electroencephalograph.html}
}

@article{combettes_stochastic_2015,
  title = {Stochastic Quasi-{{Fejér}} Block-Coordinate Fixed Point Iterations with Random Sweeping},
  volume = {25},
  url = {http://epubs.siam.org/doi/abs/10.1137/140971233},
  number = {2},
  journaltitle = {SIAM Journal on Optimization},
  urldate = {2016-03-18},
  date = {2015},
  pages = {1221--1248},
  author = {Combettes, Patrick L. and Pesquet, Jean-Christophe},
  file = {/home/arthur/Dropbox/Zotero/Combettes_Pesquet_2015_Stochastic quasi-Fejér block-coordinate fixed point iterations with random.pdf}
}

@article{platt_fast_1999,
  title = {Fast {{Training}} of {{Support Vector Machines}} Using {{Sequential Minimal Optimization}}},
  url = {http://www.cs.utsa.edu/~bylander/cs6243/smo-book.pdf},
  journaltitle = {Advances in kernel methods},
  urldate = {2016-03-18},
  date = {1999},
  pages = {185--208},
  author = {Platt, John C.},
  file = {/home/arthur/Dropbox/Zotero/Platt_1999_Fast Training of Support Vector Machines using Sequential Minimal Optimization.pdf}
}

@book{friedman_elements_2001,
  title = {The Elements of Statistical Learning},
  volume = {1},
  url = {http://statweb.stanford.edu/~tibs/book/preface.ps},
  publisher = {{Springer series in statistics Springer, Berlin}},
  urldate = {2016-03-18},
  date = {2001},
  author = {Friedman, Jerome and Hastie, Trevor and Tibshirani, Robert},
  file = {/home/arthur/Dropbox/Zotero/Friedman et al_2001_The elements of statistical learning.pdf}
}

@article{gal_dropout_2015-1,
  title = {Dropout as a {{Bayesian}} Approximation: {{Representing}} Model Uncertainty in Deep Learning},
  url = {http://arxiv.org/abs/1506.02142},
  shorttitle = {Dropout as a {{Bayesian}} Approximation},
  journaltitle = {arXiv preprint arXiv:1506.02142},
  urldate = {2016-03-18},
  date = {2015},
  author = {Gal, Yarin and Ghahramani, Zoubin},
  file = {/home/arthur/Dropbox/Zotero/Gal_Ghahramani_2015_Dropout as a Bayesian approximation.pdf;/home/arthur/Zotero/storage/BS4ZJA5H/1506.html}
}

@inproceedings{zhuang_fast_2013,
  title = {A Fast Parallel {{SGD}} for Matrix Factorization in Shared Memory Systems},
  url = {http://dl.acm.org/citation.cfm?id=2507164},
  booktitle = {Proceedings of the 7th {{ACM}} Conference on {{Recommender}} Systems},
  publisher = {{ACM}},
  urldate = {2016-03-18},
  date = {2013},
  pages = {249--256},
  author = {Zhuang, Yong and Chin, Wei-Sheng and Juan, Yu-Chin and Lin, Chih-Jen},
  file = {/home/arthur/Zotero/storage/P39G8EB4/citation.html}
}

@book{van_der_vaart_asymptotic_2000,
  title = {Asymptotic Statistics},
  volume = {3},
  url = {https://books.google.fr/books?hl=en&lr=&id=UEuQEM5RjWgC&oi=fnd&pg=PR13&dq=%22asymptotic+statistics%22&ots=mnSIOGaYLB&sig=JTbH-i1a4gDRjOG1tSeUQiMn5w0},
  publisher = {{Cambridge university press}},
  urldate = {2016-03-18},
  date = {2000},
  author = {Van der Vaart, Aad W.},
  file = {/home/arthur/Dropbox/Zotero/Van der Vaart_2000_Asymptotic statistics.pdf;/home/arthur/Zotero/storage/RWBTSE23/books.html}
}

@inproceedings{mairal_online_2009,
  title = {Online Dictionary Learning for Sparse Coding},
  url = {http://dl.acm.org/citation.cfm?id=1553463},
  booktitle = {Proceedings of the 26th Annual International Conference on Machine Learning},
  publisher = {{ACM}},
  urldate = {2016-03-18},
  date = {2009},
  pages = {689--696},
  author = {Mairal, Julien and Bach, Francis and Ponce, Jean and Sapiro, Guillermo},
  file = {/home/arthur/Dropbox/Zotero/Mairal et al_2009_Online dictionary learning for sparse coding.pdf}
}

@article{pilanci_iterative_2014,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1411.0347},
  primaryClass = {cs, math, stat},
  title = {Iterative {{Hessian}} Sketch: {{Fast}} and Accurate Solution Approximation for Constrained Least-Squares},
  url = {http://arxiv.org/abs/1411.0347},
  shorttitle = {Iterative {{Hessian}} Sketch},
  abstract = {We study randomized sketching methods for approximately solving least-squares problem with a general convex constraint. The quality of a least-squares approximation can be assessed in different ways: either in terms of the value of the quadratic objective function (cost approximation), or in terms of some distance measure between the approximate minimizer and the true minimizer (solution approximation). Focusing on the latter criterion, our first main result provides a general lower bound on any randomized method that sketches both the data matrix and vector in a least-squares problem; as a surprising consequence, the most widely used least-squares sketch is sub-optimal for solution approximation. We then present a new method known as the iterative Hessian sketch, and show that it can be used to obtain approximations to the original least-squares problem using a projection dimension proportional to the statistical complexity of the least-squares minimizer, and a logarithmic number of iterations. We illustrate our general theory with simulations for both unconstrained and constrained versions of least-squares, including \$$\backslash$ell\_1\$-regularization and nuclear norm constraints. We also numerically demonstrate the practicality of our approach in a real face expression classification experiment.},
  urldate = {2016-03-23},
  date = {2014-11-02},
  keywords = {Computer Science - Learning,Mathematics - Optimization and Control,Statistics - Machine Learning,Computer Science - Information Theory},
  author = {Pilanci, Mert and Wainwright, Martin J.},
  file = {/home/arthur/Dropbox/Zotero/Pilanci_Wainwright_2014_Iterative Hessian sketch.pdf;/home/arthur/Zotero/storage/G9GTW5E9/1411.html}
}

@article{petersen_matrix_2008,
  title = {The Matrix Cookbook},
  volume = {7},
  url = {http://www.cim.mcgill.ca/~dudek/417/Papers/matrixOperations.pdf},
  journaltitle = {Technical University of Denmark},
  urldate = {2016-03-23},
  date = {2008},
  pages = {15},
  author = {Petersen, Kaare Brandt and Pedersen, Michael Syskind and {others}},
  file = {/home/arthur/Dropbox/Zotero/Petersen et al_2008_The matrix cookbook.pdf}
}

@article{rosasco_first-order_2016,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1602.07872},
  primaryClass = {math},
  title = {A First-Order Stochastic Primal-Dual Algorithm with Correction Step},
  url = {http://arxiv.org/abs/1602.07872},
  abstract = {We investigate the convergence properties of a stochastic primal-dual splitting algorithm for solving structured monotone inclusions involving the sum of a cocoercive operator and a composite monotone operator. The proposed method is the stochastic extension to monotone inclusions of a proximal method studied in \{$\backslash$em Y. Drori, S. Sabach, and M. Teboulle, A simple algorithm for a class of nonsmooth convex-concave saddle-point problems, 2015\} and \{$\backslash$em I. Loris and C. Verhoeven, On a generalization of the iterative soft-thresholding algorithm for the case of non-separable penalty, 2011\} for saddle point problems. It consists in a forward step determined by the stochastic evaluation of the cocoercive operator, a backward step in the dual variables involving the resolvent of the monotone operator, and an additional forward step using the stochastic evaluation of the cocoercive introduced in the first step. We prove weak almost sure convergence of the iterates by showing that the primal-dual sequence generated by the method is stochastic quasi Fej$\backslash$'er-monotone with respect to the set of zeros of the considered primal and dual inclusions. Additional results on ergodic convergence in expectation are considered for the special case of saddle point models.},
  urldate = {2016-03-24},
  date = {2016-02-25},
  keywords = {Mathematics - Optimization and Control,47H05; 49M29; 49M27; 90C25},
  author = {Rosasco, Lorenzo and Villa, Silvia and Vu, Bang Cong},
  file = {/home/arthur/Dropbox/Zotero/Rosasco et al_2016_A first-order stochastic primal-dual algorithm with correction step.pdf;/home/arthur/Zotero/storage/2PNWMD4P/1602.html}
}

@article{dieuleveut_non-parametric_2014,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1408.0361},
  primaryClass = {math, stat},
  title = {Non-Parametric {{Stochastic Approximation}} with {{Large Step}} Sizes},
  url = {http://arxiv.org/abs/1408.0361},
  abstract = {We consider the random-design least-squares regression problem within the reproducing kernel Hilbert space (RKHS) framework. Given a stream of independent and identically distributed input/output data, we aim to learn a regression function within an RKHS \$$\backslash$mathcal\{H\}\$, even if the optimal predictor (i.e., the conditional expectation) is not in \$$\backslash$mathcal\{H\}\$. In a stochastic approximation framework where the estimator is updated after each observation, we show that the averaged unregularized least-mean-square algorithm (a form of stochastic gradient), given a sufficient large step-size, attains optimal rates of convergence for a variety of regimes for the smoothnesses of the optimal prediction function and the functions in \$$\backslash$mathcal\{H\}\$.},
  urldate = {2016-03-24},
  date = {2014-08-02},
  keywords = {Mathematics - Statistics Theory},
  author = {Dieuleveut, Aymeric and Bach, Francis},
  file = {/home/arthur/Dropbox/Zotero/Dieuleveut_Bach_2014_Non-parametric Stochastic Approximation with Large Step sizes.pdf;/home/arthur/Zotero/storage/N7346W23/1408.html}
}

@incollection{kundu_approximating_2015,
  title = {Approximating {{Sparse PCA}} from {{Incomplete Data}}},
  url = {http://papers.nips.cc/paper/5905-approximating-sparse-pca-from-incomplete-data.pdf},
  booktitle = {Advances in {{Neural Information Processing Systems}} 28},
  publisher = {{Curran Associates, Inc.}},
  urldate = {2016-03-25},
  date = {2015},
  pages = {388--396},
  author = {Kundu, Abhisek and Drineas, Petros and Magdon-Ismail, Malik},
  editor = {Cortes, C. and Lawrence, N. D. and Lee, D. D. and Sugiyama, M. and Garnett, R.},
  file = {/home/arthur/Dropbox/Zotero/Kundu et al_2015_Approximating Sparse PCA from Incomplete Data.pdf;/home/arthur/Zotero/storage/K7IEEV57/5905-approximating-sparse-pca-from-incomplete-data.html}
}

@article{mardani_dynamic_2013,
  title = {Dynamic {{Anomalography}}: {{Tracking Network Anomalies Via Sparsity}} and {{Low Rank}}},
  volume = {7},
  issn = {1932-4553},
  doi = {10.1109/JSTSP.2012.2233193},
  shorttitle = {Dynamic {{Anomalography}}},
  abstract = {In the backbone of large-scale networks, origin-to-destination (OD) traffic flows experience abrupt unusual changes known as traffic volume anomalies, which can result in congestion and limit the extent to which end-user quality of service requirements are met. As a means of maintaining seamless end-user experience in dynamic environments, as well as for ensuring network security, this paper deals with a crucial network monitoring task termed dynamic anomalography. Given link traffic measurements (noisy superpositions of unobserved OD flows) periodically acquired by backbone routers, the goal is to construct an estimated map of anomalies in real time, and thus summarize the network `health state' along both the flow and time dimensions. Leveraging the low intrinsic-dimensionality of OD flows and the sparse nature of anomalies, a novel online estimator is proposed based on an exponentially-weighted least-squares criterion regularized with the sparsity-promoting l1-norm of the anomalies, and the nuclear norm of the nominal traffic matrix. After recasting the non-separable nuclear norm into a form amenable to online optimization, a real-time algorithm for dynamic anomalography is developed and its convergence established under simplifying technical assumptions. For operational conditions where computational complexity reductions are at a premium, a lightweight stochastic gradient algorithm based on Nesterov's acceleration technique is developed as well. Comprehensive numerical tests with both synthetic and real network data corroborate the effectiveness of the proposed online algorithms and their tracking capabilities, and demonstrate that they outperform state-of-the-art approaches developed to diagnose traffic anomalies.},
  number = {1},
  journaltitle = {IEEE Journal of Selected Topics in Signal Processing},
  date = {2013-02},
  pages = {50-66},
  keywords = {Sparse matrices,Sparsity,computational complexity,matrix algebra,Signal processing algorithms,Heuristic algorithms,IP networks,Monitoring,Nesterov acceleration technique,OD flow intrinsic dimensionality,OD traffic flows,Real-time systems,Routing,Time measurement,backbone routers,comprehensive numerical tests,computational complexity reductions,computer network security,dynamic anomalography,end-user QoS requirement,exponentially-weighted least-squares criterion,gradient methods,large-scale networks,least squares approximations,lightweight stochastic gradient algorithm,link traffic measurements,low rank,network anomaly tracking,network cartography,network health state,network monitoring task,network security,nominal traffic matrix,nonseparable nuclear norm,online estimator,online optimization,origin-to-destination traffic flows,quality of service,real-time algorithm,seamless end-user experience,sparsity-promoting l1-norm,tracking capabilities,traffic volume anomalies},
  author = {Mardani, M. and Mateos, G. and Giannakis, G. B.},
  file = {/home/arthur/Zotero/storage/M75ESHG7/Mardani et al. - 2013 - Dynamic Anomalography Tracking Network Anomalies .pdf;/home/arthur/Zotero/storage/CTXQDVHZ/abs_all.html}
}

@article{achab_sgd_2015,
  title = {{{SGD}} with {{Variance Reduction}} beyond {{Empirical Risk Minimization}}},
  url = {http://arxiv.org/abs/1510.04822},
  journaltitle = {arXiv preprint arXiv:1510.04822},
  urldate = {2016-03-28},
  date = {2015},
  author = {Achab, Massil and Guilloux, Agathe and Gaïffas, Stéphane and Bacry, Emmanuel},
  file = {/home/arthur/Dropbox/Zotero/Achab et al_2015_SGD with Variance Reduction beyond Empirical Risk Minimization.pdf;/home/arthur/Zotero/storage/W6NXAHDS/1510.html}
}

@article{defazio_finito_2014,
  title = {Finito: {{A}} Faster, Permutable Incremental Gradient Method for Big Data Problems},
  url = {http://arxiv.org/abs/1407.2710},
  shorttitle = {Finito},
  journaltitle = {arXiv preprint arXiv:1407.2710},
  urldate = {2016-03-28},
  date = {2014},
  author = {Defazio, Aaron J. and Caetano, Tibério S. and Domke, Justin},
  file = {/home/arthur/Dropbox/Zotero/Defazio et al_2014_Finito.pdf}
}

@article{levy_improving_2015,
  title = {Improving Distributional Similarity with Lessons Learned from Word Embeddings},
  volume = {3},
  url = {https://tacl.cs.columbia.edu/ojs/index.php/tacl/article/view/570},
  journaltitle = {Transactions of the Association for Computational Linguistics},
  urldate = {2016-03-29},
  date = {2015},
  pages = {211--225},
  author = {Levy, Omer and Goldberg, Yoav and Dagan, Ido},
  file = {/home/arthur/Dropbox/Zotero/Levy et al_2015_Improving distributional similarity with lessons learned from word embeddings.pdf}
}

@article{chouzenoux_stochastic_2015,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1512.08722},
  primaryClass = {math},
  title = {A {{Stochastic Majorize}}-{{Minimize Subspace Algorithm}} for {{Online Penalized Least Squares Estimation}}},
  url = {http://arxiv.org/abs/1512.08722},
  abstract = {Stochastic approximation techniques play an important role in solving many problems encountered in machine learning or adaptive signal processing. In these contexts, the statistics of the data are often unknown a priori or their direct computation is too intensive, and they have thus to be estimated online from the observed signals. For batch optimization of an objective function being the sum of a data fidelity term and a penalization (e.g. a sparsity promoting function), Majorize-Minimize (MM) methods have recently attracted much interest since they are fast, highly flexible, and effective in ensuring convergence. The goal of this paper is to show how these methods can be successfully extended to the case when the data fidelity term corresponds to a least squares criterion and the cost function is replaced by a sequence of stochastic approximations of it. In this context, we propose an online version of an MM subspace algorithm and we study its convergence by using suitable probabilistic tools. Simulation results illustrate the good practical performance of the proposed algorithm associated with a memory gradient subspace, when applied to both non-adaptive and adaptive filter identification problems.},
  urldate = {2016-03-29},
  date = {2015-12-29},
  keywords = {Mathematics - Optimization and Control},
  author = {Chouzenoux, Emilie and Pesquet, Jean-Christophe},
  file = {/home/arthur/Dropbox/Zotero/Chouzenoux_Pesquet_2015_A Stochastic Majorize-Minimize Subspace Algorithm for Online Penalized Least.pdf;/home/arthur/Zotero/storage/XCWFQZ83/1512.html}
}

@article{defazio_saga_2014,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1407.0202},
  primaryClass = {cs, math, stat},
  title = {{{SAGA}}: {{A}} Fast Incremental Gradient Method with Support for Non-Strongly Convex Composite Objectives},
  url = {http://arxiv.org/abs/1407.0202},
  shorttitle = {{{SAGA}}},
  abstract = {In this work we introduce a new optimisation method called SAGA in the spirit of SAG, SDCA, MISO and SVRG, a set of recently proposed incremental gradient algorithms with fast linear convergence rates. SAGA improves on the theory behind SAG and SVRG, with better theoretical convergence rates, and has support for composite objectives where a proximal operator is used on the regulariser. Unlike SDCA, SAGA supports non-strongly convex problems directly, and is adaptive to any inherent strong convexity of the problem. We give experimental results showing the effectiveness of our method.},
  urldate = {2016-03-29},
  date = {2014-07-01},
  keywords = {Computer Science - Learning,Mathematics - Optimization and Control,Statistics - Machine Learning},
  author = {Defazio, Aaron and Bach, Francis and Lacoste-Julien, Simon},
  file = {/home/arthur/Dropbox/Zotero/Defazio et al_2014_SAGA.pdf;/home/arthur/Zotero/storage/E65RIQDN/1407.html}
}

@incollection{johnson_accelerating_2013,
  title = {Accelerating Stochastic Gradient Descent Using Predictive Variance Reduction},
  url = {http://papers.nips.cc/paper/4937-accelerating-stochastic-gradient-descent-using-predictive-variance-reduction.pdf},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  urldate = {2016-03-29},
  date = {2013},
  pages = {315--323},
  author = {Johnson, Rie and Zhang, Tong},
  editor = {Burges, C. J. C. and Bottou, L. and Welling, M. and Ghahramani, Z. and Weinberger, K. Q.},
  file = {/home/arthur/Dropbox/Zotero/Johnson_Zhang_2013_Accelerating stochastic gradient descent using predictive variance reduction.pdf;/home/arthur/Zotero/storage/SPBW9XVH/4937-accelerating.html}
}

@article{shalev-shwartz_stochastic_2013,
  title = {Stochastic Dual Coordinate Ascent Methods for Regularized Loss},
  volume = {14},
  url = {http://dl.acm.org/citation.cfm?id=2502598},
  number = {1},
  journaltitle = {The Journal of Machine Learning Research},
  urldate = {2016-03-29},
  date = {2013},
  pages = {567--599},
  author = {Shalev-Shwartz, Shai and Zhang, Tong},
  file = {/home/arthur/Dropbox/Zotero/Shalev-Shwartz_Zhang_2013_Stochastic dual coordinate ascent methods for regularized loss.pdf;/home/arthur/Zotero/storage/EHQA2S7S/citation.html}
}

@article{reddi_variance_2015,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1506.06840},
  primaryClass = {cs, stat},
  title = {On {{Variance Reduction}} in {{Stochastic Gradient Descent}} and Its {{Asynchronous Variants}}},
  url = {http://arxiv.org/abs/1506.06840},
  abstract = {We study optimization algorithms based on variance reduction for stochastic gradient descent (SGD). Remarkable recent progress has been made in this direction through development of algorithms like SAG, SVRG, SAGA. These algorithms have been shown to outperform SGD, both theoretically and empirically. However, asynchronous versions of these algorithms---a crucial requirement for modern large-scale applications---have not been studied. We bridge this gap by presenting a unifying framework for many variance reduction techniques. Subsequently, we propose an asynchronous algorithm grounded in our framework, and prove its fast convergence. An important consequence of our general approach is that it yields asynchronous versions of variance reduction algorithms such as SVRG and SAGA as a byproduct. Our method achieves near linear speedup in sparse settings common to machine learning. We demonstrate the empirical performance of our method through a concrete realization of asynchronous SVRG.},
  urldate = {2016-03-29},
  date = {2015-06-22},
  keywords = {Computer Science - Learning,Statistics - Machine Learning},
  author = {Reddi, Sashank J. and Hefny, Ahmed and Sra, Suvrit and Póczos, Barnabás and Smola, Alex},
  file = {/home/arthur/Dropbox/Zotero/Reddi et al_2015_On Variance Reduction in Stochastic Gradient Descent and its Asynchronous.pdf;/home/arthur/Zotero/storage/6WD4WT4K/1506.html}
}

@article{daubechies_independent_2009,
  title = {Independent Component Analysis for Brain {{fMRI}} Does Not Select for Independence},
  volume = {106},
  url = {http://www.pnas.org/content/106/26/10415.short},
  number = {26},
  journaltitle = {Proceedings of the National Academy of Sciences},
  urldate = {2016-03-29},
  date = {2009},
  pages = {10415--10422},
  author = {Daubechies, I. and Roussos, E. and Takerkart, S. and Benharrosh, M. and Golden, C. and D'ardenne, K. and Richter, W. and Cohen, J. D. and Haxby, J.},
  file = {/home/arthur/Zotero/storage/42A9R45M/10415.html;/home/arthur/Zotero/storage/6BKMNXIH/10415.html}
}

@article{raskutti_statistical_2014,
  title = {A {{Statistical Perspective}} on {{Randomized Sketching}} for {{Ordinary Least}}-{{Squares}}},
  volume = {1050},
  url = {https://www.icsi.berkeley.edu/pubs/initiatives/statisticalperspective14.pdf},
  journaltitle = {stat},
  urldate = {2016-03-30},
  date = {2014},
  pages = {23},
  author = {Raskutti, Garvesh and Mahoney, Michael},
  file = {/home/arthur/Dropbox/Zotero/Raskutti_Mahoney_2014_A Statistical Perspective on Randomized Sketching for Ordinary Least-Squares.pdf}
}

@inproceedings{wang_fast_2013,
  title = {Fast Dropout Training},
  url = {http://jmlr.org/proceedings/papers/v28/wang13a.html},
  booktitle = {Proceedings of the 30th {{International Conference}} on {{Machine Learning}}},
  urldate = {2016-04-01},
  date = {2013},
  pages = {118--126},
  author = {Wang, Sida and Manning, Christopher},
  file = {/home/arthur/Dropbox/Zotero/Wang_Manning_2013_Fast dropout training.pdf}
}

@article{mairal_optimization_2013,
  title = {Optimization with First-Order Surrogate Functions},
  url = {http://arxiv.org/abs/1305.3120},
  journaltitle = {arXiv preprint arXiv:1305.3120},
  urldate = {2016-04-04},
  date = {2013},
  author = {Mairal, Julien},
  file = {/home/arthur/Dropbox/Zotero/Mairal_2013_Optimization with first-order surrogate functions.pdf}
}

@article{daspremont_subsampling_2011,
  langid = {english},
  title = {Subsampling Algorithms for Semidefinite Programming},
  volume = {1},
  issn = {1946-5238},
  url = {http://projecteuclid.org/euclid.ssy/1393252079},
  number = {2},
  journaltitle = {Stochastic Systems},
  urldate = {2016-04-04},
  date = {2011},
  pages = {274-305},
  author = {d’ Aspremont, Alexandre},
  options = {useprefix=true},
  file = {/home/arthur/Dropbox/Zotero/d’Aspremont_2011_Subsampling algorithms for semidefinite programming.pdf}
}

@inproceedings{cohen_uniform_2015,
  title = {Uniform Sampling for Matrix Approximation},
  url = {http://dl.acm.org/citation.cfm?id=2688113},
  booktitle = {Proceedings of the 2015 {{Conference}} on {{Innovations}} in {{Theoretical Computer Science}}},
  publisher = {{ACM}},
  urldate = {2016-04-04},
  date = {2015},
  pages = {181--190},
  author = {Cohen, Michael B. and Lee, Yin Tat and Musco, Cameron and Musco, Christopher and Peng, Richard and Sidford, Aaron},
  file = {/home/arthur/Dropbox/Zotero/Cohen et al_2015_Uniform sampling for matrix approximation.pdf;/home/arthur/Zotero/storage/MA9JGGI5/citation.html}
}

@article{frostig_-regularizing_2015,
  title = {Un-Regularizing: Approximate Proximal Point and Faster Stochastic Algorithms for Empirical Risk Minimization},
  url = {http://arxiv.org/abs/1506.07512},
  shorttitle = {Un-Regularizing},
  journaltitle = {arXiv preprint arXiv:1506.07512},
  urldate = {2016-04-04},
  date = {2015},
  author = {Frostig, Roy and Ge, Rong and Kakade, Sham M. and Sidford, Aaron},
  file = {/home/arthur/Dropbox/Zotero/Frostig et al_2015_Un-regularizing.pdf;/home/arthur/Zotero/storage/PAEMHWD7/1506.html}
}

@incollection{wainwright_estimating_2006,
  title = {Estimating the Wrong {{Markov}} Random Field: {{Benefits}} in the Computation-Limited Setting},
  url = {http://papers.nips.cc/paper/2925-estimating-the-wrong-markov-random-field-benefits-in-the-computation-limited-setting.pdf},
  shorttitle = {Estimating the Wrong {{Markov}} Random Field},
  booktitle = {Advances in {{Neural Information Processing Systems}} 18},
  publisher = {{MIT Press}},
  urldate = {2016-04-04},
  date = {2006},
  pages = {1425--1432},
  author = {Wainwright, Martin J},
  editor = {Weiss, Y. and Schölkopf, B. and Platt, J. C.},
  file = {/home/arthur/Dropbox/Zotero/Wainwright_2006_Estimating the wrong Markov random field.pdf;/home/arthur/Zotero/storage/AJ3KVR9W/2925-estimating-the-wrong-markov-random-field-benefits-in-the-computation-limited-setting.html}
}

@inproceedings{kiros_skip-thought_2015,
  title = {Skip-Thought Vectors},
  url = {http://papers.nips.cc/paper/5950-skip-thought-vectors},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  urldate = {2016-04-05},
  date = {2015},
  pages = {3276--3284},
  author = {Kiros, Ryan and Zhu, Yukun and Salakhutdinov, Ruslan R. and Zemel, Richard and Urtasun, Raquel and Torralba, Antonio and Fidler, Sanja},
  file = {/home/arthur/Dropbox/Zotero/Kiros et al_2015_Skip-thought vectors.pdf}
}

@inproceedings{andrew_deep_2013,
  title = {Deep Canonical Correlation Analysis},
  url = {http://jmlr.org/proceedings/papers/v28/andrew13.html},
  booktitle = {Proceedings of the 30th {{International Conference}} on {{Machine Learning}}},
  urldate = {2016-04-05},
  date = {2013},
  pages = {1247--1255},
  author = {Andrew, Galen and Arora, Raman and Bilmes, Jeff and Livescu, Karen},
  file = {/home/arthur/Dropbox/Zotero/Andrew et al_2013_Deep canonical correlation analysis.pdf}
}

@article{chandar_correlational_2015,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1504.07225},
  primaryClass = {cs, stat},
  title = {Correlational {{Neural Networks}}},
  url = {http://arxiv.org/abs/1504.07225},
  abstract = {Common Representation Learning (CRL), wherein different descriptions (or views) of the data are embedded in a common subspace, is receiving a lot of attention recently. Two popular paradigms here are Canonical Correlation Analysis (CCA) based approaches and Autoencoder (AE) based approaches. CCA based approaches learn a joint representation by maximizing correlation of the views when projected to the common subspace. AE based methods learn a common representation by minimizing the error of reconstructing the two views. Each of these approaches has its own advantages and disadvantages. For example, while CCA based approaches outperform AE based approaches for the task of transfer learning, they are not as scalable as the latter. In this work we propose an AE based approach called Correlational Neural Network (CorrNet), that explicitly maximizes correlation among the views when projected to the common subspace. Through a series of experiments, we demonstrate that the proposed CorrNet is better than the above mentioned approaches with respect to its ability to learn correlated common representations. Further, we employ CorrNet for several cross language tasks and show that the representations learned using CorrNet perform better than the ones learned using other state of the art approaches.},
  urldate = {2016-04-05},
  date = {2015-04-27},
  keywords = {Computer Science - Learning,Statistics - Machine Learning,Computer Science - Computation and Language,Computer Science - Neural and Evolutionary Computing},
  author = {Chandar, Sarath and Khapra, Mitesh M. and Larochelle, Hugo and Ravindran, Balaraman},
  file = {/home/arthur/Dropbox/Zotero/Chandar et al_2015_Correlational Neural Networks.pdf;/home/arthur/Zotero/storage/TZZTRI39/1504.html}
}

@article{mokhtari_dsa_2015,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1506.04216},
  primaryClass = {math},
  title = {{{DSA}}: {{Decentralized Double Stochastic Averaging Gradient Algorithm}}},
  url = {http://arxiv.org/abs/1506.04216},
  shorttitle = {{{DSA}}},
  abstract = {This paper considers convex optimization problems where nodes of a network have access to summands of a global objective. Each of these local objectives is further assumed to be an average of a finite set of functions. The motivation for this setup is to solve large scale machine learning problems where elements of the training set are distributed to multiple computational elements. The decentralized double stochastic averaging gradient (DSA) algorithm is proposed as a solution alternative that relies on: (i) The use of local stochastic averaging gradients. (ii) Determination of descent steps as differences of consecutive stochastic averaging gradients. Strong convexity of local functions and Lipschitz continuity of local gradients is shown to guarantee linear convergence of the sequence generated by DSA in expectation. Local iterates are further shown to approach the optimal argument for almost all realizations. The expected linear convergence of DSA is in contrast to the sublinear rate characteristic of existing methods for decentralized stochastic optimization. Numerical experiments on a logistic regression problem illustrate reductions in convergence time and number of feature vectors processed until convergence relative to these other alternatives.},
  urldate = {2016-04-05},
  date = {2015-06-12},
  keywords = {Mathematics - Optimization and Control},
  author = {Mokhtari, Aryan and Ribeiro, Alejandro},
  file = {/home/arthur/Dropbox/Zotero/Mokhtari_Ribeiro_2015_DSA.pdf;/home/arthur/Zotero/storage/UZHMEJ5R/1506.html}
}

@article{lin_universal_2015,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1506.02186},
  primaryClass = {math},
  title = {A {{Universal Catalyst}} for {{First}}-{{Order Optimization}}},
  url = {http://arxiv.org/abs/1506.02186},
  abstract = {We introduce a generic scheme for accelerating first-order optimization methods in the sense of Nesterov, which builds upon a new analysis of the accelerated proximal point algorithm. Our approach consists of minimizing a convex objective by approximately solving a sequence of well-chosen auxiliary problems, leading to faster convergence. This strategy applies to a large class of algorithms, including gradient descent, block coordinate descent, SAG, SAGA, SDCA, SVRG, Finito/MISO, and their proximal variants. For all of these methods, we provide acceleration and explicit support for non-strongly convex objectives. In addition to theoretical speed-up, we also show that acceleration is useful in practice, especially for ill-conditioned problems where we measure significant improvements.},
  urldate = {2016-04-06},
  date = {2015-06-06},
  keywords = {Mathematics - Optimization and Control},
  author = {Lin, Hongzhou and Mairal, Julien and Harchaoui, Zaid},
  file = {/home/arthur/Dropbox/Zotero/Lin et al_2015_A Universal Catalyst for First-Order Optimization.pdf;/home/arthur/Zotero/storage/X9GI96ZB/1506.html}
}

@article{sulam_trainlets_2016,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1602.00212},
  primaryClass = {cs},
  title = {Trainlets: {{Dictionary Learning}} in {{High Dimensions}}},
  url = {http://arxiv.org/abs/1602.00212},
  shorttitle = {Trainlets},
  abstract = {Sparse representations has shown to be a very powerful model for real world signals, and has enabled the development of applications with notable performance. Combined with the ability to learn a dictionary from signal examples, sparsity-inspired algorithms are often achieving state-of-the-art results in a wide variety of tasks. Yet, these methods have traditionally been restricted to small dimensions mainly due to the computational constraints that the dictionary learning problem entails. In the context of image processing, this implies handling small image patches. In this work we show how to efficiently handle bigger dimensions and go beyond the small patches in sparsity-based signal and image processing methods. We build our approach based on a new cropped wavelet decomposition, which enables a multi-scale analysis with virtually no border effects. We then employ this as the base dictionary within a double sparsity model to enable the training of adaptive dictionaries. To cope with the increase of training data, while at the same time improving the training performance, we present an Online Sparse Dictionary Learning (OSDL) algorithm to train this model effectively, enabling it to handle millions of examples. This work shows that dictionary learning can be up-scaled to tackle a new level of signal dimensions, obtaining large adaptable atoms that we call trainlets.},
  urldate = {2016-04-06},
  date = {2016-01-31},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  author = {Sulam, Jeremias and Ophir, Boaz and Zibulevsky, Michael and Elad, Michael},
  file = {/home/arthur/Dropbox/Zotero/Sulam et al_2016_Trainlets.pdf;/home/arthur/Zotero/storage/QRE9AFIR/1602.html}
}

@article{bruckstein_sparse_2009,
  title = {From Sparse Solutions of Systems of Equations to Sparse Modeling of Signals and Images},
  volume = {51},
  url = {http://epubs.siam.org/doi/abs/10.1137/060657704},
  number = {1},
  journaltitle = {SIAM review},
  urldate = {2016-04-06},
  date = {2009},
  pages = {34--81},
  author = {Bruckstein, Alfred M. and Donoho, David L. and Elad, Michael},
  file = {/home/arthur/Dropbox/Zotero/Bruckstein et al_2009_From sparse solutions of systems of equations to sparse modeling of signals and.pdf;/home/arthur/Zotero/storage/I5BIRNFD/060657704.html}
}

@article{keriven_sketching_2015,
  title = {Sketching for {{Large}}-{{Scale Learning}} of {{Mixture Models}}},
  url = {https://hal.inria.fr/hal-01208027/},
  urldate = {2016-04-08},
  date = {2015},
  author = {Keriven, Nicolas and Bourrier, Anthony and Gribonval, Rémi and Perez, Patrick},
  file = {/home/arthur/Dropbox/Zotero/Keriven et al_2015_Sketching for Large-Scale Learning of Mixture Models.pdf;/home/arthur/Zotero/storage/7FQVSJQD/hal-01208027.html}
}

@inproceedings{le_magoarou_chasing_2015,
  title = {Chasing Butterflies: {{In}} Search of Efficient Dictionaries},
  url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=7178579},
  shorttitle = {Chasing Butterflies},
  booktitle = {Acoustics, {{Speech}} and {{Signal Processing}} ({{ICASSP}}), 2015 {{IEEE International Conference}} On},
  publisher = {{IEEE}},
  urldate = {2016-04-08},
  date = {2015},
  pages = {3287--3291},
  author = {Le Magoarou, Luc and Gribonval, Rémi},
  file = {/home/arthur/Dropbox/Zotero/Le Magoarou_Gribonval_2015_Chasing butterflies.pdf;/home/arthur/Zotero/storage/BNVPGFAF/abs_all.html}
}

@article{schulman_gradient_2015,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1506.05254},
  primaryClass = {cs},
  title = {Gradient {{Estimation Using Stochastic Computation Graphs}}},
  url = {http://arxiv.org/abs/1506.05254},
  abstract = {In a variety of problems originating in supervised, unsupervised, and reinforcement learning, the loss function is defined by an expectation over a collection of random variables, which might be part of a probabilistic model or the external world. Estimating the gradient of this loss function, using samples, lies at the core of gradient-based learning algorithms for these problems. We introduce the formalism of stochastic computation graphs---directed acyclic graphs that include both deterministic functions and conditional probability distributions---and describe how to easily and automatically derive an unbiased estimator of the loss function's gradient. The resulting algorithm for computing the gradient estimator is a simple modification of the standard backpropagation algorithm. The generic scheme we propose unifies estimators derived in variety of prior work, along with variance-reduction techniques therein. It could assist researchers in developing intricate models involving a combination of stochastic and deterministic operations, enabling, for example, attention, memory, and control actions.},
  urldate = {2016-04-11},
  date = {2015-06-17},
  keywords = {Computer Science - Learning},
  author = {Schulman, John and Heess, Nicolas and Weber, Theophane and Abbeel, Pieter},
  file = {/home/arthur/Dropbox/Zotero/Schulman et al_2015_Gradient Estimation Using Stochastic Computation Graphs.pdf;/home/arthur/Zotero/storage/VRKX4Z99/1506.html}
}

@article{greensmith_variance_2004,
  title = {Variance Reduction Techniques for Gradient Estimates in Reinforcement Learning},
  volume = {5},
  url = {http://dl.acm.org/citation.cfm?id=1044710},
  journaltitle = {The Journal of Machine Learning Research},
  urldate = {2016-04-11},
  date = {2004},
  pages = {1471--1530},
  author = {Greensmith, Evan and Bartlett, Peter L. and Baxter, Jonathan},
  file = {/home/arthur/Dropbox/Zotero/Greensmith et al_2004_Variance reduction techniques for gradient estimates in reinforcement learning.pdf;/home/arthur/Zotero/storage/KMSR5N5K/citation.html}
}

@article{kingma_auto-encoding_2013,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1312.6114},
  primaryClass = {cs, stat},
  title = {Auto-{{Encoding Variational Bayes}}},
  url = {http://arxiv.org/abs/1312.6114},
  abstract = {How can we perform efficient inference and learning in directed probabilistic models, in the presence of continuous latent variables with intractable posterior distributions, and large datasets? We introduce a stochastic variational inference and learning algorithm that scales to large datasets and, under some mild differentiability conditions, even works in the intractable case. Our contributions is two-fold. First, we show that a reparameterization of the variational lower bound yields a lower bound estimator that can be straightforwardly optimized using standard stochastic gradient methods. Second, we show that for i.i.d. datasets with continuous latent variables per datapoint, posterior inference can be made especially efficient by fitting an approximate inference model (also called a recognition model) to the intractable posterior using the proposed lower bound estimator. Theoretical advantages are reflected in experimental results.},
  urldate = {2016-04-11},
  date = {2013-12-20},
  keywords = {Computer Science - Learning,Statistics - Machine Learning},
  author = {Kingma, Diederik P. and Welling, Max},
  file = {/home/arthur/Dropbox/Zotero/Kingma_Welling_2013_Auto-Encoding Variational Bayes.pdf;/home/arthur/Zotero/storage/SQUQD2R5/1312.html}
}

@article{collobert_natural_2011,
  title = {Natural Language Processing (Almost) from Scratch},
  volume = {12},
  url = {http://dl.acm.org/citation.cfm?id=2078186},
  journaltitle = {The Journal of Machine Learning Research},
  urldate = {2016-04-19},
  date = {2011},
  pages = {2493--2537},
  author = {Collobert, Ronan and Weston, Jason and Bottou, Léon and Karlen, Michael and Kavukcuoglu, Koray and Kuksa, Pavel},
  file = {/home/arthur/Dropbox/Zotero/Collobert et al_2011_Natural language processing (almost) from scratch.pdf;/home/arthur/Zotero/storage/DW9NDS9K/citation.html}
}

@incollection{zhang_large-scale_2011,
  title = {Large-{{Scale Sparse Principal Component Analysis}} with {{Application}} to {{Text Data}}},
  url = {http://papers.nips.cc/paper/4337-large-scale-sparse-principal-component-analysis-with-application-to-text-data.pdf},
  booktitle = {Advances in {{Neural Information Processing Systems}} 24},
  publisher = {{Curran Associates, Inc.}},
  urldate = {2016-04-24},
  date = {2011},
  pages = {532--539},
  author = {Zhang, Youwei and Ghaoui, Laurent E.},
  editor = {Shawe-Taylor, J. and Zemel, R. S. and Bartlett, P. L. and Pereira, F. and Weinberger, K. Q.},
  file = {/home/arthur/Dropbox/Zotero/Zhang_Ghaoui_2011_Large-Scale Sparse Principal Component Analysis with Application to Text Data.pdf;/home/arthur/Zotero/storage/JRTAXWN4/4337-lar.html}
}

@article{papailiopoulos_sparse_2013,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1303.0551},
  primaryClass = {cs, math, stat},
  title = {Sparse {{PCA}} through {{Low}}-Rank {{Approximations}}},
  url = {http://arxiv.org/abs/1303.0551},
  abstract = {We introduce a novel algorithm that computes the \$k\$-sparse principal component of a positive semidefinite matrix \$A\$. Our algorithm is combinatorial and operates by examining a discrete set of special vectors lying in a low-dimensional eigen-subspace of \$A\$. We obtain provable approximation guarantees that depend on the spectral decay profile of the matrix: the faster the eigenvalue decay, the better the quality of our approximation. For example, if the eigenvalues of \$A\$ follow a power-law decay, we obtain a polynomial-time approximation algorithm for any desired accuracy. A key algorithmic component of our scheme is a combinatorial feature elimination step that is provably safe and in practice significantly reduces the running complexity of our algorithm. We implement our algorithm and test it on multiple artificial and real data sets. Due to the feature elimination step, it is possible to perform sparse PCA on data sets consisting of millions of entries in a few minutes. Our experimental evaluation shows that our scheme is nearly optimal while finding very sparse vectors. We compare to the prior state of the art and show that our scheme matches or outperforms previous algorithms in all tested data sets.},
  urldate = {2016-04-24},
  date = {2013-03-03},
  keywords = {Computer Science - Learning,Statistics - Machine Learning,Computer Science - Information Theory},
  author = {Papailiopoulos, Dimitris S. and Dimakis, Alexandros G. and Korokythakis, Stavros},
  file = {/home/arthur/Dropbox/Zotero/Papailiopoulos et al_2013_Sparse PCA through Low-rank Approximations.pdf;/home/arthur/Zotero/storage/32QPC257/1303.html}
}

@incollection{abraham_extracting_2013,
  title = {Extracting Brain Regions from Rest {{fMRI}} with Total-Variation Constrained Dictionary Learning},
  url = {http://link.springer.com/chapter/10.1007/978-3-642-40763-5_75},
  booktitle = {Medical {{Image Computing}} and {{Computer}}-{{Assisted Intervention}}–{{MICCAI}} 2013},
  publisher = {{Springer}},
  urldate = {2016-04-28},
  date = {2013},
  pages = {607--615},
  author = {Abraham, Alexandre and Dohmatob, Elvis and Thirion, Bertrand and Samaras, Dimitris and Varoquaux, Gael},
  file = {/home/arthur/Dropbox/Zotero/Abraham et al_2013_Extracting brain regions from rest fMRI with total-variation constrained.pdf;/home/arthur/Zotero/storage/NJ8QSJ9I/978-3-642-40763-5_75.html}
}

@article{van_essen_wu-minn_2013,
  title = {The {{WU}}-{{Minn Human Connectome Project}}: {{An}} Overview},
  volume = {80},
  url = {http://www.sciencedirect.com/science/article/pii/S1053811913005351},
  shorttitle = {The {{WU}}-{{Minn Human Connectome Project}}},
  abstract = {The Human Connectome Project consortium led by Washington University, University of Minnesota, and Oxford University is undertaking a systematic effort to map macroscopic human brain circuits and their relationship to behavior in a large population of healthy adults. This overview article focuses on progress made during the first half of the 5-year project in refining the methods for data acquisition and analysis. Preliminary analyses based on a finalized set of acquisition and preprocessing protocols demonstrate the exceptionally high quality of the data from each modality. The first quarterly release of imaging and behavioral data via the ConnectomeDB database demonstrates the commitment to making HCP datasets freely accessible. Altogether, the progress to date provides grounds for optimism that the HCP datasets and associated methods and software will become increasingly valuable resources for characterizing human brain connectivity and function, their relationship to behavior, and their heritability and genetic underpinnings.},
  journaltitle = {NeuroImage},
  shortjournal = {NeuroImage},
  series = {Mapping the Connectome},
  urldate = {2016-04-28},
  date = {2013},
  pages = {62-79},
  author = {Van Essen, David C. and Smith, Stephen M. and Barch, Deanna M. and Behrens, Timothy E. J. and Yacoub, Essa and Ugurbil, Kamil},
  file = {/home/arthur/Dropbox/Zotero/Van Essen et al_2013_The WU-Minn Human Connectome Project.pdf;/home/arthur/Zotero/storage/DHZHN4QR/S1053811913005351.html}
}

@article{kawaguchi_deep_2016,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1605.07110},
  primaryClass = {cs, math, stat},
  title = {Deep {{Learning}} without {{Poor Local Minima}}},
  url = {http://arxiv.org/abs/1605.07110},
  abstract = {In this paper, we prove a conjecture published in 1989 and also partially address an open problem announced at the Conference on Learning Theory (COLT) 2015. For an expected loss function of a deep nonlinear neural network, we prove the following statements under the independence assumption adopted from recent work: 1) the function is non-convex and non-concave, 2) every local minimum is a global minimum, 3) every critical point that is not a global minimum is a saddle point, and 4) the property of saddle points differs for shallow networks (with three layers) and deeper networks (with more than three layers). Moreover, we prove that the same four statements hold for deep linear neural networks with any depth, any widths and no unrealistic assumptions. As a result, we present an instance, for which we can answer to the following question: how difficult to directly train a deep model in theory? It is more difficult than the classical machine learning models (because of the non-convexity), but not too difficult (because of the nonexistence of poor local minima and the property of the saddle points). We note that even though we have advanced the theoretical foundations of deep learning, there is still a gap between theory and practice.},
  urldate = {2016-05-25},
  date = {2016-05-23},
  keywords = {Computer Science - Learning,Mathematics - Optimization and Control,Statistics - Machine Learning},
  author = {Kawaguchi, Kenji},
  file = {/home/arthur/Dropbox/Zotero/Kawaguchi_2016_Deep Learning without Poor Local Minima.pdf;/home/arthur/Zotero/storage/IQ6ZX6PG/1605.html}
}

@article{beck_convergence_2013,
  title = {On the Convergence of Block Coordinate Descent Type Methods},
  volume = {23},
  url = {http://epubs.siam.org/doi/abs/10.1137/120887679},
  number = {4},
  journaltitle = {SIAM Journal on Optimization},
  urldate = {2016-06-07},
  date = {2013},
  pages = {2037--2060},
  author = {Beck, Amir and Tetruashvili, Luba},
  file = {/home/arthur/Dropbox/Zotero/Beck_Tetruashvili_2013_On the convergence of block coordinate descent type methods.pdf;/home/arthur/Zotero/storage/B286HQ8P/120887679.html}
}

@article{necoara_random_2015,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1504.06340},
  primaryClass = {math},
  title = {Random Block Coordinate Descent Methods for Linearly Constrained Optimization over Networks},
  url = {http://arxiv.org/abs/1504.06340},
  abstract = {In this paper we develop random block coordinate gradient descent methods for minimizing large scale linearly constrained separable convex problems over networks. Since we have coupled constraints in the problem, we devise an algorithm that updates in parallel \$$\backslash$tau $\backslash$geq 2\$ (block) components per iteration. Moreover, for this method the computations can be performed in a distributed fashion according to the structure of the network. However, its complexity per iteration is usually cheaper than of the full gradient method when the number of nodes \$N\$ in the network is large. We prove that for this method we obtain in expectation an \$$\backslash$epsilon\$-accurate solution in at most \$$\backslash$mathcal\{O\}($\backslash$frac\{N\}\{$\backslash$tau $\backslash$epsilon\})\$ iterations and thus the convergence rate depends linearly on the number of (block) components \$$\backslash$tau\$ to be updated. For strongly convex functions the new method converges linearly. We also focus on how to choose the probabilities to make the randomized algorithm to converge as fast as possible and we arrive at solving a sparse SDP. Finally, we describe several applications that fit in our framework, in particular the convex feasibility problem. Numerically, we show that the parallel coordinate descent method with \$$\backslash$tau$>$2\$ accelerates on its basic counterpart corresponding to \$$\backslash$tau=2\$.},
  urldate = {2016-06-07},
  date = {2015-04-23},
  keywords = {Mathematics - Optimization and Control},
  author = {Necoara, I. and Nesterov, Yu and Glineur, F.},
  file = {/home/arthur/Dropbox/Zotero/Necoara et al_2015_Random block coordinate descent methods for linearly constrained optimization.pdf;/home/arthur/Zotero/storage/J8IJBX2V/1504.html}
}

@article{bonettini_inexact_2011,
  langid = {english},
  title = {Inexact Block Coordinate Descent Methods with Application to Non-Negative Matrix Factorization},
  volume = {31},
  issn = {0272-4979, 1464-3642},
  url = {http://imanum.oxfordjournals.org/cgi/doi/10.1093/imanum/drq024},
  number = {4},
  journaltitle = {IMA Journal of Numerical Analysis},
  urldate = {2016-06-08},
  date = {2011-10-01},
  pages = {1431-1452},
  author = {Bonettini, S.},
  file = {/home/arthur/Dropbox/Zotero/Bonettini_2011_Inexact block coordinate descent methods with application to non-negative.pdf}
}

@article{nesterov_efficiency_2010,
  title = {Efficiency of Coordinate Descent Methods on Huge-Scale Optimization Problems},
  url = {https://43284bdcf41602d9ee3b7f9155623bc4db1a5d55.googledrive.com/host/0B56ak7W-HmqASVBYZTJ1WXVoQUU/Nesterov%20-%20Efficiency%20of%20coordinate%20descent%20methods%20on%20huge-scale%20optimization%20problems.pdf},
  urldate = {2016-06-08},
  date = {2010},
  author = {Nesterov, Yu},
  file = {/home/arthur/Dropbox/Zotero/Nesterov_2010_Efficiency of coordinate descent methods on huge-scale optimization problems.pdf}
}

@inproceedings{li_improved_2016,
  title = {An {{Improved Convergence Analysis}} of {{Cyclic Block Coordinate Descent}}-Type {{Methods}} for {{Strongly Convex Minimization}}},
  url = {http://www.jmlr.org/proceedings/papers/v51/li16c.pdf},
  booktitle = {Proceedings of the 19th {{International Conference}} on {{Artificial Intelligence}} and {{Statistics}}},
  urldate = {2016-06-08},
  date = {2016},
  pages = {491--499},
  author = {Li, Xingguo and Zhao, Tuo and Arora, Raman and Liu, Han and Hong, Mingyi},
  file = {/home/arthur/Dropbox/Zotero/Li et al_2016_An Improved Convergence Analysis of Cyclic Block Coordinate Descent-type.pdf}
}

@article{hong_unified_2015,
  title = {A Unified Algorithmic Framework for Block-Structured Optimization Involving Big Data},
  url = {http://arxiv.org/abs/1511.02746},
  journaltitle = {arXiv preprint arXiv:1511.02746},
  urldate = {2016-06-08},
  date = {2015},
  author = {Hong, Mingyi and Razaviyayn, Meisam and Luo, Zhi-Quan and Pang, Jong-Shi},
  file = {/home/arthur/Dropbox/Zotero/Hong et al_2015_A unified algorithmic framework for block-structured optimization involving big.pdf;/home/arthur/Zotero/storage/RZZA9KUS/1511.html}
}

@inproceedings{zhi_dictionary_2014,
  title = {Dictionary Learning for Sparse Representation: Complexity and Algorithms},
  url = {http://www.redes.unb.br/lasp/files/events/ICASSP2014/papers/p5284-razaviyayn.pdf},
  shorttitle = {{{DICTIONARY LEARNING FOR SPARSE REPRESENTATION}}},
  booktitle = {Proceedings of {{ICCASP}}},
  urldate = {2016-06-08},
  date = {2014},
  author = {Zhi, Meisam Razaviyayn Hung-Wei Tseng and Luo, Quan},
  file = {/home/arthur/Dropbox/Zotero/Zhi_Luo_2014_Dictionary learning for sparse representation.pdf}
}

@article{necoara_random_2011,
  title = {A Random Coordinate Descent Method on Large Optimization Problems with Linear Constraints},
  url = {http://141.85.225.150/papers/RCD_coupledconstr.pdf},
  journaltitle = {University Politehnica Bucharest, Tech. Rep},
  urldate = {2016-06-08},
  date = {2011},
  author = {Necoara, I. and Nesterov, Y. and Glineur, F.},
  file = {/home/arthur/Dropbox/Zotero/Necoara et al_2011_A random coordinate descent method on large optimization problems with linear.pdf}
}

@incollection{zhao_accelerated_2014,
  title = {Accelerated {{Mini}}-Batch {{Randomized Block Coordinate Descent Method}}},
  url = {http://papers.nips.cc/paper/5614-accelerated-mini-batch-randomized-block-coordinate-descent-method.pdf},
  booktitle = {Advances in {{Neural Information Processing Systems}} 27},
  publisher = {{Curran Associates, Inc.}},
  urldate = {2016-06-08},
  date = {2014},
  pages = {3329--3337},
  author = {Zhao, Tuo and Yu, Mo and Wang, Yiming and Arora, Raman and Liu, Han},
  editor = {Ghahramani, Z. and Welling, M. and Cortes, C. and Lawrence, N. D. and Weinberger, K. Q.},
  file = {/home/arthur/Dropbox/Zotero/Zhao et al_2014_Accelerated Mini-batch Randomized Block Coordinate Descent Method.pdf;/home/arthur/Zotero/storage/572MDRQA/5614-accelerated-mini-batch-randomized-block-coordinate-descent-method.html}
}

@article{mairal_end--end_2016,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1605.06265},
  primaryClass = {cs, stat},
  title = {End-to-{{End Kernel Learning}} with {{Supervised Convolutional Kernel Networks}}},
  url = {http://arxiv.org/abs/1605.06265},
  abstract = {In this paper, we propose a new image representation based on a multilayer kernel machine that performs end-to-end learning. Unlike traditional kernel methods, where the kernel is handcrafted or adapted to data in an unsupervised manner, we learn how to shape the kernel for a supervised prediction problem. We proceed by generalizing convolutional kernel networks, which originally provide unsupervised image representations, and we derive backpropagation rules to optimize model parameters. As a result, we obtain a new type of convolutional neural network with the following properties: (i) at each layer, learning filters is equivalent to optimizing a linear subspace in a reproducing kernel Hilbert space (RKHS), where we project data, (ii) the network may be learned with supervision or without, (iii) the model comes with a natural regularization function (the norm in the RKHS). We show that our method achieves reasonably competitive performance on some standard "deep learning" image classification datasets such as CIFAR-10 and SVHN, and also state-of-the-art results for image super-resolution, demonstrating the applicability of our approach to a large variety of image-related tasks.},
  urldate = {2016-06-10},
  date = {2016-05-20},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Learning,Statistics - Machine Learning},
  author = {Mairal, Julien},
  file = {/home/arthur/Dropbox/Zotero/Mairal_2016_End-to-End Kernel Learning with Supervised Convolutional Kernel Networks.pdf;/home/arthur/Zotero/storage/F5FTK4JG/1605.html}
}

@article{mairal_convolutional_2014,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1406.3332},
  primaryClass = {cs, stat},
  title = {Convolutional {{Kernel Networks}}},
  url = {http://arxiv.org/abs/1406.3332},
  abstract = {An important goal in visual recognition is to devise image representations that are invariant to particular transformations. In this paper, we address this goal with a new type of convolutional neural network (CNN) whose invariance is encoded by a reproducing kernel. Unlike traditional approaches where neural networks are learned either to represent data or for solving a classification task, our network learns to approximate the kernel feature map on training data. Such an approach enjoys several benefits over classical ones. First, by teaching CNNs to be invariant, we obtain simple network architectures that achieve a similar accuracy to more complex ones, while being easy to train and robust to overfitting. Second, we bridge a gap between the neural network literature and kernels, which are natural tools to model invariance. We evaluate our methodology on visual recognition tasks where CNNs have proven to perform well, e.g., digit recognition with the MNIST dataset, and the more challenging CIFAR-10 and STL-10 datasets, where our accuracy is competitive with the state of the art.},
  urldate = {2016-06-10},
  date = {2014-06-12},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Learning,Statistics - Machine Learning},
  author = {Mairal, Julien and Koniusz, Piotr and Harchaoui, Zaid and Schmid, Cordelia},
  file = {/home/arthur/Dropbox/Zotero/Mairal et al_2014_Convolutional Kernel Networks.pdf;/home/arthur/Zotero/storage/8MB77JPE/1406.html}
}

@inproceedings{mensch_dictionary_2016,
  title = {Dictionary Learning for Massive Matrix Factorization},
  url = {http://arxiv.org/abs/1605.00937},
  booktitle = {33rd {{International Conference}} on {{Machine Learning}} ({{ICML}})},
  urldate = {2016-06-15},
  date = {2016-06},
  author = {Mensch, Arthur and Mairal, Julien and Thirion, Bertrand and Varoquaux, Gaël},
  file = {/home/arthur/Dropbox/Zotero/Mensch et al_2016_Dictionary learning for massive matrix factorization.pdf}
}

@inproceedings{mensch_compressed_2016,
  title = {Compressed {{Online Dictionary Learning}} for {{Fast fMRI Decomposition}}},
  url = {http://arxiv.org/abs/1602.02701},
  abstract = {We present a method for fast resting-state fMRI spatial decomposi-tions of very large datasets, based on the reduction of the temporal dimension before applying dictionary learning on concatenated individual records from groups of subjects. Introducing a measure of correspondence between spatial decompositions of rest fMRI, we demonstrates that time-reduced dictionary learning produces result as reliable as non-reduced decompositions. We also show that this reduction significantly improves computational scalability.},
  booktitle = {{{IEEE}} 13th {{International Symposium}} on {{Biomedical Imaging}} ({{ISBI}})},
  urldate = {2016-06-16},
  date = {2016-04},
  keywords = {Computer Science - Learning,Statistics - Machine Learning},
  author = {Mensch, Arthur and Varoquaux, Gaël and Thirion, Bertrand},
  file = {/home/arthur/Dropbox/Zotero/Mensch et al_2016_Compressed Online Dictionary Learning for Fast fMRI Decomposition.pdf;/home/arthur/Zotero/storage/8CR2VGBR/1602.html}
}

@article{bottou_optimization_2016,
  title = {Optimization {{Methods}} for {{Large}}-{{Scale Machine Learning}}},
  url = {http://arxiv.org/abs/1606.04838},
  journaltitle = {arXiv preprint arXiv:1606.04838},
  urldate = {2016-06-19},
  date = {2016},
  author = {Bottou, Léon and Curtis, Frank E. and Nocedal, Jorge},
  file = {/home/arthur/Dropbox/Zotero/Bottou et al_2016_Optimization Methods for Large-Scale Machine Learning.pdf}
}

@article{osokin_minding_2016,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1605.09346},
  primaryClass = {cs, math, stat},
  title = {Minding the {{Gaps}} for {{Block Frank}}-{{Wolfe Optimization}} of {{Structured SVMs}}},
  url = {http://arxiv.org/abs/1605.09346},
  abstract = {In this paper, we propose several improvements on the block-coordinate Frank-Wolfe (BCFW) algorithm from Lacoste-Julien et al. (2013) recently used to optimize the structured support vector machine (SSVM) objective in the context of structured prediction, though it has wider applications. The key intuition behind our improvements is that the estimates of block gaps maintained by BCFW reveal the block suboptimality that can be used as an adaptive criterion. First, we sample objects at each iteration of BCFW in an adaptive non-uniform way via gapbased sampling. Second, we incorporate pairwise and away-step variants of Frank-Wolfe into the block-coordinate setting. Third, we cache oracle calls with a cache-hit criterion based on the block gaps. Fourth, we provide the first method to compute an approximate regularization path for SSVM. Finally, we provide an exhaustive empirical evaluation of all our methods on four structured prediction datasets.},
  urldate = {2016-06-22},
  date = {2016-05-30},
  keywords = {Computer Science - Learning,Mathematics - Optimization and Control,Statistics - Machine Learning,90C52; 90C90; 90C06; 68T05,G.1.6,I.2.6},
  author = {Osokin, Anton and Alayrac, Jean-Baptiste and Lukasewitz, Isabella and Dokania, Puneet K. and Lacoste-Julien, Simon},
  file = {/home/arthur/Dropbox/Zotero/Osokin et al_2016_Minding the Gaps for Block Frank-Wolfe Optimization of Structured SVMs.pdf;/home/arthur/Zotero/storage/5MI526HG/1605.html}
}

@article{hazan_graduated_2015,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1503.03712},
  primaryClass = {cs, math},
  title = {On {{Graduated Optimization}} for {{Stochastic Non}}-{{Convex Problems}}},
  url = {http://arxiv.org/abs/1503.03712},
  abstract = {The graduated optimization approach, also known as the continuation method, is a popular heuristic to solving non-convex problems that has received renewed interest over the last decade. Despite its popularity, very little is known in terms of theoretical convergence analysis. In this paper we describe a new first-order algorithm based on graduated optimiza- tion and analyze its performance. We characterize a parameterized family of non- convex functions for which this algorithm provably converges to a global optimum. In particular, we prove that the algorithm converges to an \{$\backslash$epsilon\}-approximate solution within O(1/$\backslash$epsilon\^2) gradient-based steps. We extend our algorithm and analysis to the setting of stochastic non-convex optimization with noisy gradient feedback, attaining the same convergence rate. Additionally, we discuss the setting of zero-order optimization, and devise a a variant of our algorithm which converges at rate of O(d\^2/$\backslash$epsilon\^4).},
  urldate = {2016-06-22},
  date = {2015-03-12},
  keywords = {Computer Science - Learning,Mathematics - Optimization and Control,68},
  author = {Hazan, Elad and Levy, Kfir Y. and Shalev-Shwartz, Shai},
  file = {/home/arthur/Dropbox/Zotero/Hazan et al_2015_On Graduated Optimization for Stochastic Non-Convex Problems.pdf;/home/arthur/Zotero/storage/H25Q874V/1503.html}
}

@article{dwork_reusable_2015,
  title = {The Reusable Holdout: {{Preserving}} Validity in Adaptive Data Analysis},
  volume = {349},
  url = {http://science.sciencemag.org/content/349/6248/636.short},
  shorttitle = {The Reusable Holdout},
  number = {6248},
  journaltitle = {Science},
  urldate = {2016-06-26},
  date = {2015},
  pages = {636--638},
  author = {Dwork, Cynthia and Feldman, Vitaly and Hardt, Moritz and Pitassi, Toniann and Reingold, Omer and Roth, Aaron},
  file = {/home/arthur/Zotero/storage/XECDDCMS/636.html}
}

@article{marblestone_towards_2016,
  title = {Towards an Integration of Deep Learning and Neuroscience},
  url = {http://arxiv.org/abs/1606.03813},
  journaltitle = {arXiv preprint arXiv:1606.03813},
  urldate = {2016-06-26},
  date = {2016},
  author = {Marblestone, Adam and Wayne, Greg and Kording, Konrad},
  file = {/home/arthur/Dropbox/Zotero/Marblestone et al_2016_Towards an integration of deep learning and neuroscience.pdf;/home/arthur/Zotero/storage/XVJ97ZIS/1606.html}
}

@inproceedings{zheng_neural_2016,
  title = {A {{Neural Autoregressive Approach}} to {{Collaborative Filtering}}},
  url = {http://jmlr.org/proceedings/papers/v48/zheng16.html},
  eventtitle = {Proceedings of {{The}} 33rd {{International Conference}} on {{Machine Learning}}},
  urldate = {2016-06-28},
  date = {2016},
  pages = {764-773},
  author = {Zheng, Yin and Tang, Bangsheng and Ding, Wenkui and Zhou, Hanning},
  file = {/home/arthur/Dropbox/Zotero/Zheng et al_2016_A Neural Autoregressive Approach to Collaborative Filtering.pdf;/home/arthur/Zotero/storage/UN4R3WDR/zheng16.html}
}

@article{mania_perturbed_2015,
  title = {Perturbed Iterate Analysis for Asynchronous Stochastic Optimization},
  url = {http://arxiv.org/abs/1507.06970},
  journaltitle = {arXiv preprint arXiv:1507.06970},
  urldate = {2016-07-04},
  date = {2015},
  author = {Mania, Horia and Pan, Xinghao and Papailiopoulos, Dimitris and Recht, Benjamin and Ramchandran, Kannan and Jordan, Michael I.},
  file = {/home/arthur/Dropbox/Zotero/Mania et al_2015_Perturbed iterate analysis for asynchronous stochastic optimization.pdf;/home/arthur/Zotero/storage/7V8MWAQZ/1507.html}
}

@article{eklund_cluster_2016,
  langid = {english},
  title = {Cluster Failure: {{Why fMRI}} Inferences for Spatial Extent Have Inflated False-Positive Rates},
  issn = {0027-8424, 1091-6490},
  url = {http://www.pnas.org/content/early/2016/06/27/1602413113},
  shorttitle = {Cluster Failure},
  abstract = {The most widely used task functional magnetic resonance imaging (fMRI) analyses use parametric statistical methods that depend on a variety of assumptions. In this work, we use real resting-state data and a total of 3 million random task group analyses to compute empirical familywise error rates for the fMRI software packages SPM, FSL, and AFNI, as well as a nonparametric permutation method. For a nominal familywise error rate of 5\%, the parametric statistical methods are shown to be conservative for voxelwise inference and invalid for clusterwise inference. Our results suggest that the principal cause of the invalid cluster inferences is spatial autocorrelation functions that do not follow the assumed Gaussian shape. By comparison, the nonparametric permutation test is found to produce nominal results for voxelwise as well as clusterwise inference. These findings speak to the need of validating the statistical methods being used in the field of neuroimaging.},
  journaltitle = {Proceedings of the National Academy of Sciences},
  shortjournal = {PNAS},
  urldate = {2016-07-04},
  date = {2016-06-28},
  pages = {201602413},
  keywords = {fMRI,statistics,false positives,cluster inference,permutation test},
  author = {Eklund, Anders and Nichols, Thomas E. and Knutsson, Hans},
  file = {/home/arthur/Dropbox/Zotero/Eklund et al_2016_Cluster failure.pdf;/home/arthur/Zotero/storage/658MJIP6/1602413113.html},
  eprinttype = {pmid},
  eprint = {27357684}
}

@article{li_efficient_2016,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1603.06560},
  primaryClass = {cs, stat},
  title = {Efficient {{Hyperparameter Optimization}} and {{Infinitely Many Armed Bandits}}},
  url = {http://arxiv.org/abs/1603.06560},
  abstract = {Performance of machine learning algorithms depends critically on identifying a good set of hyperparameters. While current methods offer efficiencies by adaptively choosing new configurations to train, an alternative strategy is to adaptively allocate resources across the selected configurations. We formulate hyperparameter optimization as a pure-exploration non-stochastic infinitely many armed bandit problem where allocation of additional resources to an arm corresponds to training a configuration on larger subsets of the data. We introduce Hyperband for this framework and analyze its theoretical properties, providing several desirable guarantees. We compare Hyperband with state-of-the-art Bayesian optimization methods and a random search baseline on a comprehensive benchmark including 117 datasets. Our results on this benchmark demonstrate that while Bayesian optimization methods do not outperform random search trained for twice as long, Hyperband in favorable settings offers valuable speedups.},
  urldate = {2016-07-06},
  date = {2016-03-21},
  keywords = {Computer Science - Learning,Statistics - Machine Learning},
  author = {Li, Lisha and Jamieson, Kevin and DeSalvo, Giulia and Rostamizadeh, Afshin and Talwalkar, Ameet},
  file = {/home/arthur/Dropbox/Zotero/Li et al_2016_Efficient Hyperparameter Optimization and Infinitely Many Armed Bandits.pdf;/home/arthur/Zotero/storage/5Q8X26Q4/1603.html}
}

@article{le_gall_integration_2006,
  title = {Intégration, Probabilités et Processus Aléatoires},
  url = {http://www.math.u-psud.fr/~stafav/IMG/pdf/Legall.pdf},
  journaltitle = {Ecole Normale Supérieure de Paris},
  urldate = {2016-07-13},
  date = {2006},
  author = {Le Gall, Jean-François},
  file = {/home/arthur/Dropbox/Zotero/Le Gall_2006_Intégration, probabilités et processus aléatoires.pdf}
}

@article{li_online_nodate,
  title = {Online {{Low}}-{{Rank Subspace Clustering}} by {{Basis Dictionary Pursuit}}},
  url = {http://www.jmlr.org/proceedings/papers/v48/shen16.pdf},
  urldate = {2016-07-13},
  author = {Li, Ping and Xu, Huan},
  file = {/home/arthur/Dropbox/Zotero/Li_Xu_Online Low-Rank Subspace Clustering by Basis Dictionary Pursuit.pdf}
}

@article{soltani-farani_spatial-aware_2015,
  title = {Spatial-{{Aware Dictionary Learning}} for {{Hyperspectral Image Classification}}},
  volume = {53},
  issn = {0196-2892},
  doi = {10.1109/TGRS.2014.2325067},
  abstract = {This paper presents a structured dictionary-based model for hyperspectral data that incorporates both spectral and contextual characteristics of spectral samples. The idea is to partition the pixels of a hyperspectral image into a number of spatial neighborhoods called contextual groups and to model the pixels inside a group as members of a common subspace. That is, each pixel is represented using a linear combination of a few dictionary elements learned from the data, but since pixels inside a contextual group are often made up of the same materials, their linear combinations are constrained to use common elements from the dictionary. To this end, dictionary learning is carried out with a joint sparse regularizer to induce a common sparsity pattern in the sparse coefficients of a contextual group. The sparse coefficients are then used for classification using a linear support vector machine. Experimental results on a number of real hyperspectral images confirm the effectiveness of the proposed representation for hyperspectral image classification. Moreover, experiments with simulated multispectral data show that the proposed model is capable of finding representations that may effectively be used for classification of multispectral resolution samples.},
  number = {1},
  journaltitle = {IEEE Transactions on Geoscience and Remote Sensing},
  date = {2015-01},
  pages = {527-541},
  keywords = {Vectors,classification,dictionaries,image classification,support vector machines,geophysical image processing,hyperspectral imaging,common sparsity pattern,contextual group,hyperspectral data,hyperspectral image classification,linear support vector machine,multispectral resolution sample classification,spatial neighborhoods,spatial-aware dictionary learning,spectral sample contextual characteristics,spectral sample spectral characteristics,structured dictionary-based model,Data models,Linear programming,dictionary learning,hyperspectral imagery (HSI),linear support vector machines (SVMs),probabilistic joint sparse model},
  author = {Soltani-Farani, A. and Rabiee, H. R. and Hosseini, S. A.},
  file = {/home/arthur/Dropbox/Zotero/Soltani-Farani et al_2015_Spatial-Aware Dictionary Learning for Hyperspectral Image Classification.pdf;/home/arthur/Zotero/storage/3HCQEFK2/abs_all.html}
}

@inproceedings{fu_adaptive_2015,
  title = {Adaptive {{Spatial}}-{{Spectral Dictionary Learning}} for {{Hyperspectral Image Denoising}}},
  doi = {10.1109/ICCV.2015.47},
  abstract = {Hyperspectral imaging is beneficial in a diverse range of applications from diagnostic medicine, to agriculture, to surveillance to name a few. However, hyperspectral images often times suffer from degradation due to the limited light, which introduces noise into the imaging process. In this paper, we propose an effective model for hyperspectral image (HSI) denoising that considers underlying characteristics of HSIs: sparsity across the spatial-spectral domain, high correlation across spectra, and non-local self-similarity over space. We first exploit high correlation across spectra and non-local self-similarity over space in the noisy HSI to learn an adaptive spatial-spectral dictionary. Then, we employ the local and non-local sparsity of the HSI under the learned spatial-spectral dictionary to design an HSI denoising model, which can be effectively solved by an iterative numerical algorithm with parameters that are adaptively adjusted for different clusters and different noise levels. Experimental results on HSI denoising show that the proposed method can provide substantial improvements over the current state-of-the-art HSI denoising methods in terms of both objective metric and subjective visual quality.},
  eventtitle = {2015 {{IEEE International Conference}} on {{Computer Vision}} ({{ICCV}})},
  booktitle = {2015 {{IEEE International Conference}} on {{Computer Vision}} ({{ICCV}})},
  date = {2015-12},
  pages = {343-351},
  keywords = {Principal Component Analysis,dictionaries,geophysical image processing,hyperspectral imaging,image denoising,iterative methods,HSI denoising model,adaptive spatial-spectral dictionary learning,diagnostic medicine,hyperspectral image denoising,iterative numerical algorithm,Adaptation models,Noise reduction,Numerical models,Spectral analysis},
  author = {Fu, Y. and Lam, A. and Sato, I. and Sato, Y.},
  file = {/home/arthur/Dropbox/Zotero/Fu et al_2015_Adaptive Spatial-Spectral Dictionary Learning for Hyperspectral Image Denoising.pdf;/home/arthur/Zotero/storage/QNGMJXBU/abs_all.html}
}

@article{zhang_hyperspectral_2014,
  title = {Hyperspectral {{Image Restoration Using Low}}-{{Rank Matrix Recovery}}},
  volume = {52},
  issn = {0196-2892},
  doi = {10.1109/TGRS.2013.2284280},
  abstract = {Hyperspectral images (HSIs) are often degraded by a mixture of various kinds of noise in the acquisition process, which can include Gaussian noise, impulse noise, dead lines, stripes, and so on. This paper introduces a new HSI restoration method based on low-rank matrix recovery (LRMR), which can simultaneously remove the Gaussian noise, impulse noise, dead lines, and stripes. By lexicographically ordering a patch of the HSI into a 2-D matrix, the low-rank property of the hyperspectral imagery is explored, which suggests that a clean HSI patch can be regarded as a low-rank matrix. We then formulate the HSI restoration problem into an LRMR framework. To further remove the mixed noise, the “Go Decomposition” algorithm is applied to solve the LRMR problem. Several experiments were conducted in both simulated and real data conditions to verify the performance of the proposed LRMR-based HSI restoration method.},
  number = {8},
  journaltitle = {IEEE Transactions on Geoscience and Remote Sensing},
  date = {2014-08},
  pages = {4729-4743},
  keywords = {Sparse matrices,low rank,hyperspectral imaging,Noise reduction,image restoration,2-D matrix,Gaussian noise,dead lines,go decomposition algorithm,hyperspectral image restoration,impulse noise,low-rank matrix recovery,low-rank property,stripes,Matrix decomposition,Go Decomposition (GoDec),hyperspectral image (HSIs),restoration},
  author = {Zhang, H. and He, W. and Zhang, L. and Shen, H. and Yuan, Q.},
  file = {/home/arthur/Dropbox/Zotero/Zhang et al_2014_Hyperspectral Image Restoration Using Low-Rank Matrix Recovery.pdf;/home/arthur/Zotero/storage/IXZBB3SV/abs_all.html}
}

@article{wright_sparse_2009,
  title = {Sparse Reconstruction by Separable Approximation},
  volume = {57},
  url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4799134},
  number = {7},
  journaltitle = {IEEE Transactions on Signal Processing},
  urldate = {2016-09-02},
  date = {2009},
  pages = {2479--2493},
  author = {Wright, Stephen J. and Nowak, Robert D. and Figueiredo, Mário AT},
  file = {/home/arthur/Dropbox/Zotero/Wright et al_2009_Sparse reconstruction by separable approximation.pdf;/home/arthur/Zotero/storage/DIR8K5UR/4799134.html}
}

@inproceedings{covington_deep_2016,
  langid = {english},
  title = {Deep {{Neural Networks}} for {{YouTube Recommendations}}},
  isbn = {978-1-4503-4035-9},
  url = {http://dl.acm.org/citation.cfm?doid=2959100.2959190},
  publisher = {{ACM Press}},
  urldate = {2016-09-05},
  date = {2016},
  pages = {191-198},
  author = {Covington, Paul and Adams, Jay and Sargin, Emre},
  file = {/home/arthur/Dropbox/Zotero/Covington et al_2016_Deep Neural Networks for YouTube Recommendations.pdf}
}

@article{chen_integrating_2016,
  title = {Integrating Multiple Random Sketches for Singular Value Decomposition},
  url = {http://arxiv.org/abs/1608.08285},
  journaltitle = {arXiv preprint arXiv:1608.08285},
  urldate = {2016-09-06},
  date = {2016},
  author = {Chen, Ting-Li and Chang, Dawei D. and Huang, Su-Yun and Chen, Hung and Lin, Chienyao and Wang, Weichung},
  file = {/home/arthur/Dropbox/Zotero/Chen et al_2016_Integrating multiple random sketches for singular value decomposition.pdf}
}

@book{chen_stochastic_2002,
  langid = {english},
  location = {{Dordrecht; Boston}},
  title = {Stochastic Approximation and Its Applications},
  isbn = {978-0-306-48166-6 978-1-4020-0806-1},
  url = {http://site.ebrary.com/id/10067313},
  abstract = {This book presents the recent development of stochastic approximation algorithms with expanding truncations based on the TS (trajectory-subsequence) method, a newly developed method for convergence analysis. This approach is so powerful that conditions used for guaranteeing convergence have been considerably weakened in comparison with those applied in the classical probability and ODE methods. The general convergence theorem is presented for sample paths and is proved in a purely deterministic way. The sample-path description of theorems is particularly convenient for applications. Convergence theory takes both observation noise and structural error of the regression function into consideration. Convergence rates, asymptotic normality and other asymptotic properties are presented as well. Applications of the developed theory to global optimization, blind channel identification, adaptive filtering, system parameter identification, adaptive stabilization and other problems arising from engineering fields are demonstrated.},
  publisher = {{Kluwer Academic Publishers}},
  urldate = {2016-09-06},
  date = {2002},
  author = {Chen, Hanfu},
  file = {/home/arthur/Dropbox/Zotero/Chen_2002_Stochastic approximation and its applications.pdf},
  note = {OCLC: 54386080}
}

@article{ziniel_binary_2015,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1401.0872},
  title = {Binary {{Linear Classification}} and {{Feature Selection}} via {{Generalized Approximate Message Passing}}},
  volume = {63},
  issn = {1053-587X, 1941-0476},
  url = {http://arxiv.org/abs/1401.0872},
  abstract = {For the problem of binary linear classification and feature selection, we propose algorithmic approaches to classifier design based on the generalized approximate message passing (GAMP) algorithm, recently proposed in the context of compressive sensing. We are particularly motivated by problems where the number of features greatly exceeds the number of training examples, but where only a few features suffice for accurate classification. We show that sum-product GAMP can be used to (approximately) minimize the classification error rate and max-sum GAMP can be used to minimize a wide variety of regularized loss functions. Furthermore, we describe an expectation-maximization (EM)-based scheme to learn the associated model parameters online, as an alternative to cross-validation, and we show that GAMP's state-evolution framework can be used to accurately predict the misclassification rate. Finally, we present a detailed numerical study to confirm the accuracy, speed, and flexibility afforded by our GAMP-based approaches to binary linear classification and feature selection.},
  number = {8},
  journaltitle = {IEEE Transactions on Signal Processing},
  urldate = {2016-09-06},
  date = {2015-04},
  pages = {2020-2032},
  keywords = {Statistics - Machine Learning,Computer Science - Information Theory},
  author = {Ziniel, Justin and Schniter, Philip and Sederberg, Per},
  file = {/home/arthur/Dropbox/Zotero/Ziniel et al_2015_Binary Linear Classification and Feature Selection via Generalized Approximate.pdf;/home/arthur/Zotero/storage/VAV4SBZS/1401.html}
}

@article{zhang_convexified_2016,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1609.01000},
  primaryClass = {cs},
  title = {Convexified {{Convolutional Neural Networks}}},
  url = {http://arxiv.org/abs/1609.01000},
  abstract = {We describe the class of convexified convolutional neural networks (CCNNs), which capture the parameter sharing of convolutional neural networks in a convex manner. By representing the nonlinear convolutional filters as vectors in a reproducing kernel Hilbert space, the CNN parameters can be represented as a low-rank matrix, which can be relaxed to obtain a convex optimization problem. For learning two-layer convolutional neural networks, we prove that the generalization error obtained by a convexified CNN converges to that of the best possible CNN. For learning deeper networks, we train CCNNs in a layer-wise manner. Empirically, CCNNs achieve performance competitive with CNNs trained by backpropagation, SVMs, fully-connected neural networks, stacked denoising auto-encoders, and other baseline methods.},
  urldate = {2016-09-06},
  date = {2016-09-04},
  keywords = {Computer Science - Learning},
  author = {Zhang, Yuchen and Liang, Percy and Wainwright, Martin J.},
  file = {/home/arthur/Dropbox/Zotero/Zhang et al_2016_Convexified Convolutional Neural Networks.pdf;/home/arthur/Zotero/storage/AIN6IMC4/1609.html}
}

@article{rangan_generalized_2010,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1010.5141},
  primaryClass = {cs, math},
  title = {Generalized {{Approximate Message Passing}} for {{Estimation}} with {{Random Linear Mixing}}},
  url = {http://arxiv.org/abs/1010.5141},
  abstract = {We consider the estimation of an i.i.d.$\backslash$ random vector observed through a linear transform followed by a componentwise, probabilistic (possibly nonlinear) measurement channel. A novel algorithm, called generalized approximate message passing (GAMP), is presented that provides computationally efficient approximate implementations of max-sum and sum-problem loopy belief propagation for such problems. The algorithm extends earlier approximate message passing methods to incorporate arbitrary distributions on both the input and output of the transform and can be applied to a wide range of problems in nonlinear compressed sensing and learning. Extending an analysis by Bayati and Montanari, we argue that the asymptotic componentwise behavior of the GAMP method under large, i.i.d. Gaussian transforms is described by a simple set of state evolution (SE) equations. From the SE equations, one can $\backslash$emph\{exactly\} predict the asymptotic value of virtually any componentwise performance metric including mean-squared error or detection accuracy. Moreover, the analysis is valid for arbitrary input and output distributions, even when the corresponding optimization problems are non-convex. The results match predictions by Guo and Wang for relaxed belief propagation on large sparse matrices and, in certain instances, also agree with the optimal performance predicted by the replica method. The GAMP methodology thus provides a computationally efficient methodology, applicable to a large class of non-Gaussian estimation problems with precise asymptotic performance guarantees.},
  urldate = {2016-09-08},
  date = {2010-10-25},
  keywords = {Computer Science - Information Theory},
  author = {Rangan, Sundeep},
  file = {/home/arthur/Dropbox/Zotero/Rangan_2010_Generalized Approximate Message Passing for Estimation with Random Linear Mixing.pdf;/home/arthur/Zotero/storage/5QSU455R/1010.html}
}

@inproceedings{pennington_glove_2014,
  title = {Glove: {{Global Vectors}} for {{Word Representation}}.},
  volume = {14},
  url = {http://llcao.net/cu-deeplearning15/presentation/nn-pres.pdf},
  shorttitle = {Glove},
  booktitle = {{{EMNLP}}},
  urldate = {2016-09-19},
  date = {2014},
  pages = {1532--43},
  author = {Pennington, Jeffrey and Socher, Richard and Manning, Christopher D.},
  file = {/home/arthur/Dropbox/Zotero/Pennington et al_2014_Glove.pdf}
}

@article{xing_dictionary_2012,
  title = {Dictionary {{Learning}} for {{Noisy}} and {{Incomplete Hyperspectral Images}}},
  volume = {5},
  url = {http://epubs.siam.org/doi/abs/10.1137/110837486},
  abstract = {We consider analysis of noisy and incomplete hyperspectral imagery, with the objective of removing the noise and inferring the missing data. The noise statistics may be wavelength dependent, and the fraction of data missing (at random) may be substantial, including potentially entire bands, offering the potential to significantly reduce the quantity of data that need be measured. To achieve this objective, the imagery is divided into contiguous three-dimensional (3D) spatio-spectral blocks of spatial dimension much less than the image dimension. It is assumed that each such 3D block may be represented as a linear combination of dictionary elements of the same dimension, plus noise, and the dictionary elements are learned in situ based on the observed data (no a priori training). The number of dictionary elements needed for representation of any particular block is typically small relative to the block dimensions, and all the image blocks are processed jointly (“collaboratively") to infer the underlying dictionary. We address dictionary learning from a Bayesian perspective, considering two distinct means of imposing sparse dictionary usage. These models allow inference of the number of dictionary elements needed as well as the underlying wavelength-dependent noise statistics. It is demonstrated that drawing the dictionary elements from a Gaussian process prior, imposing structure on the wavelength dependence of the dictionary elements, yields significant advantages, relative to the more conventional approach of using an independent and identically distributed Gaussian prior for the dictionary elements; this advantage is particularly evident in the presence of noise. The framework is demonstrated by processing hyperspectral imagery with a significant number of voxels missing uniformly at random, with imagery at specific wavelengths missing entirely, and in the presence of substantial additive noise.},
  number = {1},
  journaltitle = {SIAM Journal on Imaging Sciences},
  shortjournal = {SIAM J. Imaging Sci.},
  urldate = {2016-09-19},
  date = {2012-01-01},
  pages = {33-56},
  author = {Xing, Z. and Zhou, M. and Castrodad, A. and Sapiro, G. and Carin, L.},
  file = {/home/arthur/Dropbox/Zotero/Xing et al_2012_Dictionary Learning for Noisy and Incomplete Hyperspectral Images.pdf;/home/arthur/Zotero/storage/WTQCCDBE/110837486.html}
}

@article{reddi_large-scale_2014,
  title = {Large-Scale Randomized-Coordinate Descent Methods with Non-Separable Linear Constraints},
  url = {http://arxiv.org/abs/1409.2617},
  journaltitle = {arXiv preprint arXiv:1409.2617},
  urldate = {2016-09-20},
  date = {2014},
  author = {Reddi, Sashank and Hefny, Ahmed and Downey, Carlton and Dubey, Avinava and Sra, Suvrit},
  file = {/home/arthur/Dropbox/Zotero/Reddi et al_2014_Large-scale randomized-coordinate descent methods with non-separable linear.pdf}
}

@article{chen_hyperspectral_2011,
  title = {Hyperspectral Image Classification Using Dictionary-Based Sparse Representation},
  volume = {49},
  url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5766028},
  number = {10},
  journaltitle = {IEEE Transactions on Geoscience and Remote Sensing},
  urldate = {2016-09-21},
  date = {2011},
  pages = {3973--3985},
  author = {Chen, Yi and Nasrabadi, Nasser M. and Tran, Trac D.},
  file = {/home/arthur/Dropbox/Zotero/Chen et al_2011_Hyperspectral image classification using dictionary-based sparse representation.pdf;/home/arthur/Zotero/storage/VR5U6R7P/5766028.html}
}

@inproceedings{agarwal_learning_2014,
  title = {Learning Sparsely Used Overcomplete Dictionaries},
  url = {http://www.jmlr.org/proceedings/papers/v35/agarwal14a.pdf},
  booktitle = {{{COLT}}},
  urldate = {2016-09-22},
  date = {2014},
  pages = {123--137},
  author = {Agarwal, Alekh and Anandkumar, Animashree and Jain, Prateek and Netrapalli, Praneeth and Tandon, Rashish},
  file = {/home/arthur/Dropbox/Zotero/Agarwal et al_2014_Learning sparsely used overcomplete dictionaries.pdf}
}

@article{brown_martingale_1971,
  title = {Martingale Central Limit Theorems},
  volume = {42},
  url = {http://projecteuclid.org/euclid.aoms/1177693494},
  number = {1},
  journaltitle = {The Annals of Mathematical Statistics},
  urldate = {2016-09-22},
  date = {1971},
  pages = {59--66},
  author = {Brown, Bruce M. and {others}},
  file = {/home/arthur/Zotero/storage/7B5K7ZGX/1177693494.html}
}

@article{cukur_functional_2016,
  langid = {english},
  title = {Functional {{Subdomains}} within {{Scene}}-{{Selective Cortex}}: {{Parahippocampal Place Area}}, {{Retrosplenial Complex}}, and {{Occipital Place Area}}},
  volume = {36},
  issn = {0270-6474, 1529-2401},
  url = {http://www.jneurosci.org/cgi/doi/10.1523/JNEUROSCI.4033-14.2016},
  shorttitle = {Functional {{Subdomains}} within {{Scene}}-{{Selective Cortex}}},
  number = {40},
  journaltitle = {Journal of Neuroscience},
  urldate = {2016-10-13},
  date = {2016-10-05},
  pages = {10257-10273},
  author = {Cukur, T. and Huth, A. G. and Nishimoto, S. and Gallant, J. L.},
  file = {/home/arthur/Dropbox/Zotero/Cukur et al_2016_Functional Subdomains within Scene-Selective Cortex.pdf}
}

@thesis{elyaderani_convergence_2014,
  title = {Convergence {{Analysis}} of the {{Approximate Proximal Splitting Method}} for {{Non}}-{{Smooth Convex Optimization}}},
  url = {http://conservancy.umn.edu/handle/11299/165536},
  institution = {{UNIVERSITY OF MINNESOTA}},
  urldate = {2016-10-13},
  date = {2014},
  author = {Elyaderani, Mojtaba Kadkhodaie},
  file = {/home/arthur/Dropbox/Zotero/Elyaderani_2014_Convergence Analysis of the Approximate Proximal Splitting Method for.pdf}
}

@article{hoyos-idrobo_recursive_2016,
  title = {Recursive Nearest Agglomeration ({{ReNA}}): Fast Clustering for Approximation of Structured Signals},
  url = {https://arxiv.org/abs/1609.04608},
  shorttitle = {Recursive Nearest Agglomeration ({{ReNA}})},
  journaltitle = {arXiv preprint arXiv:1609.04608},
  urldate = {2016-10-13},
  date = {2016},
  author = {Hoyos-Idrobo, Andrés and Varoquaux, Gaël and Kahn, Jonas and Thirion, Bertrand},
  file = {/home/arthur/Dropbox/Zotero/Hoyos-Idrobo et al_2016_Recursive nearest agglomeration (ReNA).pdf}
}

@inproceedings{frongillo_convergence_2015,
  title = {Convergence Analysis of Prediction Markets via Randomized Subspace Descent},
  url = {http://papers.nips.cc/paper/5727-convergence-analysis-of-prediction-markets-via-randomized-subspace-descent},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  urldate = {2016-10-13},
  date = {2015},
  pages = {3034--3042},
  author = {Frongillo, Rafael and Reid, Mark D.},
  file = {/home/arthur/Dropbox/Zotero/Frongillo_Reid_2015_Convergence analysis of prediction markets via randomized subspace descent.pdf}
}

@article{wright_coordinate_2015,
  title = {Coordinate Descent Algorithms},
  volume = {151},
  url = {http://link.springer.com/article/10.1007/s10107-015-0892-3},
  number = {1},
  journaltitle = {Mathematical Programming},
  urldate = {2016-10-13},
  date = {2015},
  pages = {3--34},
  author = {Wright, Stephen J.},
  file = {/home/arthur/Dropbox/Zotero/Wright_2015_Coordinate descent algorithms.pdf;/home/arthur/Zotero/storage/38JZKKW2/fulltext.html;/home/arthur/Zotero/storage/FRGSAHFA/s10107-015-0892-3.html}
}

@article{beck_convergence_2015,
  title = {On the {{Convergence}} of {{Alternating Minimization}} for {{Convex Programming}} with {{Applications}} to {{Iteratively Reweighted Least Squares}} and {{Decomposition Schemes}}},
  volume = {25},
  issn = {1052-6234},
  url = {http://epubs.siam.org/doi/abs/10.1137/13094829X},
  abstract = {This paper is concerned with the alternating minimization (AM) method for solving convex minimization problems where the decision variables vector is split into two blocks. The objective function is a sum of a differentiable convex function and a separable (possibly) nonsmooth extended real-valued convex function, and consequently constraints can be incorporated. We analyze the convergence rate of the method  and establish a nonasymptotic sublinear rate of convergence where the multiplicative constant depends on the minimal block Lipschitz constant. We then analyze the iteratively reweighted least squares (IRLS) method for solving convex problems involving sums of norms. Based on the results derived for the AM method, we establish a nonasymptotic sublinear rate of convergence of the IRLS method. In addition, we show an asymptotic rate of convergence whose efficiency estimate does not depend on the data of the problem. Finally, we study the convergence properties of a decomposition-based approach designed to solve a composite convex model.},
  number = {1},
  journaltitle = {SIAM Journal on Optimization},
  shortjournal = {SIAM J. Optim.},
  urldate = {2016-10-13},
  date = {2015-01-01},
  pages = {185-209},
  author = {Beck, A.},
  file = {/home/arthur/Dropbox/Zotero/Beck_2015_On the Convergence of Alternating Minimization for Convex Programming with.pdf;/home/arthur/Zotero/storage/7BWVGBJX/13094829X.html}
}

@article{hare_survival_2017,
  langid = {english},
  title = {Survival of the {{Friendliest}}: {{{\emph{Homo}}}}{\emph{ Sapiens}} {{Evolved}} via {{Selection}} for {{Prosociality}}},
  volume = {68},
  issn = {0066-4308, 1545-2085},
  url = {http://www.annualreviews.org/doi/10.1146/annurev-psych-010416-044201},
  shorttitle = {Survival of the {{Friendliest}}},
  number = {1},
  journaltitle = {Annual Review of Psychology},
  urldate = {2016-10-14},
  date = {2017-01-10},
  author = {Hare, Brian},
  file = {/home/arthur/Dropbox/Zotero/Hare_2017_Survival of the Friendliest.pdf}
}

@article{fountoulakis_robust_2014,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1407.7573},
  primaryClass = {math},
  title = {Robust {{Block Coordinate Descent}}},
  url = {http://arxiv.org/abs/1407.7573},
  abstract = {In this paper we present a novel randomized block coordinate descent method for the minimization of a convex composite objective function. The method uses (approximate) partial second-order (curvature) information, so that the algorithm performance is more robust when applied to highly nonseparable or ill conditioned problems. We call the method Robust Coordinate Descent (RCD). At each iteration of RCD, a block of coordinates is sampled randomly, a quadratic model is formed about that block and the model is minimized approximately/inexactly to determine the search direction. An inexpensive line search is then employed to ensure a monotonic decrease in the objective function and acceptance of large step sizes. We prove global convergence of the RCD algorithm, and we also present several results on the local convergence of RCD for strongly convex functions. Finally, we present numerical results on large-scale problems to demonstrate the practical performance of the method.},
  urldate = {2016-11-03},
  date = {2014-07-28},
  keywords = {Mathematics - Optimization and Control},
  author = {Fountoulakis, Kimon and Tappenden, Rachael},
  file = {/home/arthur/Dropbox/Zotero/Fountoulakis_Tappenden_2014_Robust Block Coordinate Descent.pdf;/home/arthur/Zotero/storage/HM577ZBK/1407.html}
}

@article{richtarik_parallel_2015,
  langid = {english},
  title = {Parallel Coordinate Descent Methods for Big Data Optimization},
  volume = {156},
  issn = {0025-5610, 1436-4646},
  url = {http://link.springer.com/article/10.1007/s10107-015-0901-6},
  abstract = {In this work we show that randomized (block) coordinate descent methods can be accelerated by parallelization when applied to the problem of minimizing the sum of a partially separable smooth convex function and a simple separable convex function. The theoretical speedup, as compared to the serial method, and referring to the number of iterations needed to approximately solve the problem with high probability, is a simple expression depending on the number of parallel processors and a natural and easily computable measure of separability of the smooth component of the objective function. In the worst case, when no degree of separability is present, there may be no speedup; in the best case, when the problem is separable, the speedup is equal to the number of processors. Our analysis also works in the mode when the number of blocks being updated at each iteration is random, which allows for modeling situations with busy or unreliable processors. We show that our algorithm is able to solve a LASSO problem involving a matrix with 20 billion nonzeros in 2 h on a large memory node with 24 cores.},
  number = {1-2},
  journaltitle = {Mathematical Programming},
  shortjournal = {Math. Program.},
  urldate = {2016-11-03},
  date = {2015-04-12},
  pages = {433-484},
  author = {Richtárik, Peter and Takáč, Martin},
  file = {/home/arthur/Dropbox/Zotero/Richtárik_Takáč_2015_Parallel coordinate descent methods for big data optimization.pdf;/home/arthur/Zotero/storage/49FGVC5Q/s10107-015-0901-6.html}
}

@article{zou_regularization_2005,
  title = {Regularization and Variable Selection via the Elastic Net},
  volume = {67},
  url = {http://onlinelibrary.wiley.com/doi/10.1111/j.1467-9868.2005.00503.x/full},
  number = {2},
  journaltitle = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
  urldate = {2016-11-04},
  date = {2005},
  pages = {301--320},
  author = {Zou, Hui and Hastie, Trevor},
  file = {/home/arthur/Dropbox/Zotero/Zou_Hastie_2005_Regularization and variable selection via the elastic net.pdf;/home/arthur/Zotero/storage/ZPJUWPK6/full.html}
}

@inproceedings{vane_first_1987,
  title = {First Results from the Airborne Visible/Infrared Imaging Spectrometer ({{AVIRIS}})},
  url = {http://proceedings.spiedigitallibrary.org/proceeding.aspx?articleid=1249557},
  booktitle = {31st {{Annual Technical Symposium}}},
  publisher = {{International Society for Optics and Photonics}},
  urldate = {2016-11-04},
  date = {1987},
  pages = {166--175},
  author = {Vane, Gregg},
  file = {/home/arthur/Dropbox/Zotero/Vane_1987_First results from the airborne visible-infrared imaging spectrometer (AVIRIS).pdf;/home/arthur/Zotero/storage/S2HXF2N8/proceeding.html}
}

@article{behnel_cython_2011,
  title = {Cython: {{The}} Best of Both Worlds},
  volume = {13},
  url = {http://scitation.aip.org/content/aip/journal/cise/13/2/10.1109/MCSE.2010.118},
  shorttitle = {Cython},
  number = {2},
  journaltitle = {Computing in Science \& Engineering},
  urldate = {2016-11-04},
  date = {2011},
  pages = {31--39},
  author = {Behnel, Stefan and Bradshaw, Robert and Citro, Craig and Dalcin, Lisandro and Seljebotn, Dag Sverre and Smith, Kurt},
  file = {/home/arthur/Dropbox/Zotero/Behnel et al_2011_Cython.pdf;/home/arthur/Zotero/storage/CRA9EQBH/MCSE.2010.html}
}

@article{zhang_spatio-temporal_2009,
  title = {Spatio-{{Temporal Compressive Sensing}} and {{Internet Traffic Matrices}}},
  url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.508.7851&rep=rep1&type=pdf},
  urldate = {2016-11-04},
  date = {2009},
  author = {Zhang, Yin and Roughan, Matthew and Willinger, Walter and Qiu, Lili},
  file = {/home/arthur/Dropbox/Zotero/Zhang et al_2009_Spatio-Temporal Compressive Sensing and Internet Traffic Matrices.pdf}
}

@article{rokhlin_randomized_2009,
  title = {A Randomized Algorithm for Principal Component Analysis},
  volume = {31},
  url = {http://epubs.siam.org/doi/abs/10.1137/080736417},
  number = {3},
  journaltitle = {SIAM Journal on Matrix Analysis and Applications},
  urldate = {2016-11-04},
  date = {2009},
  pages = {1100--1124},
  author = {Rokhlin, Vladimir and Szlam, Arthur and Tygert, Mark},
  file = {/home/arthur/Dropbox/Zotero/Rokhlin et al_2009_A randomized algorithm for principal component analysis.pdf;/home/arthur/Zotero/storage/222HABKW/080736417.html}
}

@inproceedings{lu_faster_2013,
  title = {Faster Ridge Regression via the Subsampled Randomized Hadamard Transform},
  url = {http://papers.nips.cc/paper/5106-faster-ridge-regression-via-the-subsampled-randomized-hadamard-transform},
  booktitle = {Advances in Neural Information Processing Systems},
  urldate = {2016-11-04},
  date = {2013},
  pages = {369--377},
  author = {Lu, Yichao and Dhillon, Paramveer and Foster, Dean P. and Ungar, Lyle},
  file = {/home/arthur/Dropbox/Zotero/Lu et al_2013_Faster ridge regression via the subsampled randomized hadamard transform.pdf;/home/arthur/Zotero/storage/Z8VD3AFE/5106-faster-ridge-regression-via-the-subsampled-randomized-hadamard-transform.html}
}

@article{kim_sparse_2007,
  langid = {english},
  title = {Sparse Non-Negative Matrix Factorizations via Alternating Non-Negativity-Constrained Least Squares for Microarray Data Analysis},
  volume = {23},
  issn = {1367-4803, 1460-2059},
  url = {http://bioinformatics.oxfordjournals.org/content/23/12/1495},
  abstract = {Motivation: Many practical pattern recognition problems require non-negativity constraints. For example, pixels in digital images and chemical concentrations in bioinformatics are non-negative. Sparse non-negative matrix factorizations (NMFs) are useful when the degree of sparseness in the non-negative basis matrix or the non-negative coefficient matrix in an NMF needs to be controlled in approximating high-dimensional data in a lower dimensional space.
Results: In this article, we introduce a novel formulation of sparse NMF and show how the new formulation leads to a convergent sparse NMF algorithm via alternating non-negativity-constrained least squares. We apply our sparse NMF algorithm to cancer-class discovery and gene expression data analysis and offer biological analysis of the results obtained. Our experimental results illustrate that the proposed sparse NMF algorithm often achieves better clustering performance with shorter computing time compared to other existing NMF algorithms.
Availability: The software is available as supplementary material.
Contact: hskim@cc.gatech.edu, hpark@acc.gatech.edu
Supplementary information: Supplementary data are available at Bioinformatics online.},
  number = {12},
  journaltitle = {Bioinformatics},
  shortjournal = {Bioinformatics},
  urldate = {2016-11-04},
  date = {2007-06-15},
  pages = {1495-1502},
  author = {Kim, Hyunsoo and Park, Haesun},
  file = {/home/arthur/Dropbox/Zotero/Kim_Park_2007_Sparse non-negative matrix factorizations via alternating.pdf;/home/arthur/Zotero/storage/PE2PC3MJ/1495.html},
  eprinttype = {pmid},
  eprint = {17483501}
}

@article{berger_computational_2013,
  title = {Computational Solutions for Omics Data},
  volume = {14},
  url = {http://www.nature.com/nrg/journal/v14/n5/abs/nrg3433.html},
  number = {5},
  journaltitle = {Nature Reviews Genetics},
  urldate = {2016-11-04},
  date = {2013},
  pages = {333--346},
  author = {Berger, Bonnie and Peng, Jian and Singh, Mona},
  file = {/home/arthur/Zotero/storage/XD59ZGRJ/PMC3966295.html;/home/arthur/Zotero/storage/Z8VPKFEI/nrg3433.html}
}

@article{burer_local_2004,
  langid = {english},
  title = {Local {{Minima}} and {{Convergence}} in {{Low}}-{{Rank Semidefinite Programming}}},
  volume = {103},
  issn = {0025-5610, 1436-4646},
  url = {http://link.springer.com/article/10.1007/s10107-004-0564-1},
  abstract = {.The low-rank semidefinite programming problem LRSDPr is a restriction of the semidefinite programming problem SDP in which a bound r is imposed on the rank of X, and it is well known that LRSDPr is equivalent to SDP if r is not too small. In this paper, we classify the local minima of LRSDPr and prove the optimal convergence of a slight variant of the successful, yet experimental, algorithm of Burer and Monteiro [5], which handles LRSDPr via the nonconvex change of variables X=RRT. In addition, for particular problem classes, we describe a practical technique for obtaining lower bounds on the optimal solution value during the execution of the algorithm. Computational results are presented on a set of combinatorial optimization relaxations, including some of the largest quadratic assignment SDPs solved to date.},
  number = {3},
  journaltitle = {Mathematical Programming},
  shortjournal = {Math. Program.},
  urldate = {2016-11-04},
  date = {2004-12-29},
  pages = {427-444},
  author = {Burer, Samuel and Monteiro, Renato D. C.},
  file = {/home/arthur/Zotero/storage/2XZDV59N/s10107-004-0564-1.html}
}

@article{recht_parallel_2013,
  title = {Parallel Stochastic Gradient Algorithms for Large-Scale Matrix Completion},
  volume = {5},
  url = {http://link.springer.com/article/10.1007/s12532-013-0053-8},
  number = {2},
  journaltitle = {Mathematical Programming Computation},
  urldate = {2016-11-04},
  date = {2013},
  pages = {201--226},
  author = {Recht, Benjamin and Ré, Christopher},
  file = {/home/arthur/Dropbox/Zotero/Recht_Ré_2013_Parallel stochastic gradient algorithms for large-scale matrix completion.pdf;/home/arthur/Zotero/storage/U6F9WEC5/s12532-013-0053-8.html}
}

@article{razaviyayn_unified_2013,
  title = {A Unified Convergence Analysis of Block Successive Minimization Methods for Nonsmooth Optimization},
  volume = {23},
  url = {http://epubs.siam.org/doi/abs/10.1137/120891009},
  number = {2},
  journaltitle = {SIAM Journal on Optimization},
  urldate = {2016-11-04},
  date = {2013},
  pages = {1126--1153},
  author = {Razaviyayn, Meisam and Hong, Mingyi and Luo, Zhi-Quan},
  file = {/home/arthur/Zotero/storage/MACF4JIS/120891009.html}
}

@article{qiu_undersampled_2016,
  title = {Undersampled {{Phase Retrieval}} via {{Majorization}}-{{Minimization}}},
  url = {https://arxiv.org/abs/1609.02842},
  journaltitle = {arXiv preprint arXiv:1609.02842},
  urldate = {2016-11-08},
  date = {2016},
  author = {Qiu, Tianyu and Palomar, Daniel P.},
  file = {/home/arthur/Dropbox/Zotero/Qiu_Palomar_2016_Undersampled Phase Retrieval via Majorization-Minimization.pdf}
}

@article{harrison_large-scale_2015,
  langid = {english},
  title = {Large-Scale {{Probabilistic Functional Modes}} from Resting State {{fMRI}}},
  volume = {109},
  issn = {10538119},
  url = {http://linkinghub.elsevier.com/retrieve/pii/S1053811915000208},
  journaltitle = {NeuroImage},
  urldate = {2016-11-08},
  date = {2015-04},
  pages = {217-231},
  author = {Harrison, Samuel J. and Woolrich, Mark W. and Robinson, Emma C. and Glasser, Matthew F. and Beckmann, Christian F. and Jenkinson, Mark and Smith, Stephen M.},
  file = {/home/arthur/Dropbox/Zotero/Harrison et al_2015_Large-scale Probabilistic Functional Modes from resting state fMRI.pdf}
}

@article{wang_randomized_2014,
  title = {Randomized Block Coordinate Descent for Online and Stochastic Optimization},
  url = {http://arxiv.org/abs/1407.0107},
  journaltitle = {arXiv preprint arXiv:1407.0107},
  urldate = {2016-11-08},
  date = {2014},
  author = {Wang, Huahua and Banerjee, Arindam},
  file = {/home/arthur/Dropbox/Zotero/Wang_Banerjee_2014_Randomized block coordinate descent for online and stochastic optimization.pdf}
}

@unpublished{gordon_coordinate_2010,
  title = {Coordinate Descent},
  url = {https://www.cs.cmu.edu/~ggordon/10725-F12/slides/25-coord-desc.pdf},
  urldate = {2016-11-07},
  date = {2010},
  author = {Gordon, Geoff and Tibshirani, Ryan},
  file = {/home/arthur/Dropbox/Zotero/Gordon_Tibshirani_2010_Coordinate descent.pdf}
}

@article{wang_subsampled_2015,
  langid = {english},
  title = {Subsampled {{Hessian Newton Methods}} for {{Supervised Learning}}},
  volume = {27},
  issn = {0899-7667, 1530-888X},
  url = {http://www.mitpressjournals.org/doi/10.1162/NECO_a_00751},
  number = {8},
  journaltitle = {Neural Computation},
  urldate = {2016-11-08},
  date = {2015-08},
  pages = {1766-1795},
  author = {Wang, Chien-Chih and Huang, Chun-Heng and Lin, Chih-Jen}
}

@misc{pitman_setup_2010,
  title = {Setup for the Central Limit Theorem},
  url = {http://www.stat.berkeley.edu/~pitman/s205f02/lecture10.pdf},
  urldate = {2016-07-18},
  date = {2010},
  author = {Pitman, Jim},
  file = {/home/arthur/Dropbox/Zotero/Pitman_2010_Setup for the central limit theorem.pdf}
}

@misc{lalley_martingale_2014,
  title = {The Martingale Central Limit Theorem},
  url = {http://galton.uchicago.edu/~lalley/Courses/383/Lindeberg.pdf},
  urldate = {2016-07-12},
  date = {2014},
  author = {Lalley, Steven},
  file = {/home/arthur/Dropbox/Zotero/Lalley_2014_The martingale central limit theorem.pdf}
}

@inproceedings{lee_efficient_2006,
  title = {Efficient Sparse Coding Algorithms},
  url = {http://machinelearning.wustl.edu/mlpapers/paper_files/NIPS2006_878.pdf},
  booktitle = {Advances in Neural Information Processing Systems},
  urldate = {2016-11-08},
  date = {2006},
  pages = {801--808},
  author = {Lee, Honglak and Battle, Alexis and Raina, Rajat and Ng, Andrew Y.},
  file = {/home/arthur/Dropbox/Zotero/Lee et al_2006_Efficient sparse coding algorithms.pdf}
}

@inproceedings{dohmatob_learning_2016,
  langid = {english},
  title = {Learning Brain Regions via Large-Scale Online Structured Sparse Dictionary-Learning},
  url = {https://hal.inria.fr/hal-01369134/document},
  abstract = {We propose a multivariate online dictionary-learning method for obtaining de-compositions of brain images with structured and sparse components (aka atoms). Sparsity is to be understood in the usual sense: the dictionary atoms are constrained to contain mostly zeros. This is imposed via an 1-norm constraint. By "struc-tured", we mean that the atoms are piece-wise smooth and compact, thus making up blobs, as opposed to scattered patterns of activation. We propose to use a Sobolev (Laplacian) penalty to impose this type of structure. Combining the two penalties, we obtain decompositions that properly delineate brain structures from functional images. This non-trivially extends the online dictionary-learning work of Mairal et al. (2010), at the price of only a factor of 2 or 3 on the overall running time. Just like the Mairal et al. (2010) reference method, the online nature of our proposed algorithm allows it to scale to arbitrarily sized datasets. Experiments on brain data show that our proposed method extracts structured and denoised dictionaries that are more intepretable and better capture inter-subject variability in small medium, and large-scale regimes alike, compared to state-of-the-art models.},
  booktitle = {30th {{Conference}} on {{Neural Information Processing Systems}} ({{NIPS}})},
  urldate = {2016-11-10},
  date = {2016-12-05},
  author = {Dohmatob, Elvis and Mensch, Arthur and Varoquaux, Gaël and Thirion, Bertrand},
  file = {/home/arthur/Dropbox/Zotero/Dohmatob et al_2016_Learning brain regions via large-scale online structured sparse.pdf;/home/arthur/Zotero/storage/8459DCJ4/hal-01369134v3.html}
}

@article{izenman_reduced-rank_1975,
  title = {Reduced-Rank Regression for the Multivariate Linear Model},
  volume = {5},
  issn = {0047-259X},
  url = {http://www.sciencedirect.com/science/article/pii/0047259X75900421},
  abstract = {The problem of estimating the regression coefficient matrix having known (reduced) rank for the multivariate linear model when both sets of variates are jointly stochastic is discussed. We show that this problem is related to the problem of deciding how many principal components or pairs of canonical variates to use in any practical situation. Under the assumption of joint normality of the two sets of variates, we give the asymptotic (large-sample) distributions of the various estimated reduced-rank regression coefficient matrices that are of interest. Approximate confidence bounds on the elements of these matrices are then suggested using either the appropriate asymptotic expressions or the jackknife technique.},
  number = {2},
  journaltitle = {Journal of Multivariate Analysis},
  shortjournal = {Journal of Multivariate Analysis},
  urldate = {2016-11-10},
  date = {1975-06-01},
  pages = {248-264},
  keywords = {Multivariate linear regression,principal components,canonical variates,asymptotic distribution theory,confidence bounds,jackknife},
  author = {Izenman, Alan Julian},
  file = {/home/arthur/Zotero/storage/85SZHU5Z/0047259X75900421.html}
}

@article{argyriou_convex_2008,
  langid = {english},
  title = {Convex Multi-Task Feature Learning},
  volume = {73},
  issn = {0885-6125, 1573-0565},
  url = {http://link.springer.com/article/10.1007/s10994-007-5040-8},
  abstract = {We present a method for learning sparse representations shared across multiple tasks. This method is a generalization of the well-known single-task 1-norm regularization. It is based on a novel non-convex regularizer which controls the number of learned features common across the tasks. We prove that the method is equivalent to solving a convex optimization problem for which there is an iterative algorithm which converges to an optimal solution. The algorithm has a simple interpretation: it alternately performs a supervised and an unsupervised step, where in the former step it learns task-specific functions and in the latter step it learns common-across-tasks sparse representations for these functions. We also provide an extension of the algorithm which learns sparse nonlinear representations using kernels. We report experiments on simulated and real data sets which demonstrate that the proposed method can both improve the performance relative to learning each task independently and lead to a few learned features common across related tasks. Our algorithm can also be used, as a special case, to simply select—not learn—a few common variables across the tasks.},
  number = {3},
  journaltitle = {Machine Learning},
  shortjournal = {Mach Learn},
  urldate = {2016-11-10},
  date = {2008-12-01},
  pages = {243-272},
  author = {Argyriou, Andreas and Evgeniou, Theodoros and Pontil, Massimiliano},
  file = {/home/arthur/Dropbox/Zotero/Argyriou et al_2008_Convex multi-task feature learning.pdf;/home/arthur/Zotero/storage/UWQDR64X/s10994-007-5040-8.html}
}

@inproceedings{bzdok_semi-supervised_2015,
  title = {Semi-Supervised Factored Logistic Regression for High-Dimensional Neuroimaging Data},
  url = {http://papers.nips.cc/paper/5646-scale-adaptive-blind-deblurring},
  booktitle = {Advances in Neural Information Processing Systems},
  urldate = {2016-11-10},
  date = {2015},
  pages = {3348--3356},
  author = {Bzdok, Danilo and Eickenberg, Michael and Grisel, Olivier and Thirion, Bertrand and Varoquaux, Gaël},
  file = {/home/arthur/Dropbox/Zotero/Bzdok et al_2015_Semi-supervised factored logistic regression for high-dimensional neuroimaging.pdf;/home/arthur/Zotero/storage/I85EVP9R/5646-scale-adaptive-blind-deblurring.html}
}

@article{chollet_xception_2016,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1610.02357},
  primaryClass = {cs},
  title = {Xception: {{Deep Learning}} with {{Depthwise Separable Convolutions}}},
  url = {http://arxiv.org/abs/1610.02357},
  shorttitle = {Xception},
  abstract = {We present an interpretation of Inception modules in convolutional neural networks as being an intermediate step in-between regular convolution and the $\backslash$textit\{depthwise separable convolution\} operation (a depthwise convolution followed by a pointwise convolution). In this light, a depthwise separable convolution can be understood as an Inception module with a maximally large number of towers. This observation leads us to propose a novel deep convolutional neural network architecture inspired by Inception, where Inception modules have been replaced with depthwise separable convolutions. We show that this architecture, dubbed Xception, slightly outperforms Inception V3 on the ImageNet dataset (which Inception V3 was designed for), and significantly outperforms Inception V3 on a larger image classification dataset comprising 350 million images and 17,000 classes. Since the Xception architecture has the same number of parameter as Inception V3, the performance gains are not due to increased capacity but rather to a more efficient use of model parameters.},
  urldate = {2016-11-10},
  date = {2016-10-07},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  author = {Chollet, Francois},
  file = {/home/arthur/Dropbox/Zotero/Chollet_2016_Xception.pdf;/home/arthur/Zotero/storage/R5UFFXXT/1610.html}
}

@inproceedings{mensch_subsampled_2016,
  title = {Subsampled Online Matrix Factorization with Convergence Guarantees},
  booktitle = {{{NIPS Workshop}} on {{Optimization}} for {{Machine Learning}} ({{OPT}})},
  date = {2016-12},
  author = {Mensch, Arthur and Mairal, Julien and Thirion, Bertrand and Varoquaux, Gaël}
}

@article{trigeorgis_deep_2015,
  title = {A Deep Matrix Factorization Method for Learning Attribute Representations},
  url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=7453156},
  urldate = {2016-11-11},
  date = {2015},
  author = {Trigeorgis, George and Bousmalis, Konstantinos and Zafeiriou, Stefanos and Schuller, Bjorn W.},
  file = {/home/arthur/Dropbox/Zotero/Trigeorgis et al_2015_A deep matrix factorization method for learning attribute representations.pdf;/home/arthur/Zotero/storage/CFJAEHN3/7453156.html}
}

@article{haeffele_global_2015,
  title = {Global Optimality in Tensor Factorization, Deep Learning, and Beyond},
  url = {http://arxiv.org/abs/1506.07540},
  journaltitle = {arXiv preprint arXiv:1506.07540},
  urldate = {2016-11-11},
  date = {2015},
  author = {Haeffele, Benjamin D. and Vidal, René},
  file = {/home/arthur/Dropbox/Zotero/Haeffele_Vidal_2015_Global optimality in tensor factorization, deep learning, and beyond.pdf}
}

@incollection{levy_neural_2014,
  title = {Neural {{Word Embedding}} as {{Implicit Matrix Factorization}}},
  url = {http://papers.nips.cc/paper/5477-neural-word-embedding-as-implicit-matrix-factorization.pdf},
  booktitle = {Advances in {{Neural Information Processing Systems}} 27},
  publisher = {{Curran Associates, Inc.}},
  urldate = {2016-11-11},
  date = {2014},
  pages = {2177--2185},
  author = {Levy, Omer and Goldberg, Yoav},
  editor = {Ghahramani, Z. and Welling, M. and Cortes, C. and Lawrence, N. D. and Weinberger, K. Q.},
  file = {/home/arthur/Dropbox/Zotero/Levy_Goldberg_2014_Neural Word Embedding as Implicit Matrix Factorization.pdf;/home/arthur/Zotero/storage/CTUXIEUW/5477-neural-word-embedding-as-implicit-matrix.html}
}

@inproceedings{li_word_2015,
  title = {Word Embedding Revisited: {{A}} New Representation Learning and Explicit Matrix Factorization Perspective},
  url = {http://ijcai.org/papers15/Papers/IJCAI15-513.pdf},
  shorttitle = {Word Embedding Revisited},
  booktitle = {Proceedings of the {{Twenty}}-{{Fourth International Joint Conference}} on {{Artificial Intelligence}}, {{IJCAI}}},
  urldate = {2016-11-11},
  date = {2015},
  pages = {25--31},
  author = {Li, Yitan and Xu, Linli and Tian, Fei and Jiang, Liang and Zhong, Xiaowei and Chen, Enhong},
  file = {/home/arthur/Dropbox/Zotero/Li et al_2015_Word embedding revisited.pdf}
}

@article{lee_learning_1999,
  title = {Learning the Parts of Objects by Non-Negative Matrix Factorization},
  volume = {401},
  url = {http://www.nature.com/nature/journal/v401/n6755/abs/401788a0.html},
  number = {6755},
  journaltitle = {Nature},
  urldate = {2016-11-11},
  date = {1999},
  pages = {788--791},
  author = {Lee, Daniel D. and Seung, H. Sebastian},
  file = {/home/arthur/Zotero/storage/B5J64DME/401788a0.html;/home/arthur/Zotero/storage/M3RTITWP/401788a0.html}
}

@article{mizutani_ellipsoidal_2014,
  title = {Ellipsoidal Rounding for Nonnegative Matrix Factorization under Noisy Separability.},
  volume = {15},
  url = {http://www.jmlr.org/papers/volume15/mizutani14a/mizutani14a.pdf},
  number = {1},
  journaltitle = {Journal of Machine Learning Research},
  urldate = {2016-11-11},
  date = {2014},
  pages = {1011--1039},
  author = {Mizutani, Tomohiko},
  file = {/home/arthur/Dropbox/Zotero/Mizutani_2014_Ellipsoidal rounding for nonnegative matrix factorization under noisy.pdf}
}

@book{aggarwal_mining_2012,
  title = {Mining Text Data},
  url = {https://books.google.fr/books?hl=en&lr=&id=vFHOx8wfSU0C&oi=fnd&pg=PR3&dq=non+negative+topic+modeling&ots=ob8h-GiGTp&sig=HuiNncCYy_W7BzjoYTclr7QQuIw},
  publisher = {{Springer Science \& Business Media}},
  urldate = {2016-11-11},
  date = {2012},
  author = {Aggarwal, Charu C. and Zhai, ChengXiang},
  file = {/home/arthur/Zotero/storage/VD64PPW8/books.html}
}

@article{pauca_nonnegative_2006,
  title = {Nonnegative Matrix Factorization for Spectral Data Analysis},
  volume = {416},
  url = {http://www.sciencedirect.com/science/article/pii/S002437950500340X},
  number = {1},
  journaltitle = {Linear algebra and its applications},
  urldate = {2016-11-11},
  date = {2006},
  pages = {29--47},
  author = {Pauca, V. Paul and Piper, Jon and Plemmons, Robert J.},
  file = {/home/arthur/Zotero/storage/WU2T8RUN/S002437950500340X.html}
}

@article{beale_chemical_2010,
  title = {Chemical Imaging of Catalytic Solids with Synchrotron Radiation},
  volume = {39},
  url = {http://pubs.rsc.org/en/content/articlehtml/2010/cs/c0cs00089b},
  number = {12},
  journaltitle = {Chemical Society Reviews},
  urldate = {2016-11-11},
  date = {2010},
  pages = {4656--4672},
  author = {Beale, Andrew M. and Jacques, Simon DM and Weckhuysen, Bert M.},
  file = {/home/arthur/Dropbox/Zotero/Beale et al_2010_Chemical imaging of catalytic solids with synchrotron radiation.pdf;/home/arthur/Zotero/storage/NSM3QA6I/c0cs00089b.html}
}

@article{rapin_sparse_2013,
  title = {Sparse and Non-Negative {{BSS}} for Noisy Data},
  volume = {61},
  url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=6584797},
  number = {22},
  journaltitle = {IEEE Transactions on Signal Processing},
  urldate = {2016-11-11},
  date = {2013},
  pages = {5620--5632},
  author = {Rapin, Jérémy and Bobin, Jérôme and Larue, Anthony and Starck, Jean-Luc},
  file = {/home/arthur/Dropbox/Zotero/Rapin et al_2013_Sparse and non-negative BSS for noisy data.pdf}
}

@article{osan_microscale_2014,
  title = {Microscale Analysis of Metal Uptake by Argillaceous Rocks Using Positive Matrix Factorization of Microscopic {{X}}-Ray Fluorescence Elemental Maps},
  volume = {91},
  url = {http://www.sciencedirect.com/science/article/pii/S0584854713002711},
  journaltitle = {Spectrochimica Acta Part B: Atomic Spectroscopy},
  urldate = {2016-11-11},
  date = {2014},
  pages = {12--23},
  author = {Osán, János and Kéri, Annamária and Breitner, Dániel and Fábián, Margit and Dähn, Rainer and Simon, Rolf and Török, Szabina},
  file = {/home/arthur/Dropbox/Zotero/Osán et al_2014_Microscale analysis of metal uptake by argillaceous rocks using positive matrix.pdf;/home/arthur/Zotero/storage/N68B6BJ8/S0584854713002711.html}
}

@book{metivier_semimartingales_1982,
  title = {Semimartingales: A Course on Stochastic Processes},
  volume = {2},
  url = {https://books.google.fr/books?hl=en&lr=&id=i_llJNqMhaoC&oi=fnd&pg=PA1&dq=M.+M%C3%A9tivier.+Semi-martingales.+&ots=S8L0udk7hp&sig=zTOOfV0nxYQt8RHrnJb-hOEHM4o},
  shorttitle = {Semimartingales},
  publisher = {{Walter de Gruyter}},
  urldate = {2016-11-14},
  date = {1982},
  author = {Métivier, Michel},
  file = {/home/arthur/Zotero/storage/KN5JKARW/books.html}
}

@article{milham_adhd-200_2012,
  langid = {english},
  title = {The Adhd-200 Consortium: A Model to Advance the Translational Potential of Neuroimaging in Clinical Neuroscience},
  volume = {6},
  issn = {1662-5137},
  url = {http://journal.frontiersin.org/article/10.3389/fnsys.2012.00062/abstract},
  shorttitle = {The Adhd-200 Consortium},
  abstract = {The adhd-200 consortium: a model to advance the translational potential of neuroimaging in clinical neuroscience},
  journaltitle = {Frontiers in Systems Neuroscience},
  shortjournal = {Front. Syst. Neurosci.},
  urldate = {2016-11-15},
  date = {2012},
  keywords = {Resting state fMRI,ADHD,Consortium,Open Science,data sharing},
  author = {Milham, Michael P. Ph D. and Fair, Damien PA-C. and Mennes, Maarten Ph D. and Mostofsky, Stewart H. M. D.},
  file = {/home/arthur/Dropbox/Zotero/Milham et al_2012_The adhd-200 consortium.pdf}
}

@article{boyd_distributed_2010,
  langid = {english},
  title = {Distributed {{Optimization}} and {{Statistical Learning}} via the {{Alternating Direction Method}} of {{Multipliers}}},
  volume = {3},
  issn = {1935-8237, 1935-8245},
  url = {http://www.nowpublishers.com/article/Details/MAL-016},
  number = {1},
  journaltitle = {Foundations and Trends® in Machine Learning},
  urldate = {2016-11-15},
  date = {2010},
  pages = {1-122},
  author = {Boyd, Stephen},
  file = {/home/arthur/Dropbox/Zotero/Boyd_2010_Distributed Optimization and Statistical Learning via the Alternating Direction.pdf}
}

@article{condat_primaldual_2013,
  title = {A Primal–Dual Splitting Method for Convex Optimization Involving {{Lipschitzian}}, Proximable and Linear Composite Terms},
  volume = {158},
  url = {http://link.springer.com/article/10.1007/s10957-012-0245-9},
  number = {2},
  journaltitle = {Journal of Optimization Theory and Applications},
  urldate = {2016-11-15},
  date = {2013},
  pages = {460--479},
  author = {Condat, Laurent},
  file = {/home/arthur/Dropbox/Zotero/Condat_2013_A primal–dual splitting method for convex optimization involving Lipschitzian,.pdf}
}

@inproceedings{su_differential_2014,
  title = {A Differential Equation for Modeling {{Nesterov}}’s Accelerated Gradient Method: Theory and Insights},
  url = {http://papers.nips.cc/paper/5322-delay-tolerant-algorithms-for-asynchronous-distributed-online-learning},
  shorttitle = {A Differential Equation for Modeling {{Nesterov}}’s Accelerated Gradient Method},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  urldate = {2016-11-15},
  date = {2014},
  pages = {2510--2518},
  author = {Su, Weijie and Boyd, Stephen and Candes, Emmanuel},
  file = {/home/arthur/Dropbox/Zotero/Su et al_2014_A differential equation for modeling Nesterov’s accelerated gradient method.pdf;/home/arthur/Zotero/storage/NHZRK92K/5322-delay-tolerant-algorithms-for-asynchronous-distributed-online-learning.html}
}

@article{wibisono_variational_2016,
  title = {A {{Variational Perspective}} on {{Accelerated Methods}} in {{Optimization}}},
  url = {http://arxiv.org/abs/1603.04245},
  journaltitle = {arXiv preprint arXiv:1603.04245},
  urldate = {2016-11-15},
  date = {2016},
  author = {Wibisono, Andre and Wilson, Ashia C. and Jordan, Michael I.},
  file = {/home/arthur/Dropbox/Zotero/Wibisono et al_2016_A Variational Perspective on Accelerated Methods in Optimization.pdf;/home/arthur/Zotero/storage/Z9Z7HRG5/1603.html}
}

@book{borwein_convex_2010,
  title = {Convex Analysis and Nonlinear Optimization: Theory and Examples},
  url = {https://books.google.fr/books?hl=en&lr=&id=eeL_Cdmowv8C&oi=fnd&pg=PP4&dq=related:olMC9rsZUrpIBM:scholar.google.com/&ots=MDBL2bCMTE&sig=AhDqQpgFqNJqAjhQ8odo5uPZBvA},
  shorttitle = {Convex Analysis and Nonlinear Optimization},
  publisher = {{Springer Science \& Business Media}},
  urldate = {2016-11-17},
  date = {2010},
  author = {Borwein, Jonathan M. and Lewis, Adrian S.},
  file = {/home/arthur/Dropbox/Zotero/Borwein_Lewis_2010_Convex analysis and nonlinear optimization.pdf;/home/arthur/Zotero/storage/5MW7VGVN/books.html}
}

@inproceedings{peng_decomposable_2014,
  title = {Decomposable Nonlocal Tensor Dictionary Learning for Multispectral Image Denoising},
  url = {http://www.cv-foundation.org/openaccess/content_cvpr_2014/html/Peng_Decomposable_Nonlocal_Tensor_2014_CVPR_paper.html},
  booktitle = {Proceedings of the {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  urldate = {2016-11-21},
  date = {2014},
  pages = {2949--2956},
  author = {Peng, Yi and Meng, Deyu and Xu, Zongben and Gao, Chenqiang and Yang, Yi and Zhang, Biao},
  file = {/home/arthur/Dropbox/Zotero/Peng et al_2014_Decomposable nonlocal tensor dictionary learning for multispectral image.pdf}
}

@article{friedman_pathwise_2007,
  title = {Pathwise Coordinate Optimization},
  volume = {1},
  url = {http://projecteuclid.org/euclid.aoas/1196438020},
  number = {2},
  journaltitle = {The Annals of Applied Statistics},
  urldate = {2016-11-22},
  date = {2007},
  pages = {302--332},
  author = {Friedman, Jerome and Hastie, Trevor and Höfling, Holger and Tibshirani, Robert and {others}},
  file = {/home/arthur/Zotero/storage/WCHAGE7N/1196438020.html}
}

@inproceedings{hoyer_non-negative_2002,
  title = {Non-Negative Sparse Coding},
  url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1030067},
  booktitle = {Neural {{Networks}} for {{Signal Processing}}, 2002. {{Proceedings}} of the 2002 12th {{IEEE Workshop}} On},
  publisher = {{IEEE}},
  urldate = {2016-11-22},
  date = {2002},
  pages = {557--565},
  author = {Hoyer, Patrik O.},
  file = {/home/arthur/Dropbox/Zotero/Hoyer_2002_Non-negative sparse coding.pdf;/home/arthur/Zotero/storage/VV85HJ6K/1030067.html}
}

@article{maggioni_nonlocal_2013,
  title = {Nonlocal Transform-Domain Filter for Volumetric Data Denoising and Reconstruction},
  volume = {22},
  url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=6253256},
  number = {1},
  journaltitle = {IEEE transactions on image processing},
  urldate = {2016-11-23},
  date = {2013},
  pages = {119--133},
  author = {Maggioni, Matteo and Katkovnik, Vladimir and Egiazarian, Karen and Foi, Alessandro},
  file = {/home/arthur/Dropbox/Zotero/Maggioni et al_2013_Nonlocal transform-domain filter for volumetric data denoising and.pdf;/home/arthur/Zotero/storage/4CFQNENX/6253256.html}
}

@article{ge_signal_2015,
  langid = {english},
  title = {Signal Sampling for Efficient Sparse Representation of Resting State {{FMRI}} Data},
  issn = {1931-7557, 1931-7565},
  url = {http://link.springer.com/10.1007/s11682-015-9487-0},
  journaltitle = {Brain Imaging and Behavior},
  urldate = {2016-11-28},
  date = {2015-12-08},
  author = {Ge, Bao and Makkie, Milad and Wang, Jin and Zhao, Shijie and Jiang, Xi and Li, Xiang and Lv, Jinglei and Zhang, Shu and Zhang, Wei and Han, Junwei and Guo, Lei and Liu, Tianming},
  file = {/home/arthur/Dropbox/Zotero/Ge et al_2015_Signal sampling for efficient sparse representation of resting state FMRI data.pdf}
}

@incollection{sheikh_select-and-sample_2016,
  title = {Select-and-{{Sample}} for {{Spike}}-and-{{Slab Sparse Coding}}},
  url = {http://papers.nips.cc/paper/6276-select-and-sample-for-spike-and-slab-sparse-coding.pdf},
  booktitle = {Advances in {{Neural Information Processing Systems}} 29},
  publisher = {{Curran Associates, Inc.}},
  urldate = {2016-11-30},
  date = {2016},
  pages = {3927--3935},
  author = {Sheikh, Abdul-Saboor and Lücke, Jörg},
  editor = {Lee, D. D. and Sugiyama, M. and Luxburg, U. V. and Guyon, I. and Garnett, R.},
  file = {/home/arthur/Dropbox/Zotero/Sheikh_Lücke_2016_Select-and-Sample for Spike-and-Slab Sparse Coding.pdf;/home/arthur/Zotero/storage/BSRJB8EV/6276-select-and-sample-for-spike-and-slab-sparse-coding.html}
}

@article{yee_reduced-rank_2003,
  langid = {english},
  title = {Reduced-Rank Vector Generalized Linear Models},
  volume = {3},
  issn = {1471-082X, 1477-0342},
  url = {http://smj.sagepub.com/content/3/1/15},
  abstract = {Reduced-rank regression is a method with great potential for dimension reduction but has found few applications in applied statistics. To address this, reduced-rank regression is proposed for the class of vector generalized linear models (VGLMs), which is very large. The resulting class, which we call reduced-rank VGLMs (RR-VGLMs), enables the benefits of reduced-rank regression to be conveyed to a wide range of data types, including categorical data. RR-VGLMs are illustrated by focussing on models for categorical data, and especially the multinomial logit model. General algorithmic details are provided and software written by the first author is described. The reduced-rank multinomial logit model is illustrated with real data in two contexts: a regression analysis of workforce data and a classification problem.},
  number = {1},
  journaltitle = {Statistical Modelling},
  shortjournal = {Statistical Modelling},
  urldate = {2016-12-12},
  date = {2003-01-04},
  pages = {15-41},
  keywords = {categorical data analysis,iteratively reweighted least squares,linear predictors,multinomial logit model,reduced rank regression,stereotype model,vector generalized linear models},
  author = {Yee, Thomas W. and Hastie, Trevor J.},
  file = {/home/arthur/Dropbox/Zotero/Yee_Hastie_2003_Reduced-rank vector generalized linear models.pdf;/home/arthur/Zotero/storage/9PU63RM7/15.html}
}

@inreference{yang_majorization-minimization_2015,
  title = {Majorization-{{Minimization}} for {{Manifold Embedding}}.},
  url = {http://www.jmlr.org/proceedings/papers/v38/yang15a.pdf},
  booktitle = {{{AISTATS}}},
  urldate = {2016-12-12},
  date = {2015},
  author = {Yang, Zhirong and Peltonen, Jaakko and Kaski, Samuel},
  file = {/home/arthur/Dropbox/Zotero/Yang et al_2015_Majorization-Minimization for Manifold Embedding.pdf}
}

@article{li_weakly_2017,
  title = {Weakly {{Supervised Deep Matrix Factorization}} for {{Social Image Understanding}}},
  volume = {26},
  issn = {1057-7149},
  doi = {10.1109/TIP.2016.2624140},
  abstract = {The number of images associated with weakly supervised user-provided tags has increased dramatically in recent years. User-provided tags are incomplete, subjective and noisy. In this paper, we focus on the problem of social image understanding, i.e., tag refinement, tag assignment, and image retrieval. Different from previous work, we propose a novel weakly supervised deep matrix factorization algorithm, which uncovers the latent image representations and tag representations embedded in the latent subspace by collaboratively exploring the weakly supervised tagging information, the visual structure, and the semantic structure. Due to the well-known semantic gap, the hidden representations of images are learned by a hierarchical model, which are progressively transformed from the visual feature space. It can naturally embed new images into the subspace using the learned deep architecture. The semantic and visual structures are jointly incorporated to learn a semantic subspace without overfitting the noisy, incomplete, or subjective tags. Besides, to remove the noisy or redundant visual features, a sparse model is imposed on the transformation matrix of the first layer in the deep architecture. Finally, a unified optimization problem with a well-defined objective function is developed to formulate the proposed problem and solved by a gradient descent procedure with curvilinear search. Extensive experiments on real-world social image databases are conducted on the tasks of image understanding: image tag refinement, assignment, and retrieval. Encouraging results are achieved with comparison with the state-of-the-art algorithms, which demonstrates the effectiveness of the proposed method.},
  number = {1},
  journaltitle = {IEEE Transactions on Image Processing},
  date = {2017-01},
  pages = {276-288},
  keywords = {Sparse matrices,matrix factorization,Data models,Image retrieval,Noise measurement,Semantics,Tagging,Visualization,Deep architecture,image tagging,weakly supervised},
  author = {Li, Z. and Tang, J.},
  file = {/home/arthur/Dropbox/Zotero/Li_Tang_2017_Weakly Supervised Deep Matrix Factorization for Social Image Understanding.pdf;/home/arthur/Zotero/storage/8M6JWHGV/7728069.html}
}

@article{bzdok_formal_2016,
  title = {Formal {{Models}} of the {{Network Co}}-Occurrence {{Underlying Mental Operations}}},
  volume = {12},
  issn = {1553-7358},
  url = {http://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1004994},
  abstract = {Author Summary Assuming the central importance of canonical brain networks for realizing human cognitive processes, the present work demonstrates the quantifiability of relative neural networks involvements during psychological tasks. This is achieved by a machine-learning approach that combines exploratory network discovery and inferential task prediction. We show that activity levels of network sets can be automatically derived from task batteries of two large reference datasets. The evidence supports the often-held suspicion that task-specific neural activity might be due in large part to distinct recombinations of the same underlying brain network units. The results further discourage the frequently embraced dichotomy between exteroceptive task-associated versus interoceptive task-unspecific brain systems. Standard fMRI brain scans can thus be used to reconstruct and quantitatively compare the entire set of major network engagements to test targeted hypotheses. In the future, such network co-occurrence signatures could perhaps be useful as biomarkers in psychiatric and neurological research.},
  number = {6},
  journaltitle = {PLOS Computational Biology},
  shortjournal = {PLOS Computational Biology},
  urldate = {2016-12-12},
  date = {2016-06-16},
  pages = {e1004994},
  keywords = {Principal Component Analysis,functional magnetic resonance imaging,support vector machines,neuroimaging,Neural networks,Network analysis,Learning,Cognition},
  author = {Bzdok, Danilo and Varoquaux, Gaël and Grisel, Olivier and Eickenberg, Michael and Poupon, Cyril and Thirion, Bertrand},
  file = {/home/arthur/Dropbox/Zotero/Bzdok et al_2016_Formal Models of the Network Co-occurrence Underlying Mental Operations.pdf;/home/arthur/Zotero/storage/3WI44TIW/article.html}
}

@article{doersch_tutorial_2016,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1606.05908},
  primaryClass = {cs, stat},
  title = {Tutorial on {{Variational Autoencoders}}},
  url = {http://arxiv.org/abs/1606.05908},
  abstract = {In just three years, Variational Autoencoders (VAEs) have emerged as one of the most popular approaches to unsupervised learning of complicated distributions. VAEs are appealing because they are built on top of standard function approximators (neural networks), and can be trained with stochastic gradient descent. VAEs have already shown promise in generating many kinds of complicated data, including handwritten digits, faces, house numbers, CIFAR images, physical models of scenes, segmentation, and predicting the future from static images. This tutorial introduces the intuitions behind VAEs, explains the mathematics behind them, and describes some empirical behavior. No prior knowledge of variational Bayesian methods is assumed.},
  urldate = {2016-12-12},
  date = {2016-06-19},
  keywords = {Computer Science - Learning,Statistics - Machine Learning},
  author = {Doersch, Carl},
  file = {/home/arthur/Dropbox/Zotero/Doersch_2016_Tutorial on Variational Autoencoders.pdf;/home/arthur/Zotero/storage/FXJ6I8RA/1606.html}
}

@article{goodfellow_generative_2014,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1406.2661},
  primaryClass = {cs, stat},
  title = {Generative {{Adversarial Networks}}},
  url = {http://arxiv.org/abs/1406.2661},
  abstract = {We propose a new framework for estimating generative models via an adversarial process, in which we simultaneously train two models: a generative model G that captures the data distribution, and a discriminative model D that estimates the probability that a sample came from the training data rather than G. The training procedure for G is to maximize the probability of D making a mistake. This framework corresponds to a minimax two-player game. In the space of arbitrary functions G and D, a unique solution exists, with G recovering the training data distribution and D equal to 1/2 everywhere. In the case where G and D are defined by multilayer perceptrons, the entire system can be trained with backpropagation. There is no need for any Markov chains or unrolled approximate inference networks during either training or generation of samples. Experiments demonstrate the potential of the framework through qualitative and quantitative evaluation of the generated samples.},
  urldate = {2016-12-12},
  date = {2014-06-10},
  keywords = {Computer Science - Learning,Statistics - Machine Learning},
  author = {Goodfellow, Ian J. and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
  file = {/home/arthur/Dropbox/Zotero/Goodfellow et al_2014_Generative Adversarial Networks.pdf;/home/arthur/Zotero/storage/XKB24G8D/1406.html}
}

@article{wilson_lyapunov_2016,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1611.02635},
  primaryClass = {cs, math},
  title = {A {{Lyapunov Analysis}} of {{Momentum Methods}} in {{Optimization}}},
  url = {http://arxiv.org/abs/1611.02635},
  abstract = {Momentum methods play a central role in optimization. Several momentum methods are provably optimal, and all use a technique called estimate sequences to analyze their convergence properties. The technique of estimate sequences has long been considered difficult to understand, leading many researchers to generate alternative, "more intuitive" methods and analyses. In this paper we show there is an equivalence between the technique of estimate sequences and a family of Lyapunov functions in both continuous and discrete time. This framework allows us to develop a simple and unified analysis of many existing momentum algorithms, introduce several new algorithms, and most importantly, strengthen the connection between algorithms and continuous-time dynamical systems.},
  urldate = {2016-12-13},
  date = {2016-11-08},
  keywords = {Mathematics - Optimization and Control,Computer Science - Data Structures and Algorithms},
  author = {Wilson, Ashia C. and Recht, Benjamin and Jordan, Michael I.},
  file = {/home/arthur/Dropbox/Zotero/Wilson et al_2016_A Lyapunov Analysis of Momentum Methods in Optimization.pdf;/home/arthur/Zotero/storage/C9RIIWU5/1611.html}
}

@book{doob_classical_2012,
  title = {Classical {{Potential Theory}} and {{Its Probabilistic Counterpart}}: {{Advanced Problems}}},
  volume = {262},
  url = {https://books.google.fr/books?hl=fr&lr=&id=DEDaBwAAQBAJ&oi=fnd&pg=PR21&dq=doob+forward+convergence+discrete&ots=g0PWmUaFb1&sig=OP71Ne4km_K5Z0-2Rwwb2AmXvCU},
  shorttitle = {Classical {{Potential Theory}} and {{Its Probabilistic Counterpart}}},
  publisher = {{Springer Science \& Business Media}},
  urldate = {2016-12-26},
  date = {2012},
  author = {Doob, Joseph L.},
  file = {/home/arthur/Dropbox/Zotero/Doob_2012_Classical Potential Theory and Its Probabilistic Counterpart.pdf;/home/arthur/Zotero/storage/MDIH7KGA/books.html}
}

@article{bietti_stochastic_2016,
  title = {Stochastic {{Optimization}} with {{Variance Reduction}} for {{Infinite Datasets}} with {{Finite}}-{{Sum Structure}}},
  url = {https://arxiv.org/abs/1610.00970},
  journaltitle = {arXiv preprint arXiv:1610.00970},
  urldate = {2016-12-26},
  date = {2016},
  author = {Bietti, Alberto and Mairal, Julien},
  file = {/home/arthur/Dropbox/Zotero/Bietti_Mairal_2016_Stochastic Optimization with Variance Reduction for Infinite Datasets with.pdf}
}

@article{shrivastava_learning_2016,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1612.07828},
  primaryClass = {cs},
  title = {Learning from {{Simulated}} and {{Unsupervised Images}} through {{Adversarial Training}}},
  url = {http://arxiv.org/abs/1612.07828},
  abstract = {With recent progress in graphics, it has become more tractable to train models on synthetic images, potentially avoiding the need for expensive annotations. However, learning from synthetic images may not achieve the desired performance due to a gap between synthetic and real image distributions. To reduce this gap, we propose Simulated+Unsupervised (S+U) learning, where the task is to learn a model to improve the realism of a simulator's output using unlabeled real data, while preserving the annotation information from the simulator. We develop a method for S+U learning that uses an adversarial network similar to Generative Adversarial Networks (GANs), but with synthetic images as inputs instead of random vectors. We make several key modifications to the standard GAN algorithm to preserve annotations, avoid artifacts and stabilize training: (i) a 'self-regularization' term, (ii) a local adversarial loss, and (iii) updating the discriminator using a history of refined images. We show that this enables generation of highly realistic images, which we demonstrate both qualitatively and with a user study. We quantitatively evaluate the generated images by training models for gaze estimation and hand pose estimation. We show a significant improvement over using synthetic images, and achieve state-of-the-art results on the MPIIGaze dataset without any labeled real data.},
  urldate = {2016-12-26},
  date = {2016-12-22},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Learning},
  author = {Shrivastava, Ashish and Pfister, Tomas and Tuzel, Oncel and Susskind, Josh and Wang, Wenda and Webb, Russ},
  file = {/home/arthur/Dropbox/Zotero/Shrivastava et al_2016_Learning from Simulated and Unsupervised Images through Adversarial Training.pdf;/home/arthur/Zotero/storage/A4Q47ZAX/1612.html}
}

@article{gittens_matrix_2016,
  title = {Matrix {{Factorization}} at {{Scale}}: A {{Comparison}} of {{Scientific Data Analytics}} in {{Spark}} and {{C}}+ {{MPI Using Three Case Studies}}},
  url = {https://arxiv.org/abs/1607.01335},
  shorttitle = {Matrix {{Factorization}} at {{Scale}}},
  journaltitle = {arXiv preprint arXiv:1607.01335},
  urldate = {2016-12-27},
  date = {2016},
  author = {Gittens, Alex and Devarakonda, Aditya and Racah, Evan and Ringenburg, Michael and Gerhardt, Lisa and Kottalam, Jey and Liu, Jialin and Maschhoff, Kristyn and Canon, Shane and Chhugani, Jatin and {others}},
  file = {/home/arthur/Dropbox/Zotero/Gittens et al_2016_Matrix Factorization at Scale.pdf}
}

@inproceedings{bachem_fast_2016,
  title = {Fast and {{Provably Good Seedings}} for K-{{Means}}},
  url = {http://papers.nips.cc/paper/6478-fast-and-provably-good-seedings-for-k-means},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  urldate = {2017-01-03},
  date = {2016},
  pages = {55--63},
  author = {Bachem, Olivier and Lucic, Mario and Hassani, Hamed and Krause, Andreas},
  file = {/home/arthur/Dropbox/Zotero/Bachem et al_2016_Fast and Provably Good Seedings for k-Means.pdf}
}

@article{lin_quickening_2016,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1610.00960},
  primaryClass = {math, stat},
  title = {{{QuickeNing}}: {{A Generic Quasi}}-{{Newton Algorithm}} for {{Faster Gradient}}-{{Based Optimization}} *},
  url = {http://arxiv.org/abs/1610.00960},
  shorttitle = {{{QuickeNing}}},
  abstract = {We propose an approach to accelerate gradient-based optimization algorithms by giving them the ability to exploit curvature information using quasi-Newton update rules. The proposed scheme, called QuickeNing, is generic and can be applied to a large class of first-order methods such as incremental and block-coordinate algorithms; it is also compatible with composite objectives, meaning that it has the ability to provide exactly sparse solutions when the objective involves a sparsity-inducing regularization. QuickeNing relies on limited-memory BFGS rules, making it appropriate for solving high-dimensional optimization problems; with no line-search, it is also simple to use and to implement. Besides, it enjoys a worst-case linear convergence rate for strongly convex problems. We present experimental results where QuickeNing gives significant improvements over competing methods for solving large-scale high-dimensional machine learning problems.},
  urldate = {2017-01-11},
  date = {2016-10-04},
  keywords = {Mathematics - Optimization and Control,Statistics - Machine Learning},
  author = {Lin, Hongzhou and Mairal, Julien and Harchaoui, Zaid},
  file = {/home/arthur/Dropbox/Zotero/Lin et al_2016_QuickeNing.pdf;/home/arthur/Zotero/storage/ST6MFWDI/1610.html}
}

@article{wu-minn_hcp_2014,
  title = {{{HCP Release Manual}}},
  url = {https://www.humanconnectome.org/documentation/S900/HCP_S900_Release_Reference_Manual.pdf},
  shorttitle = {500 {{Subjects Data Release}}},
  urldate = {2017-01-12},
  date = {2014},
  author = {WU-Minn, H. C. P.},
  file = {/home/arthur/Dropbox/Zotero/WU-Minn_2014_HCP Release Manual.pdf;/home/arthur/Dropbox/Zotero/WU-Minn_2014_HCP Release Manual2.pdf}
}

@article{mukherjee_reduced_2011,
  title = {Reduced {{Rank Ridge Regression}} and {{Its Kernel Extensions}}},
  volume = {4},
  issn = {1932-1864},
  url = {http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3444519/},
  abstract = {In multivariate linear regression, it is often assumed that the response matrix is intrinsically of lower rank. This could be because of the correlation structure among the prediction variables or the coefficient matrix being lower rank. To accommodate both, we propose a reduced rank ridge regression for multivariate linear regression. Specifically, we combine the ridge penalty with the reduced rank constraint on the coefficient matrix to come up with a computationally straightforward algorithm. Numerical studies indicate that the proposed method consistently outperforms relevant competitors. A novel extension of the proposed method to the reproducing kernel Hilbert space (RKHS) set-up is also developed.},
  number = {6},
  journaltitle = {Statistical analysis and data mining},
  shortjournal = {Stat Anal Data Min},
  urldate = {2017-01-12},
  date = {2011-12},
  pages = {612-622},
  author = {Mukherjee, Ashin and Zhu, Ji},
  file = {/home/arthur/Dropbox/Zotero/Mukherjee_Zhu_2011_Reduced Rank Ridge Regression and Its Kernel Extensions.pdf},
  eprinttype = {pmid},
  eprint = {22993641},
  pmcid = {PMC3444519}
}

@article{shefi_rate_2014,
  title = {Rate of {{Convergence Analysis}} of {{Decomposition Methods Based}} on the {{Proximal Method}} of {{Multipliers}} for {{Convex Minimization}}},
  volume = {24},
  issn = {1052-6234},
  url = {http://epubs.siam.org/doi/abs/10.1137/130910774},
  abstract = {This paper presents two classes of decomposition algorithms based on the proximal method of multipliers (PMM) introduced in the mid-1970s by Rockafellar for convex minimization. We first show that the PMM framework is at the root of many past and recent decomposition schemes suggested in the literature allowing for an elementary analysis of these methods through a unified scheme. We then prove various sublinear global convergence rate results for the two classes of PMM based decomposition algorithms for function values and constraints violation. Furthermore, under a mild assumption on the problem's data we derive rate of convergence results in terms of the original primal function values for both classes. As a by-product of our analysis we also obtain convergence of the sequences produced by the two algorithm classes to optimal primal-dual solutions.},
  number = {1},
  journaltitle = {SIAM Journal on Optimization},
  shortjournal = {SIAM J. Optim.},
  urldate = {2017-01-26},
  date = {2014-01-01},
  pages = {269-297},
  author = {Shefi, R. and Teboulle, M.},
  file = {/home/arthur/Dropbox/Zotero/Shefi_Teboulle_2014_Rate of Convergence Analysis of Decomposition Methods Based on the Proximal.pdf;/home/arthur/Zotero/storage/A9HU9PS6/130910774.html}
}

@book{bertsekas_convex_2015,
  title = {Convex {{Optimization Algorithms}}},
  url = {http://www.athenasc.com/Contents_Preface_NEW_ALG.pdf},
  publisher = {{Athena Scientific}},
  urldate = {2017-01-26},
  date = {2015},
  author = {Bertsekas, Dimitri P.},
  file = {/home/arthur/Dropbox/Zotero/Bertsekas_2015_Convex Optimization Algorithms.pdf}
}

@unpublished{teboulle_first_2017,
  title = {First Order Methods, 1-6},
  url = {http://www.math.tau.ac.il/~teboulle/lx/lec1-to-6.pdf},
  urldate = {2017-01-25},
  date = {2017},
  author = {Teboulle, Marc},
  file = {/home/arthur/Dropbox/Zotero/Teboulle_2017_First order methods, 1-6.pdf}
}

@unpublished{teboulle_first_2017-1,
  title = {First Order Methods, 7-8},
  url = {http://www.math.tau.ac.il/~teboulle/lx/lec1-to-6.pdf},
  urldate = {2017-01-25},
  date = {2017},
  author = {Teboulle, Marc},
  file = {/home/arthur/Dropbox/Zotero/Teboulle_2017_First order methods, 7-8.pdf}
}

@article{goodfellow_nips_2016,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1701.00160},
  primaryClass = {cs},
  title = {{{NIPS}} 2016 {{Tutorial}}: {{Generative Adversarial Networks}}},
  url = {http://arxiv.org/abs/1701.00160},
  shorttitle = {{{NIPS}} 2016 {{Tutorial}}},
  abstract = {This report summarizes the tutorial presented by the author at NIPS 2016 on generative adversarial networks (GANs). The tutorial describes: (1) Why generative modeling is a topic worth studying, (2) how generative models work, and how GANs compare to other generative models, (3) the details of how GANs work, (4) research frontiers in GANs, and (5) state-of-the-art image models that combine GANs with other methods. Finally, the tutorial contains three exercises for readers to complete, and the solutions to these exercises.},
  urldate = {2017-02-06},
  date = {2016-12-31},
  keywords = {Computer Science - Learning},
  author = {Goodfellow, Ian},
  file = {/home/arthur/Dropbox/Zotero/Goodfellow_2016_NIPS 2016 Tutorial.pdf;/home/arthur/Zotero/storage/N7G37HH9/1701.html}
}

@article{mensch_stochastic_2017,
  title={Stochastic subsampling for factorizing huge matrices},
  author={Mensch, Arthur and Mairal, Julien and Thirion, Bertrand and Varoquaux, Ga{\"e}l},
  journal={IEEE Transactions on Signal Processing},
  volume={66},
  number={1},
  pages={113--128},
  year={2018},
  publisher={IEEE}
}

@article{wu_coordinate_2008,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {0803.3876},
  title = {Coordinate Descent Algorithms for Lasso Penalized Regression},
  volume = {2},
  issn = {1932-6157},
  url = {http://arxiv.org/abs/0803.3876},
  abstract = {Imposition of a lasso penalty shrinks parameter estimates toward zero and performs continuous model selection. Lasso penalized regression is capable of handling linear regression problems where the number of predictors far exceeds the number of cases. This paper tests two exceptionally fast algorithms for estimating regression coefficients with a lasso penalty. The previously known \$$\backslash$ell\_2\$ algorithm is based on cyclic coordinate descent. Our new \$$\backslash$ell\_1\$ algorithm is based on greedy coordinate descent and Edgeworth's algorithm for ordinary \$$\backslash$ell\_1\$ regression. Each algorithm relies on a tuning constant that can be chosen by cross-validation. In some regression problems it is natural to group parameters and penalize parameters group by group rather than separately. If the group penalty is proportional to the Euclidean norm of the parameters of the group, then it is possible to majorize the norm and reduce parameter estimation to \$$\backslash$ell\_2\$ regression with a lasso penalty. Thus, the existing algorithm can be extended to novel settings. Each of the algorithms discussed is tested via either simulated or real data or both. The Appendix proves that a greedy form of the \$$\backslash$ell\_2\$ algorithm converges to the minimum value of the objective function.},
  number = {1},
  journaltitle = {The Annals of Applied Statistics},
  urldate = {2017-02-09},
  date = {2008-03},
  pages = {224-244},
  keywords = {Statistics - Applications},
  author = {Wu, Tong Tong and Lange, Kenneth},
  file = {/home/arthur/Dropbox/Zotero/Wu_Lange_2008_Coordinate descent algorithms for lasso penalized regression.pdf;/home/arthur/Zotero/storage/7RFEB77A/0803.html}
}

@article{kim_interior-point_2007,
  title = {An {{Interior}}-{{Point Method}} for {{Large}}-{{Scale}} -{{Regularized Least Squares}}},
  volume = {1},
  issn = {1932-4553, 1941-0484},
  url = {http://ieeexplore.ieee.org/document/4407767/},
  number = {4},
  journaltitle = {IEEE Journal of Selected Topics in Signal Processing},
  urldate = {2017-02-09},
  date = {2007-12},
  pages = {606-617},
  author = {Kim, Seung-Jean and Koh, K. and Lustig, M. and Boyd, Stephen and Gorinevsky, Dimitry},
  file = {/home/arthur/Dropbox/Zotero/Kim et al_2007_An Interior-Point Method for Large-Scale -Regularized Least Squares.pdf}
}

@article{xiang_screening_2016,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1405.4897},
  title = {Screening {{Tests}} for {{Lasso Problems}}},
  issn = {0162-8828, 2160-9292},
  url = {http://arxiv.org/abs/1405.4897},
  abstract = {This paper is a survey of dictionary screening for the lasso problem. The lasso problem seeks a sparse linear combination of the columns of a dictionary to best match a given target vector. This sparse representation has proven useful in a variety of subsequent processing and decision tasks. For a given target vector, dictionary screening quickly identifies a subset of dictionary columns that will receive zero weight in a solution of the corresponding lasso problem. These columns can be removed from the dictionary prior to solving the lasso problem without impacting the optimality of the solution obtained. This has two potential advantages: it reduces the size of the dictionary, allowing the lasso problem to be solved with less resources, and it may speed up obtaining a solution. Using a geometrically intuitive framework, we provide basic insights for understanding useful lasso screening tests and their limitations. We also provide illustrative numerical studies on several datasets.},
  journaltitle = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  urldate = {2017-02-09},
  date = {2016},
  pages = {1-1},
  keywords = {Computer Science - Learning,Statistics - Machine Learning},
  author = {Xiang, Zhen James and Wang, Yun and Ramadge, Peter J.},
  file = {/home/arthur/Dropbox/Zotero/Xiang et al_2016_Screening Tests for Lasso Problems.pdf;/home/arthur/Zotero/storage/5NPRJWRR/1405.html}
}

@inproceedings{nowozin_f-gan_2016,
  title = {F-{{GAN}}: {{Training}} Generative Neural Samplers Using Variational Divergence Minimization},
  url = {http://papers.nips.cc/paper/6066-f-gan-training-generative-neural-samplers-using-variational-divergence-minimization},
  shorttitle = {F-{{GAN}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  urldate = {2017-02-09},
  date = {2016},
  pages = {271--279},
  author = {Nowozin, Sebastian and Cseke, Botond and Tomioka, Ryota},
  file = {/home/arthur/Dropbox/Zotero/Nowozin et al_2016_f-GAN.pdf;/home/arthur/Zotero/storage/IQNQRKUG/6066-f-gan-training-generative-neural-samplers-using-variational-divergence-minimization.html}
}

@article{baldi_neural_1989,
  title = {Neural Networks and Principal Component Analysis: {{Learning}} from Examples without Local Minima},
  volume = {2},
  url = {http://www.sciencedirect.com/science/article/pii/0893608089900142},
  shorttitle = {Neural Networks and Principal Component Analysis},
  number = {1},
  journaltitle = {Neural networks},
  urldate = {2017-02-14},
  date = {1989},
  pages = {53--58},
  author = {Baldi, Pierre and Hornik, Kurt},
  file = {/home/arthur/Dropbox/Zotero/Baldi_Hornik_1989_Neural networks and principal component analysis.pdf;/home/arthur/Zotero/storage/MES9DJ7U/0893608089900142.html}
}

@article{ndiaye_gap_2015,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1506.03736},
  primaryClass = {cs, math, stat},
  title = {{{GAP Safe}} Screening Rules for Sparse Multi-Task and Multi-Class Models},
  url = {http://arxiv.org/abs/1506.03736},
  abstract = {High dimensional regression benefits from sparsity promoting regularizations. Screening rules leverage the known sparsity of the solution by ignoring some variables in the optimization, hence speeding up solvers. When the procedure is proven not to discard features wrongly the rules are said to be $\backslash$emph\{safe\}. In this paper we derive new safe rules for generalized linear models regularized with \$$\backslash$ell\_1\$ and \$$\backslash$ell\_1/$\backslash$ell\_2\$ norms. The rules are based on duality gap computations and spherical safe regions whose diameters converge to zero. This allows to discard safely more variables, in particular for low regularization parameters. The GAP Safe rule can cope with any iterative solver and we illustrate its performance on coordinate descent for multi-task Lasso, binary and multinomial logistic regression, demonstrating significant speed ups on all tested datasets with respect to previous safe rules.},
  urldate = {2017-02-16},
  date = {2015-06-11},
  keywords = {68Uxx; 49N15; 62Jxx; 68Q32; 62-04,Computer Science - Learning,Mathematics - Optimization and Control,Statistics - Computation,Statistics - Machine Learning},
  author = {Ndiaye, Eugene and Fercoq, Olivier and Gramfort, Alexandre and Salmon, Joseph},
  file = {/home/arthur/Dropbox/Zotero/Ndiaye et al_2015_GAP Safe screening rules for sparse multi-task and multi-class models.pdf;/home/arthur/Zotero/storage/DK4P8NDC/1506.html}
}

@article{cawley_sparse_2007,
  title = {Sparse Multinomial Logistic Regression via Bayesian L1 Regularisation},
  volume = {19},
  url = {https://papers.nips.cc/paper/3155-sparse-multinomial-logistic-regression-via-bayesian-l1-regularisation.pdf},
  journaltitle = {Advances in neural information processing systems},
  urldate = {2017-02-17},
  date = {2007},
  pages = {209},
  author = {Cawley, Gavin C. and Talbot, Nicola LC and Girolami, Mark},
  file = {/home/arthur/Dropbox/Zotero/Cawley et al_2007_Sparse multinomial logistic regression via bayesian l1 regularisation.pdf}
}

@article{blondel_block_2013,
  langid = {english},
  title = {Block Coordinate Descent Algorithms for Large-Scale Sparse Multiclass Classification},
  volume = {93},
  issn = {0885-6125, 1573-0565},
  url = {http://link.springer.com/10.1007/s10994-013-5367-2},
  number = {1},
  journaltitle = {Machine Learning},
  urldate = {2017-02-17},
  date = {2013-10},
  pages = {31-52},
  author = {Blondel, Mathieu and Seki, Kazuhiro and Uehara, Kuniaki},
  file = {/home/arthur/Dropbox/Zotero/Blondel et al_2013_Block coordinate descent algorithms for large-scale sparse multiclass.pdf}
}

@inproceedings{johnson_blitz_2015,
  title = {Blitz: {{A Principled Meta}}-{{Algorithm}} for {{Scaling Sparse Optimization}}.},
  url = {http://www.jmlr.org/proceedings/papers/v37/johnson15.pdf},
  shorttitle = {Blitz},
  booktitle = {{{ICML}}},
  urldate = {2017-02-22},
  date = {2015},
  pages = {1171--1179},
  author = {Johnson, Tyler and Guestrin, Carlos},
  file = {/home/arthur/Dropbox/Zotero/Johnson_Guestrin_2015_Blitz.pdf}
}

@unpublished{schmidt_convex_2015,
  title = {Convex {{Optimization}}},
  url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.672.5064&rep=rep1&type=pdf},
  urldate = {2017-02-24},
  date = {2015},
  author = {Schmidt, Mark},
  file = {/home/arthur/Dropbox/Zotero/Schmidt_2015_Convex Optimization.pdf}
}

@book{watt_machine_2016,
  title = {Machine {{Learning Refined}}: {{Foundations}}, {{Algorithms}}, and {{Applications}}},
  url = {https://books.google.fr/books?hl=fr&lr=&id=GoYwDQAAQBAJ&oi=fnd&pg=PR11&dq=machine+learning+refined&ots=dFgwD9H5cp&sig=c78SQVDl9qO5dYqR9o88dNQt5gA},
  shorttitle = {Machine {{Learning Refined}}},
  publisher = {{Cambridge University Press}},
  urldate = {2017-02-24},
  date = {2016},
  author = {Watt, Jeremy and Borhani, Reza and Katsaggelos, Aggelos},
  file = {/home/arthur/Dropbox/Zotero/Watt et al_2016_Machine Learning Refined.pdf;/home/arthur/Zotero/storage/836KF4UF/Jeremy Watt, Reza Borhani, Aggelos Katsaggelos-Machine Learning Refined_ Foundations, Algorithms, and Applications-Cambridge University Press (2016).epub;/home/arthur/Zotero/storage/XUJN5E24/books.html}
}

@article{martens_new_2014,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1412.1193},
  primaryClass = {cs, stat},
  title = {New Insights and Perspectives on the Natural Gradient Method},
  url = {http://arxiv.org/abs/1412.1193},
  abstract = {Natural gradient descent is an optimization method traditionally motivated from the perspective of information geometry, and works well for many applications as an alternative to stochastic gradient descent. In this paper we critically analyze this method and its properties, and show how it can be viewed as a type of approximate 2nd-order optimization method, where the Fisher information matrix used to compute the natural gradient direction can be viewed as an approximation of the Hessian. This perspective turns out to have significant implications for how to design a practical and robust version of the method. Among our various other contributions is a thorough analysis of the convergence speed of natural gradient descent and more general stochastic methods, a critical examination of the oft-used "empirical" approximation of the Fisher matrix, and an analysis of the (approximate) parameterization invariance property possessed by the method, which we show still holds for certain other choices of the curvature matrix, but notably not the Hessian.},
  urldate = {2017-02-27},
  date = {2014-12-03},
  keywords = {Computer Science - Learning,Statistics - Machine Learning},
  author = {Martens, James},
  file = {/home/arthur/Dropbox/Zotero/Martens_2014_New insights and perspectives on the natural gradient method.pdf;/home/arthur/Zotero/storage/BMESNQMD/1412.html}
}

@article{orhan_skip_2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1701.09175},
  primaryClass = {cs},
  title = {Skip {{Connections}} as {{Effective Symmetry}}-{{Breaking}}},
  url = {http://arxiv.org/abs/1701.09175},
  abstract = {Skip connections made the training of very deep neural networks possible and have become an indispendable component in a variety of neural architectures. A completely satisfactory explanation for their success remains elusive. Here, we present a novel explanation for the benefits of skip connections in training very deep neural networks. We argue that skip connections help break symmetries inherent in the loss landscapes of deep networks, leading to drastically simplified landscapes. In particular, skip connections between adjacent layers in a multilayer network break the permutation symmetry of nodes in a given layer, and the recently proposed DenseNet architecture, where each layer projects skip connections to every layer above it, also breaks the rescaling symmetry of connectivity matrices between different layers. This hypothesis is supported by evidence from a toy model with binary weights and from experiments with fully-connected networks suggesting (i) that skip connections do not necessarily improve training unless they help break symmetries and (ii) that alternative ways of breaking the symmetries also lead to significant performance improvements in training deep networks, hence there is nothing special about skip connections in this respect. We find, however, that skip connections confer additional benefits over and above symmetry-breaking, such as the ability to deal effectively with the vanishing gradients problem.},
  urldate = {2017-02-27},
  date = {2017-01-31},
  keywords = {Computer Science - Learning,Computer Science - Neural and Evolutionary Computing},
  author = {Orhan, A. Emin},
  file = {/home/arthur/Dropbox/Zotero/Orhan_2017_Skip Connections as Effective Symmetry-Breaking.pdf;/home/arthur/Zotero/storage/BI5H5ZRT/1701.html}
}

@article{meinshausen_high-dimensional_2006,
  langid = {english},
  title = {High-Dimensional Graphs and Variable Selection with the {{Lasso}}},
  volume = {34},
  issn = {0090-5364},
  url = {http://projecteuclid.org/Dienst/getRecord?id=euclid.aos/1152540754/},
  number = {3},
  journaltitle = {The Annals of Statistics},
  urldate = {2017-03-13},
  date = {2006-06},
  pages = {1436-1462},
  author = {Meinshausen, Nicolai and Bühlmann, Peter},
  file = {/home/arthur/Dropbox/Zotero/Meinshausen_Bühlmann_2006_High-dimensional graphs and variable selection with the Lasso.pdf}
}

@inproceedings{moritz_linearly-convergent_2016,
  title = {A Linearly-Convergent Stochastic l-Bfgs Algorithm},
  url = {http://www.jmlr.org/proceedings/papers/v51/moritz16.pdf},
  booktitle = {Proceedings of the {{Nineteenth International Conference}} on {{Artificial Intelligence}} and {{Statistics}}},
  urldate = {2017-03-14},
  date = {2016},
  author = {Moritz, Philipp and Nishihara, Robert and Jordan, Michael I.},
  file = {/home/arthur/Dropbox/Zotero/Moritz et al_2016_A linearly-convergent stochastic l-bfgs algorithm.pdf}
}

@article{morerio_curriculum_2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1703.06229},
  primaryClass = {cs, stat},
  title = {Curriculum {{Dropout}}},
  url = {http://arxiv.org/abs/1703.06229},
  abstract = {Dropout is a very effective way of regularizing neural networks. Stochastically "dropping out" units with a certain probability discourages over-specific co-adaptations of feature detectors, preventing overfitting and improving network generalization. Besides, Dropout can be interpreted as an approximate model aggregation technique, where an exponential number of smaller networks are averaged in order to get a more powerful ensemble. In this paper, we show that using a fixed dropout probability during training is a suboptimal choice. We thus propose a time scheduling for the probability of retaining neurons in the network. This induces an adaptive regularization scheme that smoothly increases the difficulty of the optimization problem. This idea of "starting easy" and adaptively increasing the difficulty of the learning problem has its roots in curriculum learning and allows one to train better models. Indeed, we prove that our optimization strategy implements a very general curriculum scheme, by gradually adding noise to both the input and intermediate feature representations within the network architecture. Experiments on seven image classification datasets and different network architectures show that our method, named Curriculum Dropout, frequently yields to better generalization and, at worst, performs just as well as the standard Dropout method.},
  urldate = {2017-03-28},
  date = {2017-03-17},
  keywords = {Computer Science - Learning,Statistics - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  author = {Morerio, Pietro and Cavazza, Jacopo and Volpi, Riccardo and Vidal, Rene and Murino, Vittorio},
  file = {/home/arthur/Dropbox/Zotero/Morerio et al_2017_Curriculum Dropout.pdf;/home/arthur/Zotero/storage/PJJDHWFM/1703.html}
}

@article{umeyama_eigendecomposition_1988,
  title = {An Eigendecomposition Approach to Weighted Graph Matching Problems},
  volume = {10},
  url = {http://ieeexplore.ieee.org/abstract/document/6778/},
  number = {5},
  journaltitle = {IEEE transactions on pattern analysis and machine intelligence},
  urldate = {2017-03-30},
  date = {1988},
  pages = {695--703},
  author = {Umeyama, Shinji},
  file = {/home/arthur/Dropbox/Zotero/Umeyama_1988_An eigendecomposition approach to weighted graph matching problems.pdf}
}

@book{Goodfellow-et-al-2016,
  title = {Deep {{Learning}}},
  publisher = {{MIT Press}},
  date = {2016},
  author = {Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
  file = {/home/arthur/Dropbox/Zotero/Goodfellow et al_2016_Deep Learning.pdf},
  note = {http://www.deeplearningbook.org}
}

@article{rao_neural_2008,
  title = {Neural Correlates of Voluntary and Involuntary Risk Taking in the Human Brain: An {{fMRI Study}} of the {{Balloon Analog Risk Task}} ({{BART}})},
  volume = {42},
  url = {http://www.sciencedirect.com/science/article/pii/S1053811908006927},
  shorttitle = {Neural Correlates of Voluntary and Involuntary Risk Taking in the Human Brain},
  number = {2},
  journaltitle = {Neuroimage},
  urldate = {2017-04-04},
  date = {2008},
  pages = {902--910},
  author = {Rao, Hengyi and Korczykowski, Marc and Pluta, John and Hoang, Angela and Detre, John A.},
  file = {/home/arthur/Dropbox/Zotero/Rao et al_2008_Neural correlates of voluntary and involuntary risk taking in the human brain.pdf;/home/arthur/Zotero/storage/MXPT3Q6S/S1053811908006927.html}
}

@article{glahn_spatial_2003,
  title = {Spatial Working Memory as an Endophenotype for Schizophrenia},
  volume = {53},
  issn = {0006-3223},
  url = {http://www.sciencedirect.com/science/article/pii/S0006322302016414},
  abstract = {Background
Spatial working memory impairments are among the neurocognitive deficits that may mark genetic predisposition toward schizophrenia. We previously reported that impairment on the spatial span subtask of the Wechsler Adult Intelligence Scale—Revised increased in a dose-dependent manner with increasing genetic predisposition toward schizophrenia in a sample of discordant twins; however, it remains to be determined whether these deficits reflect difficulties with encoding, maintenance, manipulation, time-tagging of visual spatial information, storage capacity, or complex motor response.
Methods
We developed a spatial delayed response task in which memory set size was parametrically varied, holding constant manipulation and decision processes. We then reassessed 80 of the previously studied twins (17 probands with 8 monozygotic co-twins and 13 dizygotic co-twins, and 42 healthy twins).
Results
The spatial delayed response task was sensitive to genetic loading for schizophrenia but did not provide evidence for capacity limitations in probands or their co-twins.
Conclusions
The findings suggest that deficits in the encoding or storage aspects of short-term spatial mnemonic processing may be an effective endophenotypic marker for schizophrenia.},
  number = {7},
  journaltitle = {Biological Psychiatry},
  shortjournal = {Biological Psychiatry},
  urldate = {2017-04-04},
  date = {2003-04-01},
  pages = {624-626},
  keywords = {Schizophrenia,spatial working memory,behavioral genetics,twins,neuropsychology,endophenotype},
  author = {Glahn, David C and Therman, Sebastian and Manninen, Marko and Huttunen, Matti and Kaprio, Joakko and Lönnqvist, Jouko and Cannon, Tyrone D},
  file = {/home/arthur/Dropbox/Zotero/Glahn et al_2003_Spatial working memory as an endophenotype for schizophrenia.pdf;/home/arthur/Zotero/storage/WP2E255N/S0006322302016414.html}
}

@article{han_learning_2015,
  title = {Learning Both {{Weights}} and {{Connections}} for {{Efficient Neural Networks}}},
  url = {zotero://attachment/1231/},
  urldate = {2017-04-07},
  date = {2015-06-08},
  author = {Han, Song and Pool, Jeff and Tran, John and Dally, William J.},
  file = {/home/arthur/Dropbox/Zotero/Han et al_2015_Learning both Weights and Connections for Efficient Neural Networks.pdf;/home/arthur/Zotero/storage/XI28CR6Z/1231.html}
}

@article{li_improved_2016-1,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1602.02220},
  primaryClass = {cs, stat},
  title = {Improved {{Dropout}} for {{Shallow}} and {{Deep Learning}}},
  url = {http://arxiv.org/abs/1602.02220},
  abstract = {Dropout has been witnessed with great success in training deep neural networks by independently zeroing out the outputs of neurons at random. It has also received a surge of interest for shallow learning, e.g., logistic regression. However, the independent sampling for dropout could be suboptimal for the sake of convergence. In this paper, we propose to use multinomial sampling for dropout, i.e., sampling features or neurons according to a multinomial distribution with different probabilities for different features/neurons. To exhibit the optimal dropout probabilities, we analyze the shallow learning with multinomial dropout and establish the risk bound for stochastic optimization. By minimizing a sampling dependent factor in the risk bound, we obtain a distribution-dependent dropout with sampling probabilities dependent on the second order statistics of the data distribution. To tackle the issue of evolving distribution of neurons in deep learning, we propose an efficient adaptive dropout (named $\backslash$textbf\{evolutional dropout\}) that computes the sampling probabilities on-the-fly from a mini-batch of examples. Empirical studies on several benchmark datasets demonstrate that the proposed dropouts achieve not only much faster convergence and but also a smaller testing error than the standard dropout. For example, on the CIFAR-100 data, the evolutional dropout achieves relative improvements over 10$\backslash$\% on the prediction performance and over 50$\backslash$\% on the convergence speed compared to the standard dropout.},
  urldate = {2017-04-07},
  date = {2016-02-06},
  keywords = {Computer Science - Learning,Statistics - Machine Learning},
  author = {Li, Zhe and Gong, Boqing and Yang, Tianbao},
  file = {/home/arthur/Dropbox/Zotero/Li et al_2016_Improved Dropout for Shallow and Deep Learning.pdf;/home/arthur/Zotero/storage/AK8H87B6/1602.html}
}

@article{meinshausen_stability_2008,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {0809.2932},
  primaryClass = {stat},
  title = {Stability {{Selection}}},
  url = {http://arxiv.org/abs/0809.2932},
  abstract = {Estimation of structure, such as in variable selection, graphical modelling or cluster analysis is notoriously difficult, especially for high-dimensional data. We introduce stability selection. It is based on subsampling in combination with (high-dimensional) selection algorithms. As such, the method is extremely general and has a very wide range of applicability. Stability selection provides finite sample control for some error rates of false discoveries and hence a transparent principle to choose a proper amount of regularisation for structure estimation. Variable selection and structure estimation improve markedly for a range of selection methods if stability selection is applied. We prove for randomised Lasso that stability selection will be variable selection consistent even if the necessary conditions needed for consistency of the original Lasso method are violated. We demonstrate stability selection for variable selection and Gaussian graphical modelling, using real and simulated data.},
  urldate = {2017-04-07},
  date = {2008-09-17},
  keywords = {Statistics - Methodology},
  author = {Meinshausen, Nicolai and Buehlmann, Peter},
  file = {/home/arthur/Dropbox/Zotero/Meinshausen_Buehlmann_2008_Stability Selection.pdf;/home/arthur/Zotero/storage/5IC2J7SN/0809.html}
}

@article{hinton_improving_2012,
  title = {Improving Neural Networks by Preventing Co-Adaptation of Feature Detectors},
  url = {https://arxiv.org/abs/1207.0580},
  journaltitle = {arXiv preprint arXiv:1207.0580},
  urldate = {2017-04-07},
  date = {2012},
  author = {Hinton, Geoffrey E. and Srivastava, Nitish and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan R.},
  file = {/home/arthur/Dropbox/Zotero/Hinton et al_2012_Improving neural networks by preventing co-adaptation of feature detectors.pdf}
}

@article{kingma_adam_2014,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1412.6980},
  primaryClass = {cs},
  title = {Adam: {{A Method}} for {{Stochastic Optimization}}},
  url = {http://arxiv.org/abs/1412.6980},
  shorttitle = {Adam},
  abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.},
  urldate = {2017-04-07},
  date = {2014-12-22},
  keywords = {Computer Science - Learning},
  author = {Kingma, Diederik P. and Ba, Jimmy},
  file = {/home/arthur/Dropbox/Zotero/Kingma_Ba_2014_Adam.pdf;/home/arthur/Zotero/storage/GQRBSNAZ/1412.html}
}

@article{titsias_one-vs-each_2016,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1609.07410},
  primaryClass = {stat},
  title = {One-vs-{{Each Approximation}} to {{Softmax}} for {{Scalable Estimation}} of {{Probabilities}}},
  url = {http://arxiv.org/abs/1609.07410},
  abstract = {The softmax representation of probabilities for categorical variables plays a prominent role in modern machine learning with numerous applications in areas such as large scale classification, neural language modeling and recommendation systems. However, softmax estimation is very expensive for large scale inference because of the high cost associated with computing the normalizing constant. Here, we introduce an efficient approximation to softmax probabilities which takes the form of a rigorous lower bound on the exact probability. This bound is expressed as a product over pairwise probabilities and it leads to scalable estimation based on stochastic optimization. It allows us to perform doubly stochastic estimation by subsampling both training instances and class labels. We show that the new bound has interesting theoretical properties and we demonstrate its use in classification problems.},
  urldate = {2017-04-07},
  date = {2016-09-23},
  keywords = {Statistics - Machine Learning},
  author = {Titsias, Michalis K.},
  file = {/home/arthur/Dropbox/Zotero/Titsias_2016_One-vs-Each Approximation to Softmax for Scalable Estimation of Probabilities.pdf;/home/arthur/Zotero/storage/RBAR98VF/1609.html}
}

@online{ruder_approximating_2016,
  title = {Approximating the {{Softmax}} for {{Learning Word Embeddings}}},
  url = {u=http://sebastianruder.com/word-embeddings-softmax/},
  abstract = {This blog post gives an overview of softmax-based and sampling-based approaches that approximate the softmax layer for learning word embeddings.},
  urldate = {2017-04-12},
  date = {2016-06-13T10:00:00.000Z},
  author = {Ruder, Sebastian},
  file = {/home/arthur/Zotero/storage/8II3EEF2/index.html}
}

@article{varoquaux_how_2014,
  title = {How Machine Learning Is Shaping Cognitive Neuroimaging},
  volume = {3},
  issn = {2047-217X},
  url = {http://dx.doi.org/10.1186/2047-217X-3-28},
  abstract = {Functional brain images are rich and noisy data that can capture indirect signatures of neural activity underlying cognition in a given experimental setting. Can data mining leverage them to build models of cognition? Only if it is applied to well-posed questions, crafted to reveal cognitive mechanisms. Here we review how predictive models have been used on neuroimaging data to ask new questions, i.e., to uncover new aspects of cognitive organization. We also give a statistical learning perspective on these progresses and on the remaining gaping holes.},
  journaltitle = {GigaScience},
  shortjournal = {GigaScience},
  urldate = {2017-05-05},
  date = {2014},
  pages = {28},
  keywords = {machine learning,fMRI,neuroimaging,Cognition,Decoding,Encoding},
  author = {Varoquaux, Gael and Thirion, Bertrand},
  file = {/home/arthur/Dropbox/Zotero/Varoquaux_Thirion_2014_How machine learning is shaping cognitive neuroimaging.pdf;/home/arthur/Zotero/storage/S3MDUTHA/2047-217X-3-28.html}
}

@article{poldrack_decoding_2009,
  title = {Decoding the Large-Scale Structure of Brain Function by Classifying Mental States across Individuals},
  volume = {20},
  url = {http://pss.sagepub.com/content/20/11/1364.short},
  number = {11},
  journaltitle = {Psychological Science},
  urldate = {2017-05-05},
  date = {2009},
  pages = {1364--1372},
  author = {Poldrack, Russell A. and Halchenko, Yaroslav O. and Hanson, Stephen José},
  file = {/home/arthur/Dropbox/Zotero/Poldrack et al_2009_Decoding the large-scale structure of brain function by classifying mental.pdf}
}

@article{medaglia_cognitive_2015,
  langid = {english},
  title = {Cognitive {{Network Neuroscience}}},
  volume = {27},
  issn = {0898-929X, 1530-8898},
  url = {http://www.mitpressjournals.org/doi/10.1162/jocn_a_00810},
  number = {8},
  journaltitle = {Journal of Cognitive Neuroscience},
  urldate = {2017-05-05},
  date = {2015-08},
  pages = {1471-1491},
  author = {Medaglia, John D. and Lynall, Mary-Ellen and Bassett, Danielle S.},
  file = {/home/arthur/Dropbox/Zotero/Medaglia et al_2015_Cognitive Network Neuroscience.pdf}
}

@article{tenenbaum_how_2011,
  langid = {english},
  title = {How to {{Grow}} a {{Mind}}: {{Statistics}}, {{Structure}}, and {{Abstraction}}},
  volume = {331},
  issn = {0036-8075, 1095-9203},
  url = {http://www.sciencemag.org/cgi/doi/10.1126/science.1192788},
  shorttitle = {How to {{Grow}} a {{Mind}}},
  number = {6022},
  journaltitle = {Science},
  urldate = {2017-05-05},
  date = {2011-03-11},
  pages = {1279-1285},
  author = {Tenenbaum, J. B. and Kemp, C. and Griffiths, T. L. and Goodman, N. D.},
  file = {/home/arthur/Dropbox/Zotero/Tenenbaum et al_2011_How to Grow a Mind.pdf}
}

@article{yeo_estimates_2014,
  langid = {english},
  title = {Estimates of Segregation and Overlap of Functional Connectivity Networks in the Human Cerebral Cortex},
  volume = {88},
  issn = {10538119},
  url = {http://linkinghub.elsevier.com/retrieve/pii/S1053811913010690},
  journaltitle = {NeuroImage},
  urldate = {2017-05-05},
  date = {2014-03},
  pages = {212-227},
  author = {Yeo, B.T. Thomas and Krienen, Fenna M. and Chee, Michael W.L. and Buckner, Randy L.},
  file = {/home/arthur/Dropbox/Zotero/Yeo et al_2014_Estimates of segregation and overlap of functional connectivity networks in the.pdf}
}

@article{yeo_functional_2015,
  langid = {english},
  title = {Functional {{Specialization}} and {{Flexibility}} in {{Human Association Cortex}}},
  volume = {25},
  issn = {1047-3211, 1460-2199},
  url = {https://academic.oup.com/cercor/article-lookup/doi/10.1093/cercor/bhu217},
  number = {10},
  journaltitle = {Cerebral Cortex},
  urldate = {2017-05-05},
  date = {2015-10},
  pages = {3654-3672},
  author = {Yeo, B. T. Thomas and Krienen, Fenna M. and Eickhoff, Simon B. and Yaakub, Siti N. and Fox, Peter T. and Buckner, Randy L. and Asplund, Christopher L. and Chee, Michael W.L.},
  file = {/home/arthur/Dropbox/Zotero/Yeo et al_2015_Functional Specialization and Flexibility in Human Association Cortex.pdf}
}

@article{srivastava_dropout_2014,
  title = {Dropout: A Simple Way to Prevent Neural Networks from Overfitting.},
  volume = {15},
  url = {http://www.jmlr.org/papers/volume15/srivastava14a.old/source/srivastava14a.pdf},
  shorttitle = {Dropout},
  number = {1},
  journaltitle = {Journal of Machine Learning Research},
  urldate = {2017-05-10},
  date = {2014},
  pages = {1929--1958},
  author = {Srivastava, Nitish and Hinton, Geoffrey E. and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan},
  file = {/home/arthur/Dropbox/Zotero/Srivastava et al_2014_Dropout.pdf}
}

@article{ocraven_fmri_1999,
  title = {{{fMRI}} Evidence for Objects as the Units of Attentional Selection},
  volume = {401},
  url = {http://www.nature.com/nature/journal/v401/n6753/abs/401584a0.html},
  number = {6753},
  journaltitle = {Nature},
  urldate = {2017-05-12},
  date = {1999},
  pages = {584--587},
  author = {O'craven, Kathleen M. and Downing, Paul E. and Kanwisher, Nancy},
  file = {/home/arthur/Zotero/storage/9EHC4QII/401584a0.html;/home/arthur/Zotero/storage/CP4CXZ4T/401584a0.html}
}

@article{thirion_which_2014,
  title = {Which {{fMRI}} Clustering Gives Good Brain Parcellations?},
  volume = {8},
  issn = {1662-4548},
  url = {http://www.ncbi.nlm.nih.gov/pmc/articles/PMC4076743/},
  abstract = {Analysis and interpretation of neuroimaging data often require one to divide the brain into a number of regions, or parcels, with homogeneous characteristics, be these regions defined in the brain volume or on the cortical surface. While predefined brain atlases do not adapt to the signal in the individual subject images, parcellation approaches use brain activity (e.g., found in some functional contrasts of interest) and clustering techniques to define regions with some degree of signal homogeneity. In this work, we address the question of which clustering technique is appropriate and how to optimize the corresponding model. We use two principled criteria: goodness of fit (accuracy), and reproducibility of the parcellation across bootstrap samples. We study these criteria on both simulated and two task-based functional Magnetic Resonance Imaging datasets for the Ward, spectral and k-means clustering algorithms. We show that in general Ward’s clustering performs better than alternative methods with regard to reproducibility and accuracy and that the two criteria diverge regarding the preferred models (reproducibility leading to more conservative solutions), thus deferring the practical decision to a higher level alternative, namely the choice of a trade-off between accuracy and stability.},
  journaltitle = {Frontiers in Neuroscience},
  shortjournal = {Front Neurosci},
  urldate = {2017-05-12},
  date = {2014-07-01},
  author = {Thirion, Bertrand and Varoquaux, Gaël and Dohmatob, Elvis and Poline, Jean-Baptiste},
  file = {/home/arthur/Dropbox/Zotero/Thirion et al_2014_Which fMRI clustering gives good brain parcellations.pdf},
  eprinttype = {pmid},
  eprint = {25071425},
  pmcid = {PMC4076743}
}

@inproceedings{gramfort_identifying_2013,
  title = {Identifying {{Predictive Regions}} from {{fMRI}} with {{TV}}-{{L1 Prior}}},
  doi = {10.1109/PRNI.2013.14},
  abstract = {Decoding, i.e. predicting stimulus related quantities from functional brain images, is a powerful tool to demonstrate differences between brain activity across conditions. However, unlike standard brain mapping, it offers no guaranties on the localization of this information. Here, we consider decoding as a statistical estimation problem and show that injecting a spatial segmentation prior leads to unmatched performance in recovering predictive regions. Specifically, we use ℓ1-penalization to set voxels to zero and Total-Variation (TV) penalization to segment regions. Our contribution is two-fold. On the one hand, we show via extensive experiments that, amongst a large selection of decoding and brain-mapping strategies, TV+ℓ1 leads to best region recovery. On the other hand, we consider implementation issues related to this estimator. To tackle efficiently this joint prediction-segmentation problem we introduce a fast optimization algorithm based on a primal-dual approach. We also tackle automatic setting of hyper-parameters and fast computation of image operation on the irregular masks that arise in brain imaging.},
  eventtitle = {2013 {{International Workshop}} on {{Pattern Recognition}} in {{Neuroimaging}}},
  booktitle = {2013 {{International Workshop}} on {{Pattern Recognition}} in {{Neuroimaging}}},
  date = {2013-06},
  pages = {17-20},
  keywords = {Brain,Brain Mapping,biomedical MRI,image coding,medical image processing,optimisation,Estimation,fMRI,Decoding,image segmentation,statistical analysis,ℓ1-penalization,TV-ℓ1 prior,brain activity,brain mapping strategy,decoding strategy,fast optimization algorithm,functional brain images,prediction-segmentation problem,predictive region identification,primal-dual approach,spatial segmentation prior,statistical estimation problem,stimulus prediction,total-variation penalization,Predictive models,Signal to noise ratio,Standards,primal-dual optimization,sparse,supervised learning,support recovery,total-variation},
  author = {Gramfort, A. and Thirion, B. and Varoquaux, G.},
  file = {/home/arthur/Zotero/storage/MREN79AJ/6603546.html}
}

@book{chollet_keras_2015,
  title = {Keras},
  url = {http://203.195.193.174/nat123CacheFolder/646F63732E626470742E6E6574/dab609e1e1fc485e93e3755a4c2814abCD30CE37D034D031CA20CE38C532CD3ACA39_e22880a46b0f1f3f3eb1e14dd5452984/media/pdf/kerascn/latest/kerascn.pdf#page=59},
  urldate = {2017-05-14},
  date = {2015},
  author = {Chollet, Francois},
  file = {/home/arthur/Dropbox/Zotero/Chollet_2015_Keras.pdf}
}

@article{abadi_tensorflow_2016,
  title = {Tensorflow: {{Large}}-Scale Machine Learning on Heterogeneous Distributed Systems},
  url = {https://arxiv.org/abs/1603.04467},
  shorttitle = {Tensorflow},
  journaltitle = {arXiv preprint arXiv:1603.04467},
  urldate = {2017-05-14},
  date = {2016},
  author = {Abadi, Martín and Agarwal, Ashish and Barham, Paul and Brevdo, Eugene and Chen, Zhifeng and Citro, Craig and Corrado, Greg S. and Davis, Andy and Dean, Jeffrey and Devin, Matthieu and {others}},
  file = {/home/arthur/Dropbox/Zotero/Abadi et al_2016_Tensorflow.pdf;/home/arthur/Zotero/storage/RHJ4KS2Q/1603.html}
}

@article{papadopoulos_orfanos_brainomics/localizer_2017,
  langid = {english},
  title = {The {{Brainomics}}/{{Localizer}} Database},
  volume = {144},
  issn = {10538119},
  url = {http://linkinghub.elsevier.com/retrieve/pii/S1053811915008745},
  journaltitle = {NeuroImage},
  urldate = {2017-05-14},
  date = {2017-01},
  pages = {309-314},
  author = {Papadopoulos Orfanos, Dimitri and Michel, Vincent and Schwartz, Yannick and Pinel, Philippe and Moreno, Antonio and Le Bihan, Denis and Frouin, Vincent},
  file = {/home/arthur/Dropbox/Zotero/Papadopoulos Orfanos et al_2017_The Brainomics-Localizer database.pdf}
}

@article{pinel_fast_2007,
  title = {Fast Reproducible Identification and Large-Scale Databasing of Individual Functional Cognitive Networks},
  volume = {8},
  issn = {1471-2202},
  url = {http://dx.doi.org/10.1186/1471-2202-8-91},
  abstract = {Although cognitive processes such as reading and calculation are associated with reproducible cerebral networks, inter-individual variability is considerable. Understanding the origins of this variability will require the elaboration of large multimodal databases compiling behavioral, anatomical, genetic and functional neuroimaging data over hundreds of subjects. With this goal in mind, we designed a simple and fast acquisition procedure based on a 5-minute functional magnetic resonance imaging (fMRI) sequence that can be run as easily and as systematically as an anatomical scan, and is therefore used in every subject undergoing fMRI in our laboratory. This protocol captures the cerebral bases of auditory and visual perception, motor actions, reading, language comprehension and mental calculation at an individual level.},
  journaltitle = {BMC Neuroscience},
  shortjournal = {BMC Neuroscience},
  urldate = {2017-05-14},
  date = {2007},
  pages = {91},
  author = {Pinel, Philippe and Thirion, Bertrand and Meriaux, Sébastien and Jobert, Antoinette and Serres, Julien and Le Bihan, Denis and Poline, Jean-Baptiste and Dehaene, Stanislas},
  file = {/home/arthur/Dropbox/Zotero/Pinel et al_2007_Fast reproducible identification and large-scale databasing of individual.pdf;/home/arthur/Zotero/storage/3N8ATTZ6/1471-2202-8-91.html}
}

@article{mallat_matching_1993,
  title = {Matching Pursuits with Time-Frequency Dictionaries},
  volume = {41},
  url = {http://ieeexplore.ieee.org/abstract/document/258082/},
  number = {12},
  journaltitle = {IEEE Transactions on signal processing},
  urldate = {2017-05-15},
  date = {1993},
  pages = {3397--3415},
  author = {Mallat, Stéphane G. and Zhang, Zhifeng},
  file = {/home/arthur/Dropbox/Zotero/Mallat_Zhang_1993_Matching pursuits with time-frequency dictionaries.pdf;/home/arthur/Zotero/storage/8FX3FHJQ/258082.html}
}

@incollection{rubin_generalized_2016,
  title = {Generalized {{Correspondence}}-{{LDA Models}} ({{GC}}-{{LDA}}) for {{Identifying Functional Regions}} in the {{Brain}}},
  url = {http://papers.nips.cc/paper/6274-generalized-correspondence-lda-models-gc-lda-for-identifying-functional-regions-in-the-brain.pdf},
  booktitle = {Advances in {{Neural Information Processing Systems}} 29},
  publisher = {{Curran Associates, Inc.}},
  urldate = {2017-05-15},
  date = {2016},
  pages = {1118--1126},
  author = {Rubin, Timothy and Koyejo, Oluwasanmi O and Jones, Michael N and Yarkoni, Tal},
  editor = {Lee, D. D. and Sugiyama, M. and Luxburg, U. V. and Guyon, I. and Garnett, R.},
  file = {/home/arthur/Dropbox/Zotero/Rubin et al_2016_Generalized Correspondence-LDA Models (GC-LDA) for Identifying Functional.pdf;/home/arthur/Zotero/storage/I78D5QIW/6274-generalized-correspondence-lda-models-gc-lda-for-identifying-functional-regions-in-the-bra.html}
}

@article{bzdok_inference_2017,
  title = {Inference in the Age of Big Data: {{Future}} Perspectives on Neuroscience},
  issn = {1053-8119},
  url = {http://www.sciencedirect.com/science/article/pii/S1053811917303816},
  shorttitle = {Inference in the Age of Big Data},
  abstract = {Neuroscience is undergoing faster changes than ever before. Over 100 years our field qualitatively described and invasively manipulated single or few organisms to gain anatomical, physiological, and pharmacological insights. In the last 10 years neuroscience spawned quantitative datasets of unprecedented breadth (e.g., microanatomy, synaptic connections, and optogenetic brain-behavior assays) and size (e.g., cognition, brain imaging, and genetics). While growing data availability and information granularity have been amply discussed, we direct attention to a less explored question: How will the unprecedented data richness shape data analysis practices? Statistical reasoning is becoming more important to distill neurobiological knowledge from healthy and pathological brain measurements. We argue that large-scale data analysis will use more statistical models that are non-parametric, generative, and mixing frequentist and Bayesian aspects, while supplementing classical hypothesis testing with out-of-sample predictions.},
  journaltitle = {NeuroImage},
  shortjournal = {NeuroImage},
  urldate = {2017-05-15},
  date = {2017},
  keywords = {machine learning,Systems biology,Epistemology,Hypothesis testing,High-dimensional statistics,Sample complexity},
  author = {Bzdok, Danilo and Yeo, B. T. Thomas},
  file = {/home/arthur/Dropbox/Zotero/Bzdok_Yeo_2017_Inference in the age of big data.pdf;/home/arthur/Zotero/storage/GRCX6UI7/S1053811917303816.html}
}

@article{laird_brainmap_2005,
  title = {Brainmap},
  volume = {3},
  url = {http://link.springer.com/article/10.1385/NI:3:1:065},
  number = {1},
  journaltitle = {Neuroinformatics},
  urldate = {2017-05-16},
  date = {2005},
  pages = {65--77},
  author = {Laird, Angela R. and Lancaster, Jack J. and Fox, Peter T.},
  file = {/home/arthur/Dropbox/Zotero/Laird et al_2005_Brainmap.pdf;/home/arthur/Zotero/storage/XGR3DPD5/NI31065.html}
}

@article{hinton_reducing_2006,
  title = {Reducing the Dimensionality of Data with Neural Networks},
  volume = {313},
  url = {http://science.sciencemag.org/content/313/5786/504.short},
  number = {5786},
  journaltitle = {science},
  urldate = {2017-05-16},
  date = {2006},
  pages = {504--507},
  author = {Hinton, Geoffrey E. and Salakhutdinov, Ruslan R.},
  file = {/home/arthur/Dropbox/Zotero/Hinton_Salakhutdinov_2006_Reducing the dimensionality of data with neural networks.pdf;/home/arthur/Zotero/storage/NQ42DIRE/504.html}
}

@inproceedings{donahue_decaf_2014,
  title = {{{DeCAF}}: {{A Deep Convolutional Activation Feature}} for {{Generic Visual Recognition}}.},
  volume = {32},
  url = {http://www.jmlr.org/proceedings/papers/v32/donahue14.pdf},
  shorttitle = {{{DeCAF}}},
  booktitle = {Icml},
  urldate = {2017-05-16},
  date = {2014},
  pages = {647--655},
  author = {Donahue, Jeff and Jia, Yangqing and Vinyals, Oriol and Hoffman, Judy and Zhang, Ning and Tzeng, Eric and Darrell, Trevor},
  file = {/home/arthur/Dropbox/Zotero/Donahue et al_2014_DeCAF.pdf}
}

@article{eickhoff_connectivity-based_2015,
  title = {Connectivity-Based Parcellation: {{Critique}} and Implications},
  volume = {36},
  url = {http://onlinelibrary.wiley.com/doi/10.1002/hbm.22933/full},
  shorttitle = {Connectivity-Based Parcellation},
  number = {12},
  journaltitle = {Human brain mapping},
  urldate = {2017-05-17},
  date = {2015},
  pages = {4771--4792},
  author = {Eickhoff, Simon B. and Thirion, Bertrand and Varoquaux, Gaël and Bzdok, Danilo},
  file = {/home/arthur/Dropbox/Zotero/Eickhoff et al_2015_Connectivity-based parcellation.pdf;/home/arthur/Zotero/storage/QFIWEW8P/full.html}
}

@article{bugden_role_2012,
  langid = {english},
  title = {The Role of the Left Intraparietal Sulcus in the Relationship between Symbolic Number Processing and Children's Arithmetic Competence},
  volume = {2},
  issn = {18789293},
  url = {http://linkinghub.elsevier.com/retrieve/pii/S1878929312000412},
  number = {4},
  journaltitle = {Developmental Cognitive Neuroscience},
  urldate = {2017-05-17},
  date = {2012-10},
  pages = {448-457},
  author = {Bugden, Stephanie and Price, Gavin R. and McLean, D. Adam and Ansari, Daniel},
  file = {/home/arthur/Dropbox/Zotero/Bugden et al_2012_The role of the left intraparietal sulcus in the relationship between symbolic.pdf}
}

@article{blumensath_spatially_2013,
  langid = {english},
  title = {Spatially Constrained Hierarchical Parcellation of the Brain with Resting-State {{fMRI}}},
  volume = {76},
  issn = {10538119},
  url = {http://linkinghub.elsevier.com/retrieve/pii/S1053811913002668},
  journaltitle = {NeuroImage},
  urldate = {2017-05-18},
  date = {2013-08},
  pages = {313-324},
  author = {Blumensath, Thomas and Jbabdi, Saad and Glasser, Matthew F. and Van Essen, David C. and Ugurbil, Kamil and Behrens, Timothy E.J. and Smith, Stephen M.},
  file = {/home/arthur/Dropbox/Zotero/Blumensath et al_2013_Spatially constrained hierarchical parcellation of the brain with resting-state.pdf}
}

@article{chang_adaptive_2000,
  title = {Adaptive Wavelet Thresholding for Image Denoising and Compression},
  volume = {9},
  url = {http://ieeexplore.ieee.org/abstract/document/862633/},
  number = {9},
  journaltitle = {IEEE Transactions on image processing},
  urldate = {2017-05-22},
  date = {2000},
  pages = {1532--1546},
  author = {Chang, S. Grace and Yu, Bin and Vetterli, Martin},
  file = {/home/arthur/Dropbox/Zotero/Chang et al_2000_Adaptive wavelet thresholding for image denoising and compression.pdf}
}

@article{arjovsky_wasserstein_2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1701.07875},
  primaryClass = {cs, stat},
  title = {Wasserstein {{GAN}}},
  url = {http://arxiv.org/abs/1701.07875},
  abstract = {We introduce a new algorithm named WGAN, an alternative to traditional GAN training. In this new model, we show that we can improve the stability of learning, get rid of problems like mode collapse, and provide meaningful learning curves useful for debugging and hyperparameter searches. Furthermore, we show that the corresponding optimization problem is sound, and provide extensive theoretical work highlighting the deep connections to other distances between distributions.},
  urldate = {2017-05-23},
  date = {2017-01-26},
  keywords = {Computer Science - Learning,Statistics - Machine Learning},
  author = {Arjovsky, Martin and Chintala, Soumith and Bottou, Léon},
  file = {/home/arthur/Dropbox/Zotero/Arjovsky et al_2017_Wasserstein GAN.pdf;/home/arthur/Zotero/storage/CK948IT9/1701.html}
}

@book{mallat_wavelet_2009,
  location = {{Amsterdam ; Boston}},
  title = {A Wavelet Tour of Signal Processing: The Sparse Way},
  edition = {3rd ed},
  isbn = {978-0-12-374370-1},
  shorttitle = {A Wavelet Tour of Signal Processing},
  pagetotal = {805},
  publisher = {{Elsevier/Academic Press}},
  date = {2009},
  keywords = {signal processing,Mathematics,Wavelets (Mathematics)},
  author = {Mallat, S. G.},
  file = {/home/arthur/Dropbox/Zotero/Mallat_2009_A wavelet tour of signal processing.pdf}
}

@article{wilson_marginal_2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1705.08292},
  primaryClass = {cs, stat},
  title = {The {{Marginal Value}} of {{Adaptive Gradient Methods}} in {{Machine Learning}}},
  url = {http://arxiv.org/abs/1705.08292},
  abstract = {Adaptive optimization methods, which perform local optimization with a metric constructed from the history of iterates, are becoming increasingly popular for training deep neural networks. Examples include AdaGrad, RMSProp, and Adam. We show that for simple overparameterized problems, adaptive methods often find drastically different solutions than gradient descent (GD) or stochastic gradient descent (SGD). We construct an illustrative binary classification problem where the data is linearly separable, GD and SGD achieve zero test error, and AdaGrad, Adam, and RMSProp attain test errors arbitrarily close to half. We additionally study the empirical generalization capability of adaptive methods on several state-of-the-art deep learning models. We observe that the solutions found by adaptive methods generalize worse (often significantly worse) than SGD, even when these solutions have better training performance. These results suggest that practitioners should reconsider the use of adaptive methods to train neural networks.},
  urldate = {2017-05-24},
  date = {2017-05-23},
  keywords = {Computer Science - Learning,Statistics - Machine Learning},
  author = {Wilson, Ashia C. and Roelofs, Rebecca and Stern, Mitchell and Srebro, Nathan and Recht, Benjamin},
  file = {/home/arthur/Dropbox/Zotero/Wilson et al_2017_The Marginal Value of Adaptive Gradient Methods in Machine Learning.pdf;/home/arthur/Zotero/storage/BR4TKKFC/1705.html}
}

@article{udell_generalized_2016,
  title = {Generalized Low Rank Models},
  volume = {9},
  url = {http://ftp.nowpublishers.com/article/Details/MAL-055},
  number = {1},
  journaltitle = {Foundations and Trends® in Machine Learning},
  urldate = {2017-05-31},
  date = {2016},
  pages = {1--118},
  author = {Udell, Madeleine and Horn, Corinne and Zadeh, Reza and Boyd, Stephen and {others}},
  file = {/home/arthur/Dropbox/Zotero/Udell et al_2016_Generalized low rank models.pdf;/home/arthur/Zotero/storage/Q79GW49X/MAL-055.html}
}

@article{yang_deep_2016,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1605.06391},
  primaryClass = {cs},
  title = {Deep {{Multi}}-Task {{Representation Learning}}: {{A Tensor Factorisation Approach}}},
  url = {http://arxiv.org/abs/1605.06391},
  shorttitle = {Deep {{Multi}}-Task {{Representation Learning}}},
  abstract = {Most contemporary multi-task learning methods assume linear models. This setting is considered shallow in the era of deep learning. In this paper, we present a new deep multi-task representation learning framework that learns cross-task sharing structure at every layer in a deep network. Our approach is based on generalising the matrix factorisation techniques explicitly or implicitly used by many conventional MTL algorithms to tensor factorisation, to realise automatic learning of end-to-end knowledge sharing in deep networks. This is in contrast to existing deep learning approaches that need a user-defined multi-task sharing strategy. Our approach applies to both homogeneous and heterogeneous MTL. Experiments demonstrate the efficacy of our deep multi-task representation learning in terms of both higher accuracy and fewer design choices.},
  urldate = {2017-05-31},
  date = {2016-05-20},
  keywords = {Computer Science - Learning},
  author = {Yang, Yongxin and Hospedales, Timothy},
  file = {/home/arthur/Dropbox/Zotero/Yang_Hospedales_2016_Deep Multi-task Representation Learning.pdf;/home/arthur/Zotero/storage/KBUP4CDC/1605.html}
}

@article{kumar_learning_2012,
  title = {Learning Task Grouping and Overlap in Multi-Task Learning},
  url = {https://arxiv.org/abs/1206.6417},
  journaltitle = {arXiv preprint arXiv:1206.6417},
  urldate = {2017-05-31},
  date = {2012},
  author = {Kumar, Abhishek and Daume III, Hal},
  file = {/home/arthur/Dropbox/Zotero/Kumar_Daume III_2012_Learning task grouping and overlap in multi-task learning.pdf;/home/arthur/Zotero/storage/86965EVR/1206.html}
}

@thesis{caruana_multitask_1997,
  title = {Multitask {{Learning}}},
  url = {http://www8.cs.umu.se/research/ifor/dl/LEARNING/multitask%20learning.pdf},
  urldate = {2017-06-01},
  date = {1997},
  author = {Caruana, Rich},
  file = {/home/arthur/Dropbox/Zotero/Caruana_1997_Multitask Learning.pdf}
}

@article{lipton_mythos_2016,
  title = {The Mythos of Model Interpretability},
  url = {https://arxiv.org/abs/1606.03490},
  journaltitle = {arXiv preprint arXiv:1606.03490},
  urldate = {2017-06-01},
  date = {2016},
  author = {Lipton, Zachary C.},
  file = {/home/arthur/Dropbox/Zotero/Lipton_2016_The mythos of model interpretability.pdf;/home/arthur/Zotero/storage/9X5ITTAM/1606.html}
}

@article{ribeiro_why_2016,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1602.04938},
  primaryClass = {cs, stat},
  title = {"{{Why Should I Trust You}}?": {{Explaining}} the {{Predictions}} of {{Any Classifier}}},
  url = {http://arxiv.org/abs/1602.04938},
  shorttitle = {"{{Why Should I Trust You}}?},
  abstract = {Despite widespread adoption, machine learning models remain mostly black boxes. Understanding the reasons behind predictions is, however, quite important in assessing trust, which is fundamental if one plans to take action based on a prediction, or when choosing whether to deploy a new model. Such understanding also provides insights into the model, which can be used to transform an untrustworthy model or prediction into a trustworthy one. In this work, we propose LIME, a novel explanation technique that explains the predictions of any classifier in an interpretable and faithful manner, by learning an interpretable model locally around the prediction. We also propose a method to explain models by presenting representative individual predictions and their explanations in a non-redundant way, framing the task as a submodular optimization problem. We demonstrate the flexibility of these methods by explaining different models for text (e.g. random forests) and image classification (e.g. neural networks). We show the utility of explanations via novel experiments, both simulated and with human subjects, on various scenarios that require trust: deciding if one should trust a prediction, choosing between models, improving an untrustworthy classifier, and identifying why a classifier should not be trusted.},
  urldate = {2017-06-02},
  date = {2016-02-16},
  keywords = {Computer Science - Learning,Statistics - Machine Learning,Computer Science - Artificial Intelligence},
  author = {Ribeiro, Marco Tulio and Singh, Sameer and Guestrin, Carlos},
  file = {/home/arthur/Dropbox/Zotero/Ribeiro et al_2016_Why Should I Trust You.pdf;/home/arthur/Zotero/storage/EG68KKDX/1602.html}
}

@article{chi_petrels_2013,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1207.6353},
  title = {{{PETRELS}}: {{Parallel Subspace Estimation}} and {{Tracking}} by {{Recursive Least Squares}} from {{Partial Observations}}},
  volume = {61},
  issn = {1053-587X, 1941-0476},
  url = {http://arxiv.org/abs/1207.6353},
  shorttitle = {{{PETRELS}}},
  abstract = {Many real world data sets exhibit an embedding of low-dimensional structure in a high-dimensional manifold. Examples include images, videos and internet traffic data. It is of great significance to reduce the storage requirements and computational complexity when the data dimension is high. Therefore we consider the problem of reconstructing a data stream from a small subset of its entries, where the data is assumed to lie in a low-dimensional linear subspace, possibly corrupted by noise. We further consider tracking the change of the underlying subspace, which can be applied to applications such as video denoising, network monitoring and anomaly detection. Our problem can be viewed as a sequential low-rank matrix completion problem in which the subspace is learned in an on-line fashion. The proposed algorithm, dubbed Parallel Estimation and Tracking by REcursive Least Squares (PETRELS), first identifies the underlying low-dimensional subspace via a recursive procedure for each row of the subspace matrix in parallel with discounting for previous observations, and then reconstructs the missing entries via least-squares estimation if required. Numerical examples are provided for direction-of-arrival estimation and matrix completion, comparing PETRELS with state of the art batch algorithms.},
  number = {23},
  journaltitle = {IEEE Transactions on Signal Processing},
  urldate = {2017-06-13},
  date = {2013-12},
  pages = {5947-5959},
  keywords = {Statistics - Methodology,Computer Science - Information Theory},
  author = {Chi, Yuejie and Eldar, Yonina C. and Calderbank, Robert},
  file = {/home/arthur/Dropbox/Zotero/Chi et al_2013_PETRELS.pdf;/home/arthur/Zotero/storage/JKMTHGNP/1207.html}
}

@article{mardani_subspace_2015,
  title = {Subspace {{Learning}} and {{Imputation}} for {{Streaming Big Data Matrices}} and {{Tensors}}},
  volume = {63},
  issn = {1053-587X, 1941-0476},
  url = {http://ieeexplore.ieee.org/document/7072498/},
  number = {10},
  journaltitle = {IEEE Transactions on Signal Processing},
  urldate = {2017-06-13},
  date = {2015-05},
  pages = {2663-2677},
  author = {Mardani, Morteza and Mateos, Gonzalo and Giannakis, Georgios B.},
  file = {/home/arthur/Dropbox/Zotero/Mardani et al_2015_Subspace Learning and Imputation for Streaming Big Data Matrices and Tensors.pdf}
}

@article{klambauer_self-normalizing_2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1706.02515},
  primaryClass = {cs, stat},
  title = {Self-{{Normalizing Neural Networks}}},
  url = {http://arxiv.org/abs/1706.02515},
  abstract = {Deep Learning has revolutionized vision via convolutional neural networks (CNNs) and natural language processing via recurrent neural networks (RNNs). However, success stories of Deep Learning with standard feed-forward neural networks (FNNs) are rare. FNNs that perform well are typically shallow and, therefore cannot exploit many levels of abstract representations. We introduce self-normalizing neural networks (SNNs) to enable high-level abstract representations. While batch normalization requires explicit normalization, neuron activations of SNNs automatically converge towards zero mean and unit variance. The activation function of SNNs are "scaled exponential linear units" (SELUs), which induce self-normalizing properties. Using the Banach fixed-point theorem, we prove that activations close to zero mean and unit variance that are propagated through many network layers will converge towards zero mean and unit variance -- even under the presence of noise and perturbations. This convergence property of SNNs allows to (1) train deep networks with many layers, (2) employ strong regularization, and (3) to make learning highly robust. Furthermore, for activations not close to unit variance, we prove an upper and lower bound on the variance, thus, vanishing and exploding gradients are impossible. We compared SNNs on (a) 121 tasks from the UCI machine learning repository, on (b) drug discovery benchmarks, and on (c) astronomy tasks with standard FNNs and other machine learning methods such as random forests and support vector machines. SNNs significantly outperformed all competing FNN methods at 121 UCI tasks, outperformed all competing methods at the Tox21 dataset, and set a new record at an astronomy data set. The winning SNN architectures are often very deep. Implementations are available at: github.com/bioinf-jku/SNNs.},
  urldate = {2017-06-14},
  date = {2017-06-08},
  keywords = {Computer Science - Learning,Statistics - Machine Learning},
  author = {Klambauer, Günter and Unterthiner, Thomas and Mayr, Andreas and Hochreiter, Sepp},
  file = {/home/arthur/Dropbox/Zotero/Klambauer et al_2017_Self-Normalizing Neural Networks.pdf;/home/arthur/Zotero/storage/U8VB7ZZZ/1706.html}
}

@article{dhingra_gated-attention_2016,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1606.01549},
  primaryClass = {cs},
  title = {Gated-{{Attention Readers}} for {{Text Comprehension}}},
  url = {http://arxiv.org/abs/1606.01549},
  abstract = {In this paper we study the problem of answering cloze-style questions over documents. Our model, the Gated-Attention (GA) Reader, integrates a multi-hop architecture with a novel attention mechanism, which is based on multiplicative interactions between the query embedding and the intermediate states of a recurrent neural network document reader. This enables the reader to build query-specific representations of tokens in the document for accurate answer selection. The GA Reader obtains state-of-the-art results on three benchmarks for this task--the CNN $\backslash$\& Daily Mail news stories and the Who Did What dataset. The effectiveness of multiplicative interaction is demonstrated by an ablation study, and by comparing to alternative compositional operators for implementing the gated-attention. The code is available at https://github.com/bdhingra/ga-reader.},
  urldate = {2017-06-22},
  date = {2016-06-05},
  keywords = {Computer Science - Learning,Computer Science - Computation and Language},
  author = {Dhingra, Bhuwan and Liu, Hanxiao and Yang, Zhilin and Cohen, William W. and Salakhutdinov, Ruslan},
  file = {/home/arthur/Dropbox/Zotero/Dhingra et al_2016_Gated-Attention Readers for Text Comprehension.pdf;/home/arthur/Zotero/storage/ZVR8P4SZ/1606.html}
}

@article{degraux_online_2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1706.04256},
  primaryClass = {cs},
  title = {Online {{Convolutional Dictionary Learning}} for {{Multimodal Imaging}}},
  url = {http://arxiv.org/abs/1706.04256},
  abstract = {Computational imaging methods that can exploit multiple modalities have the potential to enhance the capabilities of traditional sensing systems. In this paper, we propose a new method that reconstructs multimodal images from their linear measurements by exploiting redundancies across different modalities. Our method combines a convolutional group-sparse representation of images with total variation (TV) regularization for high-quality multimodal imaging. We develop an online algorithm that enables the unsupervised learning of convolutional dictionaries on large-scale datasets that are typical in such applications. We illustrate the benefit of our approach in the context of joint intensity-depth imaging.},
  urldate = {2017-06-22},
  date = {2017-06-13},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  author = {Degraux, Kevin and Kamilov, Ulugbek S. and Boufounos, Petros T. and Liu, Dehong},
  file = {/home/arthur/Dropbox/Zotero/Degraux et al_2017_Online Convolutional Dictionary Learning for Multimodal Imaging.pdf;/home/arthur/Zotero/storage/3GXN48BE/1706.html}
}

@article{hardt_train_2015,
  title = {Train Faster, Generalize Better: {{Stability}} of Stochastic Gradient Descent},
  url = {https://arxiv.org/abs/1509.01240},
  shorttitle = {Train Faster, Generalize Better},
  journaltitle = {arXiv preprint arXiv:1509.01240},
  urldate = {2017-06-27},
  date = {2015},
  author = {Hardt, Moritz and Recht, Benjamin and Singer, Yoram},
  file = {/home/arthur/Dropbox/Zotero/Hardt et al_2015_Train faster, generalize better.pdf;/home/arthur/Zotero/storage/PA88UQTS/1509.html}
}

@article{jin_how_2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1703.00887},
  primaryClass = {cs, math, stat},
  title = {How to {{Escape Saddle Points Efficiently}}},
  url = {http://arxiv.org/abs/1703.00887},
  abstract = {This paper shows that a perturbed form of gradient descent converges to a second-order stationary point in a number iterations which depends only poly-logarithmically on dimension (i.e., it is almost "dimension-free"). The convergence rate of this procedure matches the well-known convergence rate of gradient descent to first-order stationary points, up to log factors. When all saddle points are non-degenerate, all second-order stationary points are local minima, and our result thus shows that perturbed gradient descent can escape saddle points almost for free. Our results can be directly applied to many machine learning applications, including deep learning. As a particular concrete example of such an application, we show that our results can be used directly to establish sharp global convergence rates for matrix factorization. Our results rely on a novel characterization of the geometry around saddle points, which may be of independent interest to the non-convex optimization community.},
  urldate = {2017-06-29},
  date = {2017-03-02},
  keywords = {Computer Science - Learning,Mathematics - Optimization and Control,Statistics - Machine Learning},
  author = {Jin, Chi and Ge, Rong and Netrapalli, Praneeth and Kakade, Sham M. and Jordan, Michael I.},
  file = {/home/arthur/Dropbox/Zotero/Jin et al_2017_How to Escape Saddle Points Efficiently.pdf;/home/arthur/Zotero/storage/S2XHT7JZ/1703.html}
}

@article{du_gradient_2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1705.10412},
  primaryClass = {cs, math, stat},
  title = {Gradient {{Descent Can Take Exponential Time}} to {{Escape Saddle Points}}},
  url = {http://arxiv.org/abs/1705.10412},
  abstract = {Although gradient descent (GD) almost always escapes saddle points asymptotically [Lee et al., 2016], this paper shows that even with fairly natural random initialization schemes and non-pathological functions, GD can be significantly slowed down by saddle points, taking exponential time to escape. On the other hand, gradient descent with perturbations [Ge et al., 2015, Jin et al., 2017] is not slowed down by saddle points - it can find an approximate local minimizer in polynomial time. This result implies that GD is inherently slower than perturbed GD, and justifies the importance of adding perturbations for efficient non-convex optimization. While our focus is theoretical, we also present experiments that illustrate our theoretical findings.},
  urldate = {2017-06-29},
  date = {2017-05-29},
  keywords = {Computer Science - Learning,Mathematics - Optimization and Control,Statistics - Machine Learning},
  author = {Du, Simon S. and Jin, Chi and Lee, Jason D. and Jordan, Michael I. and Poczos, Barnabas and Singh, Aarti},
  file = {/home/arthur/Dropbox/Zotero/Du et al_2017_Gradient Descent Can Take Exponential Time to Escape Saddle Points.pdf;/home/arthur/Zotero/storage/2GESAVXZ/1705.html}
}

@article{lei_less_2016,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1609.03261},
  primaryClass = {cs, math, stat},
  title = {Less than a {{Single Pass}}: {{Stochastically Controlled Stochastic Gradient Method}}},
  url = {http://arxiv.org/abs/1609.03261},
  shorttitle = {Less than a {{Single Pass}}},
  abstract = {We develop and analyze a procedure for gradient-based optimization that we refer to as stochastically controlled stochastic gradient (SCSG). As a member of the SVRG family of algorithms, SCSG makes use of gradient estimates at two scales. Unlike most existing algorithms in this family, both the computation cost and the communication cost of SCSG do not necessarily scale linearly with the sample size n; indeed, these costs are independent of n when the target accuracy is low. An experimental evaluation of SCSG on the MNIST dataset shows that it can yield accurate results on this dataset on a single commodity machine with a memory footprint of only 2.6MB and only eight disk accesses.},
  urldate = {2017-06-29},
  date = {2016-09-11},
  keywords = {Computer Science - Learning,Mathematics - Optimization and Control,Statistics - Machine Learning,Computer Science - Data Structures and Algorithms},
  author = {Lei, Lihua and Jordan, Michael I.},
  file = {/home/arthur/Dropbox/Zotero/Lei_Jordan_2016_Less than a Single Pass.pdf;/home/arthur/Zotero/storage/IJ3ZT7AP/1609.html}
}

@article{jaderberg_reinforcement_2016,
  title = {Reinforcement Learning with Unsupervised Auxiliary Tasks},
  url = {https://arxiv.org/abs/1611.05397},
  journaltitle = {arXiv preprint arXiv:1611.05397},
  urldate = {2017-06-30},
  date = {2016},
  author = {Jaderberg, Max and Mnih, Volodymyr and Czarnecki, Wojciech Marian and Schaul, Tom and Leibo, Joel Z. and Silver, David and Kavukcuoglu, Koray},
  file = {/home/arthur/Dropbox/Zotero/Jaderberg et al_2016_Reinforcement learning with unsupervised auxiliary tasks.pdf}
}

@book{shalev-shwartz_understanding_2014,
  location = {{New York, NY, USA}},
  title = {Understanding Machine Learning: From Theory to Algorithms},
  isbn = {978-1-107-05713-5},
  shorttitle = {Understanding Machine Learning},
  abstract = {"Machine learning is one of the fastest growing areas of computer science, with far-reaching applications. The aim of this textbook is to introduce machine learning, and the algorithmic paradigms it offers, in a principled way. The book provides an extensive theoretical account of the fundamental ideas underlying machine learning and the mathematical derivations that transform these principles into practical algorithms. Following a presentation of the basics of the field, the book covers a wide array of central topics that have not been addressed by previous textbooks. These include a discussion of the computational complexity of learning and the concepts of convexity and stability; important algorithmic paradigms including stochastic gradient descent, neural networks, and structured output learning; and emerging theoretical concepts such as the PAC-Bayes approach and compression-based bounds. Designed for an advanced undergraduate or beginning graduate course, the text makes the fundamentals and algorithms of machine learning accessible to students and non-expert readers in statistics, computer science, mathematics, and engineering"--},
  pagetotal = {397},
  publisher = {{Cambridge University Press}},
  date = {2014},
  keywords = {Algorithms,machine learning,COMPUTERS / Computer Vision & Pattern Recognition},
  author = {Shalev-Shwartz, Shai and Ben-David, Shai},
  file = {/home/arthur/Dropbox/Zotero/Shalev-Shwartz_Ben-David_2014_Understanding machine learning.pdf}
}

@book{boucheron_concentration_2013,
  location = {{Oxford}},
  title = {Concentration Inequalities: A Nonasymptotic Theory of Independence},
  edition = {1st ed},
  isbn = {978-0-19-953525-5},
  shorttitle = {Concentration Inequalities},
  pagetotal = {481},
  publisher = {{Oxford University Press}},
  date = {2013},
  keywords = {Probabilities,Inequalities (Mathematics)},
  author = {Boucheron, Stéphane and Lugosi, Gábor and Massart, Pascal},
  file = {/home/arthur/Dropbox/Zotero/Boucheron et al_2013_Concentration inequalities.pdf},
  note = {OCLC: ocn818449985}
}

@article{scieur_nonlinear_2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1706.07270},
  primaryClass = {math},
  title = {Nonlinear {{Acceleration}} of {{Stochastic Algorithms}}},
  url = {http://arxiv.org/abs/1706.07270},
  abstract = {Extrapolation methods use the last few iterates of an optimization algorithm to produce a better estimate of the optimum. They were shown to achieve optimal convergence rates in a deterministic setting using simple gradient iterates. Here, we study extrapolation methods in a stochastic setting, where the iterates are produced by either a simple or an accelerated stochastic gradient algorithm. We first derive convergence bounds for arbitrary, potentially biased perturbations, then produce asymptotic bounds using the ratio between the variance of the noise and the accuracy of the current point. Finally, we apply this acceleration technique to stochastic algorithms such as SGD, SAGA, SVRG and Katyusha in different settings, and show significant performance gains.},
  urldate = {2017-06-30},
  date = {2017-06-22},
  keywords = {Mathematics - Optimization and Control},
  author = {Scieur, Damien and d' Aspremont, Alexandre and Bach, Francis},
  options = {useprefix=true},
  file = {/home/arthur/Dropbox/Zotero/Scieur et al_2017_Nonlinear Acceleration of Stochastic Algorithms.pdf;/home/arthur/Zotero/storage/ISJUQEMT/1706.html}
}

@article{bietti_group_2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1706.03078},
  primaryClass = {cs, stat},
  title = {Group {{Invariance}} and {{Stability}} to {{Deformations}} of {{Deep Convolutional Representations}}},
  url = {http://arxiv.org/abs/1706.03078},
  abstract = {In this paper, we study deep signal representations that are invariant to groups of transformations and stable to the action of diffeomorphisms without losing signal information. This is achieved by generalizing the multilayer kernel introduced in the context of convolutional kernel networks and by studying the geometry of the corresponding reproducing kernel Hilbert space. We show that the signal representation is stable, and that models from this functional space, such as a large class of convolutional neural networks with homogeneous activation functions, may enjoy the same stability.},
  urldate = {2017-06-30},
  date = {2017-06-09},
  keywords = {Computer Science - Learning,Statistics - Machine Learning},
  author = {Bietti, Alberto and Mairal, Julien},
  file = {/home/arthur/Dropbox/Zotero/Bietti_Mairal_2017_Group Invariance and Stability to Deformations of Deep Convolutional.pdf;/home/arthur/Zotero/storage/GTQ85SXD/1706.html}
}

@article{lin_generic_2016,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1610.00960},
  primaryClass = {math, stat},
  title = {A {{Generic Quasi}}-{{Newton Algorithm}} for {{Faster Gradient}}-{{Based Optimization}}},
  url = {http://arxiv.org/abs/1610.00960},
  abstract = {We propose a generic approach to accelerate gradient-based optimization algorithms with quasi-Newton principles. The proposed scheme, called QuickeNing, can be applied to incremental first-order methods such as stochastic variance-reduced gradient (SVRG) or incremental surrogate optimization (MISO). It is also compatible with composite objectives, meaning that it has the ability to provide exactly sparse solutions when the objective involves a sparsity-inducing regularization. QuickeNing relies on limited-memory BFGS rules, making it appropriate for solving high-dimensional optimization problems. Besides, it enjoys a worst-case linear convergence rate for strongly convex problems. We present experimental results where QuickeNing gives significant improvements over competing methods for solving large-scale high-dimensional machine learning problems.},
  urldate = {2017-06-30},
  date = {2016-10-04},
  keywords = {Mathematics - Optimization and Control,Statistics - Machine Learning},
  author = {Lin, Hongzhou and Mairal, Julien and Harchaoui, Zaid},
  file = {/home/arthur/Dropbox/Zotero/Lin et al_2016_A Generic Quasi-Newton Algorithm for Faster Gradient-Based Optimization.pdf;/home/arthur/Zotero/storage/KHGAFRRK/1610.html}
}

@online{akalin_why_2016,
  title = {Why Is the {{Quintic Unsolvable}}?},
  url = {https://www.akalin.com/quintic-unsolvability},
  urldate = {2017-07-03},
  date = {2016},
  author = {Akalin, Fred},
  file = {/home/arthur/Zotero/storage/ZPH4WWFI/quintic-unsolvability.html}
}

@article{liu_limited_1989,
  langid = {english},
  title = {On the Limited Memory {{BFGS}} Method for Large Scale Optimization},
  volume = {45},
  issn = {0025-5610, 1436-4646},
  url = {https://link.springer.com/article/10.1007/BF01589116},
  abstract = {We study the numerical performance of a limited memory quasi-Newton method for large scale optimization, which we call the L-BFGS method. We compare its performance with that of the method developed by Buckley and LeNir (1985), which combines cycles of BFGS steps and conjugate direction steps. Our numerical tests indicate that the L-BFGS method is faster than the method of Buckley and LeNir, and is better able to use additional storage to accelerate convergence. We show that the L-BFGS method can be greatly accelerated by means of a simple scaling. We then compare the L-BFGS method with the partitioned quasi-Newton method of Griewank and Toint (1982a). The results show that, for some problems, the partitioned quasi-Newton method is clearly superior to the L-BFGS method. However we find that for other problems the L-BFGS method is very competitive due to its low iteration cost. We also study the convergence properties of the L-BFGS method, and prove global convergence on uniformly convex problems.},
  number = {1-3},
  journaltitle = {Mathematical Programming},
  shortjournal = {Mathematical Programming},
  urldate = {2017-07-04},
  date = {1989-08-01},
  pages = {503-528},
  author = {Liu, Dong C. and Nocedal, Jorge},
  file = {/home/arthur/Dropbox/Zotero/Liu_Nocedal_1989_On the limited memory BFGS method for large scale optimization.pdf;/home/arthur/Zotero/storage/9DHJDQIW/BF01589116.html}
}

@article{zhang_yellowfin_2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1706.03471},
  primaryClass = {cs, stat},
  title = {{{YellowFin}} and the {{Art}} of {{Momentum Tuning}}},
  url = {http://arxiv.org/abs/1706.03471},
  abstract = {Hyperparameter tuning is one of the big costs of deep learning. State-of-the-art optimizers, such as Adagrad, RMSProp and Adam, make things easier by adaptively tuning an individual learning rate for each variable. This level of fine adaptation is understood to yield a more powerful method. However, our experiments, as well as recent theory by Wilson et al., show that hand-tuned stochastic gradient descent (SGD) achieves better results, at the same rate or faster. The hypothesis put forth is that adaptive methods converge to different minima (Wilson et al.). Here we point out another factor: none of these methods tune their momentum parameter, known to be very important for deep learning applications (Sutskever et al.). Tuning the momentum parameter becomes even more important in asynchronous-parallel systems: recent theory (Mitliagkas et al.) shows that asynchrony introduces momentum-like dynamics, and that tuning down algorithmic momentum is important for efficient parallelization. We revisit the simple momentum SGD algorithm and show that hand-tuning a single learning rate and momentum value makes it competitive with Adam. We then analyze its robustness in learning rate misspecification and objective curvature variation. Based on these insights, we design YellowFin, an automatic tuner for both momentum and learning rate in SGD. YellowFin optionally uses a novel momentum-sensing component along with a negative-feedback loop mechanism to compensate for the added dynamics of asynchrony on the fly. We empirically show YellowFin converges in fewer iterations than Adam on large ResNet and LSTM models, with a speedup up to \$2.8\$x in synchronous and \$2.7\$x in asynchronous settings.},
  urldate = {2017-07-04},
  date = {2017-06-12},
  keywords = {Statistics - Machine Learning,Computer Science - Artificial Intelligence},
  author = {Zhang, Jian and Mitliagkas, Ioannis and Ré, Christopher},
  file = {/home/arthur/Dropbox/Zotero/Zhang et al_2017_YellowFin and the Art of Momentum Tuning.pdf;/home/arthur/Zotero/storage/GMUHZJTQ/1706.html}
}

@article{tran_deep_2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1701.03757},
  primaryClass = {cs, stat},
  title = {Deep {{Probabilistic Programming}}},
  url = {http://arxiv.org/abs/1701.03757},
  abstract = {We propose Edward, a Turing-complete probabilistic programming language. Edward defines two compositional representations---random variables and inference. By treating inference as a first class citizen, on a par with modeling, we show that probabilistic programming can be as flexible and computationally efficient as traditional deep learning. For flexibility, Edward makes it easy to fit the same model using a variety of composable inference methods, ranging from point estimation to variational inference to MCMC. In addition, Edward can reuse the modeling representation as part of inference, facilitating the design of rich variational models and generative adversarial networks. For efficiency, Edward is integrated into TensorFlow, providing significant speedups over existing probabilistic systems. For example, we show on a benchmark logistic regression task that Edward is at least 35x faster than Stan and 6x faster than PyMC3. Further, Edward incurs no runtime overhead: it is as fast as handwritten TensorFlow.},
  urldate = {2017-07-06},
  date = {2017-01-13},
  keywords = {Computer Science - Learning,Statistics - Computation,Statistics - Machine Learning,Computer Science - Artificial Intelligence,Computer Science - Programming Languages},
  author = {Tran, Dustin and Hoffman, Matthew D. and Saurous, Rif A. and Brevdo, Eugene and Murphy, Kevin and Blei, David M.},
  file = {/home/arthur/Dropbox/Zotero/Tran et al_2017_Deep Probabilistic Programming.pdf;/home/arthur/Zotero/storage/85G33VSV/1701.html}
}

@article{hassabis_neuroscience-inspired_2017,
  langid = {english},
  title = {Neuroscience-{{Inspired Artificial Intelligence}}},
  volume = {95},
  issn = {0896-6273},
  url = {http://www.cell.com/neuron/abstract/S0896-6273(17)30509-3},
  number = {2},
  journaltitle = {Neuron},
  shortjournal = {Neuron},
  urldate = {2017-07-21},
  date = {2017-07-19},
  pages = {245-258},
  keywords = {Brain,Learning,Cognition,artificial intelligence,neural network},
  author = {Hassabis, Demis and Kumaran, Dharshan and Summerfield, Christopher and Botvinick, Matthew},
  file = {/home/arthur/Dropbox/Zotero/Hassabis et al_2017_Neuroscience-Inspired Artificial Intelligence.pdf;/home/arthur/Zotero/storage/NAK8IC86/S0896-6273(17)30509-3.html},
  eprinttype = {pmid},
  eprint = {28728020}
}

@inproceedings{srebro_collaborative_2010,
  title = {Collaborative Filtering in a Non-Uniform World: {{Learning}} with the Weighted Trace Norm},
  url = {http://papers.nips.cc/paper/4102-collaborative-filtering-in-a-non-uniform-world-learning-with-the-weighted-trace-norm},
  shorttitle = {Collaborative Filtering in a Non-Uniform World},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  urldate = {2017-07-27},
  date = {2010},
  pages = {2056--2064},
  author = {Srebro, Nathan and Salakhutdinov, Ruslan R.},
  file = {/home/arthur/Dropbox/Zotero/Srebro_Salakhutdinov_2010_Collaborative filtering in a non-uniform world.pdf;/home/arthur/Zotero/storage/SZPXSDZ9/4102-collaborative-filtering-in-a-non-uniform-world-learning-with-the-weighted-trace-norm.html}
}

@article{kendall_multi-task_2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1705.07115},
  primaryClass = {cs},
  title = {Multi-{{Task Learning Using Uncertainty}} to {{Weigh Losses}} for {{Scene Geometry}} and {{Semantics}}},
  url = {http://arxiv.org/abs/1705.07115},
  abstract = {Numerous deep learning applications benefit from multi-task learning with multiple regression and classification objectives. In this paper we make the observation that the performance of such systems is strongly dependent on the relative weighting between each task's loss. Tuning these weights by hand is a difficult and expensive process, making multi-task learning prohibitive in practice. We propose a principled approach to multi-task deep learning which weighs multiple loss functions by considering the homoscedastic uncertainty of each task. This allows us to simultaneously learn various quantities with different units or scales in both classification and regression settings. We demonstrate our model learning per-pixel depth regression, semantic and instance segmentation from a monocular input image. Perhaps surprisingly, we show our model can learn multi-task weightings and outperform separate models trained individually on each task.},
  urldate = {2017-07-27},
  date = {2017-05-19},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  author = {Kendall, Alex and Gal, Yarin and Cipolla, Roberto},
  file = {/home/arthur/Dropbox/Zotero/Kendall et al_2017_Multi-Task Learning Using Uncertainty to Weigh Losses for Scene Geometry and.pdf;/home/arthur/Zotero/storage/PM3RIAF9/1705.html}
}

@inproceedings{cuturi_fast_2011,
  title = {Fast Global Alignment Kernels},
  url = {http://machinelearning.wustl.edu/mlpapers/paper_files/ICML2011Cuturi_489.pdf},
  booktitle = {Proceedings of the 28th International Conference on Machine Learning ({{ICML}}-11)},
  urldate = {2017-09-05},
  date = {2011},
  pages = {929--936},
  author = {Cuturi, Marco},
  file = {/home/arthur/Dropbox/Zotero/Cuturi_2011_Fast global alignment kernels.pdf}
}

@article{vaswani_attention_2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1706.03762},
  primaryClass = {cs},
  title = {Attention {{Is All You Need}}},
  url = {http://arxiv.org/abs/1706.03762},
  abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.0 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
  urldate = {2017-09-14},
  date = {2017-06-12},
  keywords = {Computer Science - Learning,Computer Science - Computation and Language},
  author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
  file = {/home/arthur/Dropbox/Zotero/Vaswani et al_2017_Attention Is All You Need.pdf;/home/arthur/Zotero/storage/4UJ4QX4Z/1706.html}
}

@article{rabiner_tutorial_1989,
  title = {A Tutorial on Hidden {{Markov}} Models and Selected Applications in Speech Recognition},
  volume = {77},
  issn = {0018-9219},
  doi = {10.1109/5.18626},
  abstract = {This tutorial provides an overview of the basic theory of hidden Markov models (HMMs) as originated by L.E. Baum and T. Petrie (1966) and gives practical details on methods of implementation of the theory along with a description of selected applications of the theory to distinct problems in speech recognition. Results from a number of original sources are combined to provide a single source of acquiring the background required to pursue further this area of research. The author first reviews the theory of discrete Markov chains and shows how the concept of hidden states, where the observation is a probabilistic function of the state, can be used effectively. The theory is illustrated with two simple examples, namely coin-tossing, and the classic balls-in-urns system. Three fundamental problems of HMMs are noted and several practical techniques for solving these problems are given. The various types of HMMs that have been studied, including ergodic as well as left-right models, are described},
  number = {2},
  journaltitle = {Proceedings of the IEEE},
  date = {1989-02},
  pages = {257-286},
  keywords = {balls-in-urns system,coin-tossing,discrete Markov chains,Distortion,ergodic models,hidden Markov models,Hidden Markov models,hidden states,left-right models,Markov processes,Mathematical model,Multiple signal classification,probabilistic function,Signal processing,speech recognition,Speech recognition,Statistical analysis,Stochastic processes,Temperature measurement,Tutorial},
  author = {Rabiner, L. R.},
  file = {/home/arthur/Dropbox/Zotero/Rabiner_1989_A tutorial on hidden Markov models and selected applications in speech.pdf;/home/arthur/Zotero/storage/F4KDVU6I/18626.html}
}

@online{jordan_introduction_2017,
  title = {An {{Introduction}} to {{Probabilistic Graphical Models}}},
  url = {http://people.eecs.berkeley.edu/~jordan/prelims/},
  urldate = {2017-09-14},
  date = {2017},
  author = {Jordan, Michael I.},
  file = {/home/arthur/Zotero/storage/MZ8X6LE6/prelims.html}
}

@book{jordan_probability_2003,
  title = {Probability {{Propagation}} and {{Factor Graphs}}},
  url = {http://people.eecs.berkeley.edu/~jordan/prelims/chapter4.pdf},
  urldate = {2017-09-14},
  date = {2003},
  author = {Jordan, Michael I.},
  file = {/home/arthur/Dropbox/Zotero/Jordan_2003_Probability Propagation and Factor Graphs.pdf}
}

@online{sutton_introduction_2011,
  title = {An {{Introduction}} to {{Conditional Random Fields}}},
  url = {http://homepages.inf.ed.ac.uk/csutton/publications/crftutv2.pdf},
  urldate = {2017-09-14},
  date = {2011},
  author = {Sutton, Charles and McCallum, Andrew},
  file = {/home/arthur/Dropbox/Zotero/Sutton_McCallum_2011_An Introduction to Conditional Random Fields.pdf}
}

@article{marteau_recursive_2015,
  title = {On {{Recursive Edit Distance Kernels With Application}} to {{Time Series Classification}}},
  volume = {26},
  issn = {2162-237X},
  doi = {10.1109/TNNLS.2014.2333876},
  abstract = {This paper proposes some extensions to the work on kernels dedicated to string or time series global alignment based on the aggregation of scores obtained by local alignments. The extensions that we propose allow us to construct, from classical recursive definition of elastic distances, recursive edit distance (or time-warp) kernels that are positive definite if some sufficient conditions are satisfied. The sufficient conditions we end up with are original and weaker than those proposed in earlier works, although a recursive regularizing term is required to get proof of the positive definiteness as a direct consequence of the Haussler's convolution theorem. Furthermore, the positive definiteness is maintained when a symmetric corridor is used to reduce the search space, and thus the algorithmic complexity, which is quadratic in the worst case. The classification experiment we conducted on three classical time-warp distances (two of which are metrics), using support vector machine classifier, leads to the conclusion that when the pairwise distance matrix obtained from the training data is far from definiteness, the positive definite recursive elastic kernels outperform in general the distance substituting kernels for several classical elastic distances we have tested.},
  number = {6},
  journaltitle = {IEEE Transactions on Neural Networks and Learning Systems},
  date = {2015-06},
  pages = {1121-1133},
  keywords = {Vectors,computational complexity,pattern classification,support vector machines,training data,matrix algebra,Time measurement,algorithmic complexity,classical time-warp distances,Convolution,distance substituting kernels,Dynamic time warping (DTW),edit distance,elastic distances,Haussler convolution theorem,Kernel,kernel definiteness,local alignments,pairwise distance matrix,recursive edit distance kernels,recursive regularizing term,search space reduction,sufficient conditions,support vector machine (SVM),support vector machine classifier,Support vector machines,symmetric corridor,time series,Time series analysis,time series classification,time series global alignment},
  author = {Marteau, P. F. and Gibet, S.},
  file = {/home/arthur/Dropbox/Zotero/Marteau_Gibet_2015_On Recursive Edit Distance Kernels With Application to Time Series.pdf;/home/arthur/Zotero/storage/29HEHKJM/6866161.html}
}

@inproceedings{blondel_soft-dtw_2017,
  title = {Soft-{{DTW}}: A {{Differentiable Loss Function}} for {{Time}}-{{Series}}},
  url = {http://mblondel.org/publications/mcuturi-mblondel-icml2017.pdf},
  urldate = {2017-09-14},
  date = {2017},
  author = {Blondel, Mathieu and Cuturi, Marco},
  file = {/home/arthur/Dropbox/Zotero/Blondel_Cuturi_2017_Soft-DTW.pdf}
}

@article{eisner_inside-outside_2016,
  title = {Inside-Outside and Forward-Backward Algorithms Are Just Backprop},
  url = {http://www.aclweb.org/old_anthology/W/W16/W16-59.pdf#page=13},
  journaltitle = {EMNLP 2016},
  urldate = {2017-09-14},
  date = {2016},
  pages = {1},
  author = {Eisner, Jason},
  file = {/home/arthur/Dropbox/Zotero/Eisner_2016_Inside-outside and forward-backward algorithms are just backprop.pdf}
}

@unpublished{marteau_times_2016,
  title = {Times Series Averaging and Denoising from a Probabilistic Perspective on Time-Elastic Kernels},
  url = {https://hal.archives-ouvertes.fr/hal-01401072},
  abstract = {In the light of regularized dynamic time warping kernels, this paper re-considers the concept of time elastic centroid for a set
of time series. We derive a new algorithm based on a probabilistic interpretation of kernel alignment matrices. This algorithm expresses
the averaging process in terms of a stochastic alignment automata. It uses an iterative agglomerative heuristic method for averaging
the aligned samples, while also averaging the times of occurrence of the aligned samples. By comparing classification accuracies for
45 heterogeneous time series datasets obtained by first nearest centroid/medoid classifiers we show that: i) centroid-based
approaches significantly outperform medoid-based approaches, ii) for the considered datasets, our algorithm that combines averaging
in the sample space and along the time axes, emerges as the most significantly robust model for time-elastic averaging with a
promising noise reduction capability. We also demonstrate its benefit in an isolated gesture recognition experiment and its ability to
significantly reduce the size of training instance sets. Finally we highlight its denoising capability using demonstrative synthetic data:
we show that it is possible to retrieve, from few noisy instances, a signal whose components are scattered in a wide spectral band.},
  urldate = {2017-09-14},
  date = {2016-11},
  keywords = {Classification,Denoising,Dynamic Time Warping,Hidden Markov Model,Time elastic kernel,Time series averaging},
  author = {Marteau, Pierre-François},
  file = {/home/arthur/Dropbox/Zotero/Marteau_2016_Times series averaging and denoising from a probabilistic perspective on.pdf}
}

@article{eisner_inside-outside_2016-1,
  title = {Inside-Outside and Forward-Backward Algorithms Are Just Backprop (Tutorial Paper)},
  url = {http://www.aclweb.org/old_anthology/W/W16/W16-59.pdf#page=13},
  journaltitle = {EMNLP 2016},
  urldate = {2017-09-14},
  date = {2016},
  pages = {1},
  author = {Eisner, Jason},
  file = {/home/arthur/Dropbox/Zotero/Eisner_2016_Inside-outside and forward-backward algorithms are just backprop (tutorial.pdf}
}

@article{che_decade_2017,
  title = {{{DECADE}}: {{A Deep Metric Learning Model}} for {{Multivariate Time Series}}},
  url = {http://www-bcf.usc.edu/~liu32/milets17/paper/MiLeTS17_paper_8.pdf},
  shorttitle = {{{DECADE}}},
  urldate = {2017-09-14},
  date = {2017},
  author = {Che, Zhengping and He, Xinran and Xu, Ke and Liu, Yan},
  file = {/home/arthur/Dropbox/Zotero/Che et al_2017_DECADE.pdf}
}

@article{courty_joint_2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1705.08848},
  primaryClass = {cs, stat},
  title = {Joint {{Distribution Optimal Transportation}} for {{Domain Adaptation}}},
  url = {http://arxiv.org/abs/1705.08848},
  abstract = {This paper deals with the unsupervised domain adaptation problem, where one wants to estimate a prediction function \$f\$ in a given target domain without any labeled sample by exploiting the knowledge available from a source domain where labels are known. Our work makes the following assumption: there exists a non-linear transformation between the joint feature/label space distributions of the two domain \$$\backslash$mathcal\{P\}\_s\$ and \$$\backslash$mathcal\{P\}\_t\$. We propose a solution of this problem with optimal transport, that allows to recover an estimated target \$$\backslash$mathcal\{P\}\^f\_t=(X,f(X))\$ by optimizing simultaneously the optimal coupling and \$f\$. We show that our method corresponds to the minimization of a bound on the target error, and provide an efficient algorithmic solution, for which convergence is proved. The versatility of our approach, both in terms of class of hypothesis or loss functions is demonstrated with real world classification and regression problems, for which we reach or surpass state-of-the-art results.},
  urldate = {2017-09-14},
  date = {2017-05-24},
  keywords = {Computer Science - Learning,Statistics - Machine Learning},
  author = {Courty, Nicolas and Flamary, Rémi and Habrard, Amaury and Rakotomamonjy, Alain},
  file = {/home/arthur/Dropbox/Zotero/Courty et al_2017_Joint Distribution Optimal Transportation for Domain Adaptation.pdf}
}

@article{resnik_gibbs_2010,
  title = {Gibbs {{Sampling}} for the {{Uninitiated}}},
  url = {https://www.umiacs.umd.edu/~resnik/pubs/LAMP-TR-153.pdf},
  urldate = {2017-09-07},
  date = {2010},
  author = {Resnik, Philip and Hardisty, Eric},
  file = {/home/arthur/Zotero/storage/P5LL2YEB/LAMP-TR-153.pdf.pdf}
}

@article{ao_query-by-example_2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1709.00354},
  primaryClass = {cs},
  title = {Query-by-Example {{Spoken Term Detection}} Using {{Attention}}-Based {{Multi}}-Hop {{Networks}}},
  url = {http://arxiv.org/abs/1709.00354},
  abstract = {Retrieving spoken content with spoken queries, or query-by- example spoken term detection (STD), is attractive because it makes possible the matching of signals directly on the acoustic level without transcribing them into text. Here, we propose an end-to-end query-by-example STD model based on an attention-based multi-hop network, whose input is a spoken query and an audio segment containing several utterances; the output states whether the audio segment includes the query. The model can be trained in either a supervised scenario using labeled data, or in an unsupervised fashion. In the supervised scenario, we find that the attention mechanism and multiple hops improve performance, and that the attention weights indicate the time span of the detected terms. In the unsupervised setting, the model mimics the behavior of the existing query-by-example STD system, yielding performance comparable to the existing system but with a lower search time complexity.},
  urldate = {2017-09-14},
  date = {2017-09-01},
  keywords = {Computer Science - Computation and Language,Computer Science - Multimedia},
  author = {Ao, Chia-Wei and Lee, Hung-yi},
  file = {/home/arthur/Dropbox/Zotero/Ao_Lee_2017_Query-by-example Spoken Term Detection using Attention-based Multi-hop Networks.pdf;/home/arthur/Zotero/storage/QMUWS4WH/1709.html}
}

@article{jendoubi_dynamic_2015,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1701.07756},
  primaryClass = {cs, stat},
  title = {Dynamic Time Warping Distance for Message Propagation Classification in {{Twitter}}},
  volume = {9161},
  url = {http://arxiv.org/abs/1701.07756},
  abstract = {Social messages classification is a research domain that has attracted the attention of many researchers in these last years. Indeed, the social message is different from ordinary text because it has some special characteristics like its shortness. Then the development of new approaches for the processing of the social message is now essential to make its classification more efficient. In this paper, we are mainly interested in the classification of social messages based on their spreading on online social networks (OSN). We proposed a new distance metric based on the Dynamic Time Warping distance and we use it with the probabilistic and the evidential k Nearest Neighbors (k-NN) classifiers to classify propagation networks (PrNets) of messages. The propagation network is a directed acyclic graph (DAG) that is used to record propagation traces of the message, the traversed links and their types. We tested the proposed metric with the chosen k-NN classifiers on real world propagation traces that were collected from Twitter social network and we got good classification accuracies.},
  urldate = {2017-09-13},
  date = {2015},
  pages = {419-428},
  keywords = {Statistics - Machine Learning,Computer Science - Artificial Intelligence,Computer Science - Social and Information Networks},
  author = {Jendoubi, Siwar and Martin, Arnaud and Liétard, Ludovic and Yaghlane, Boutheina Ben and Hadji, Hend Ben},
  file = {/home/arthur/Dropbox/Zotero/Jendoubi et al_2015_Dynamic time warping distance for message propagation classification in Twitter.pdf;/home/arthur/Zotero/storage/5LM82UUR/1701.html}
}

@inproceedings{hundt_cuda-accelerated_2014,
  title = {{{CUDA}}-{{Accelerated Alignment}} of {{Subsequences}} in {{Streamed Time Series Data}}},
  doi = {10.1109/ICPP.2014.10},
  abstract = {Euclidean Distance (ED) and Dynamic Time Warping (DTW) are cornerstones in the field of time series data mining. Many high-level algorithms like kNN-classification, clustering or anomaly detection make excessive use of these distance measures as subroutines. Furthermore, the vast growth of recorded data produced by automated monitoring systems or integrated sensors establishes the need for efficient implementations. In this paper, we introduce linear memory parallelization schemes for the alignment of a given query Q in a stream of time series data S for both ED and DTW using CUDA-enabled accelerators. The ED parallelization features a log-linear calculation scheme in contrast to the naive implementation with quadratic time complexity which allows for more efficient processing of long queries. The DTW implementation makes extensive use of a lower-bound cascade to avoid expensive calculations for unpromising candidates. Our CUDA-parallelizations for both ED and DTW outperform state-of-the-art algorithms, namely the UCR-Suite. The gained speedups range from one to two orders-of-magnitude which allows for significantly faster processing of exceedingly bigger data streams.},
  eventtitle = {2014 43rd {{International Conference}} on {{Parallel Processing}}},
  booktitle = {2014 43rd {{International Conference}} on {{Parallel Processing}}},
  date = {2014-09},
  pages = {10-19},
  keywords = {pattern classification,Monitoring,Time measurement,time series,Time series analysis,anomaly detection,automated monitoring systems,bigger data streams,CUDA,CUDA-accelerated alignment,CUDA-enabled accelerators,CUDA-parallelizations,data mining,distance measures,DTW,dynamic time warping,ED parallelization features,Euclidean distance,high level algorithms,Indexes,Instruction sets,kNN-classification,linear memory parallelization schemes,log-linear calculation scheme,quadratic time complexity,Runtime,streamed time series data mining,Subsequence,Time complexity},
  author = {Hundt, C. and Schmidt, B. and Schömer, E.},
  file = {/home/arthur/Dropbox/Zotero/Hundt et al_2014_CUDA-Accelerated Alignment of Subsequences in Streamed Time Series Data.pdf;/home/arthur/Zotero/storage/I9RUAYPU/6957210.html}
}

@article{kim_structured_2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1702.00887},
  primaryClass = {cs},
  title = {Structured {{Attention Networks}}},
  url = {http://arxiv.org/abs/1702.00887},
  abstract = {Attention networks have proven to be an effective approach for embedding categorical inference within a deep neural network. However, for many tasks we may want to model richer structural dependencies without abandoning end-to-end training. In this work, we experiment with incorporating richer structural distributions, encoded using graphical models, within deep networks. We show that these structured attention networks are simple extensions of the basic attention procedure, and that they allow for extending attention beyond the standard soft-selection approach, such as attending to partial segmentations or to subtrees. We experiment with two different classes of structured attention networks: a linear-chain conditional random field and a graph-based parsing model, and describe how these models can be practically implemented as neural network layers. Experiments show that this approach is effective for incorporating structural biases, and structured attention networks outperform baseline attention models on a variety of synthetic and real tasks: tree transduction, neural machine translation, question answering, and natural language inference. We further find that models trained in this way learn interesting unsupervised hidden representations that generalize simple attention.},
  urldate = {2017-09-12},
  date = {2017-02-02},
  keywords = {Computer Science - Learning,Computer Science - Computation and Language,Computer Science - Neural and Evolutionary Computing},
  author = {Kim, Yoon and Denton, Carl and Hoang, Luong and Rush, Alexander M.},
  file = {/home/arthur/Dropbox/Zotero/Kim et al_2017_Structured Attention Networks.pdf;/home/arthur/Zotero/storage/2ISJ58WC/1702.html}
}

@inproceedings{garreau_metric_2014,
  title = {Metric Learning for Temporal Sequence Alignment},
  url = {http://papers.nips.cc/paper/5383-metric-learning-for-temporal-sequence-alignment},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  urldate = {2017-09-07},
  date = {2014},
  pages = {1817--1825},
  author = {Garreau, Damien and Lajugie, Rémi and Arlot, Sylvain and Bach, Francis},
  file = {/home/arthur/Dropbox/Zotero/Garreau et al_2014_Metric learning for temporal sequence alignment.pdf}
}

@inproceedings{lajugie_weakly-supervised_2016,
  title = {A Weakly-Supervised Discriminative Model for Audio-to-Score Alignment},
  url = {http://ieeexplore.ieee.org/abstract/document/7472124/},
  booktitle = {Acoustics, {{Speech}} and {{Signal Processing}} ({{ICASSP}}), 2016 {{IEEE International Conference}} On},
  publisher = {{IEEE}},
  urldate = {2017-09-07},
  date = {2016},
  pages = {2484--2488},
  author = {Lajugie, Rémi and Bojanowski, Piotr and Cuvillier, Philippe and Arlot, Sylvain and Bach, Francis},
  file = {/home/arthur/Dropbox/Zotero/Lajugie et al_2016_A weakly-supervised discriminative model for audio-to-score alignment.pdf}
}

@article{lample_neural_2016,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1603.01360},
  primaryClass = {cs},
  title = {Neural {{Architectures}} for {{Named Entity Recognition}}},
  url = {http://arxiv.org/abs/1603.01360},
  abstract = {State-of-the-art named entity recognition systems rely heavily on hand-crafted features and domain-specific knowledge in order to learn effectively from the small, supervised training corpora that are available. In this paper, we introduce two new neural architectures---one based on bidirectional LSTMs and conditional random fields, and the other that constructs and labels segments using a transition-based approach inspired by shift-reduce parsers. Our models rely on two sources of information about words: character-based word representations learned from the supervised corpus and unsupervised word representations learned from unannotated corpora. Our models obtain state-of-the-art performance in NER in four languages without resorting to any language-specific knowledge or resources such as gazetteers.},
  urldate = {2017-09-07},
  date = {2016-03-04},
  keywords = {Computer Science - Computation and Language},
  author = {Lample, Guillaume and Ballesteros, Miguel and Subramanian, Sandeep and Kawakami, Kazuya and Dyer, Chris},
  file = {/home/arthur/Dropbox/Zotero/Lample et al_2016_Neural Architectures for Named Entity Recognition.pdf;/home/arthur/Zotero/storage/HVAMHE3K/1603.html}
}

@inproceedings{cuturi_kernel_2007,
  title = {A {{Kernel}} for {{Time Series Based}} on {{Global Alignments}}},
  volume = {2},
  doi = {10.1109/ICASSP.2007.366260},
  abstract = {We propose in this paper a new family of kernels to handle time series, notably speech data, within the framework of kernel methods which includes popular algorithms such as the support vector machine. These kernels elaborate on the well known dynamic time warping (DTW) family of distances by considering the same set of elementary operations, namely substitutions and repetitions of tokens, to map a sequence onto another. Associating to each of these operations a given score, DTW algorithms use dynamic programming techniques to compute an optimal sequence of operations with high overall score, in this paper we consider instead the score spanned by all possible alignments, take a smoothed version of their maximum and derive a kernel out of this formulation. We prove that this kernel is positive definite under favorable conditions and show how it can be tuned effectively for practical applications as we report encouraging results on a speech recognition task.},
  eventtitle = {2007 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} - {{ICASSP}} '07},
  booktitle = {2007 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} - {{ICASSP}} '07},
  date = {2007-04},
  pages = {II-413-II-416},
  keywords = {Databases,support vector machine,support vector machines,Heuristic algorithms,Mathematics,speech recognition,Speech recognition,Kernel,Support vector machines,time series,dynamic time warping,Bioinformatics,Buildings,dynamic programming,Dynamic programming,dynamic programming techniques,global alignments,kernel methods,Polynomials,speech data},
  author = {Cuturi, M. and Vert, J. P. and Birkenes, O. and Matsui, T.},
  file = {/home/arthur/Dropbox/Zotero/Cuturi et al_2007_A Kernel for Time Series Based on Global Alignments.pdf;/home/arthur/Zotero/storage/BVR8MGIF/4217433.html}
}

@article{raffel_online_2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1704.00784},
  primaryClass = {cs},
  title = {Online and {{Linear}}-{{Time Attention}} by {{Enforcing Monotonic Alignments}}},
  url = {http://arxiv.org/abs/1704.00784},
  abstract = {Recurrent neural network models with an attention mechanism have proven to be extremely effective on a wide variety of sequence-to-sequence problems. However, the fact that soft attention mechanisms perform a pass over the entire input sequence when producing each element in the output sequence precludes their use in online settings and results in a quadratic time complexity. Based on the insight that the alignment between input and output sequence elements is monotonic in many problems of interest, we propose an end-to-end differentiable method for learning monotonic alignments which, at test time, enables computing attention online and in linear time. We validate our approach on sentence summarization, machine translation, and online speech recognition problems and achieve results competitive with existing sequence-to-sequence models.},
  urldate = {2017-09-07},
  date = {2017-04-03},
  keywords = {Computer Science - Learning,Computer Science - Computation and Language},
  author = {Raffel, Colin and Luong, Minh-Thang and Liu, Peter J. and Weiss, Ron J. and Eck, Douglas},
  file = {/home/arthur/Dropbox/Zotero/Raffel et al_2017_Online and Linear-Time Attention by Enforcing Monotonic Alignments.pdf;/home/arthur/Zotero/storage/J7XCSQLS/1704.html}
}

@article{cuturi_sinkhorn_2013,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1306.0895},
  primaryClass = {stat},
  title = {Sinkhorn {{Distances}}: {{Lightspeed Computation}} of {{Optimal Transportation Distances}}},
  url = {http://arxiv.org/abs/1306.0895},
  shorttitle = {Sinkhorn {{Distances}}},
  abstract = {Optimal transportation distances are a fundamental family of parameterized distances for histograms. Despite their appealing theoretical properties, excellent performance in retrieval tasks and intuitive formulation, their computation involves the resolution of a linear program whose cost is prohibitive whenever the histograms' dimension exceeds a few hundreds. We propose in this work a new family of optimal transportation distances that look at transportation problems from a maximum-entropy perspective. We smooth the classical optimal transportation problem with an entropic regularization term, and show that the resulting optimum is also a distance which can be computed through Sinkhorn-Knopp's matrix scaling algorithm at a speed that is several orders of magnitude faster than that of transportation solvers. We also report improved performance over classical optimal transportation distances on the MNIST benchmark problem.},
  urldate = {2017-09-07},
  date = {2013-06-04},
  keywords = {Statistics - Machine Learning},
  author = {Cuturi, Marco},
  file = {/home/arthur/Dropbox/Zotero/Cuturi_2013_Sinkhorn Distances.pdf;/home/arthur/Zotero/storage/8XYP836Y/1306.html}
}

@article{blondel_higher-order_2016,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1607.07195},
  primaryClass = {cs, stat},
  title = {Higher-{{Order Factorization Machines}}},
  url = {http://arxiv.org/abs/1607.07195},
  abstract = {Factorization machines (FMs) are a supervised learning approach that can use second-order feature combinations even when the data is very high-dimensional. Unfortunately, despite increasing interest in FMs, there exists to date no efficient training algorithm for higher-order FMs (HOFMs). In this paper, we present the first generic yet efficient algorithms for training arbitrary-order HOFMs. We also present new variants of HOFMs with shared parameters, which greatly reduce model size and prediction times while maintaining similar accuracy. We demonstrate the proposed approaches on four different link prediction tasks.},
  urldate = {2017-09-05},
  date = {2016-07-25},
  keywords = {Computer Science - Learning,Statistics - Machine Learning},
  author = {Blondel, Mathieu and Fujino, Akinori and Ueda, Naonori and Ishihata, Masakazu},
  file = {/home/arthur/Dropbox/Zotero/Blondel et al_2016_Higher-Order Factorization Machines.pdf;/home/arthur/Zotero/storage/2S82T5PY/1607.html}
}

@online{nakagawa_seaker_1987,
  title = {Seaker Independent {{English Consonant}} and {{Japanese}} Word Recognition by a Stochastic Dynamic Time Warping Method},
  url = {http://www.slp.ics.tut.ac.jp/shiryou/number-2/E1988-16.pdf},
  urldate = {2017-09-14},
  date = {1987},
  author = {Nakagawa, Seiichi},
  file = {/home/arthur/Dropbox/Zotero/Nakagawa_1987_Seaker independent English Consonant and Japanese word recognition by a.pdf}
}

@article{jacob_correcting_nodate,
  title = {Correcting Gene Expression Data When Neither the Unwanted Variation nor the Factor of Interest Are Observed},
  url = {https://oup.silverchair-cdn.com/oup/backfile/Content_public/Journal/biostatistics/17/1/10.1093_biostatistics_kxv026/3/kxv026.pdf?Expires=1497111145&Signature=VPGpByr2VsLpR4DX5UDMyb7kpHl2dmaC16DG04aIMN9p6OeNJXkm7AkrnfkVyiBBJDe3iYRwAs8GjLdHx0Xjclb9YFlMmkAHE~F0NSKbKqITgoTNVu9h0hj2rF0VMAqsiHtcKLv0DwufpqmxskcOG2DwWG7hhG2V5RmvqRF4SmNo1O9FefunteF3OBGk0eRNUIfoKsDI3XhDL5qdcDgdHFRPuj88LGz4Mu2ReAAMtkKj32JJ31lr4T42R0TctmaGR4fUdKLn1VKpuMoSt3UXWu5micGunPAbf4V6ezjnn891h7T9jNXOBP8lOkhFOsKTABND8slKGcxqnEFYzo0NIQ__&Key-Pair-Id=APKAIUCZBIA4LVPAVW3Q},
  urldate = {2017-06-09},
  author = {Jacob, Laurent and Speed, Terence P. and Gagnon-Bartsch},
  file = {/home/arthur/Dropbox/Zotero/Jacob et al_Correcting gene expression data when neither the unwanted variation nor the.pdf}
}

@article{condat_fast_2016,
  langid = {english},
  title = {Fast Projection onto the Simplex and the L1 Ball},
  volume = {158},
  issn = {0025-5610, 1436-4646},
  url = {https://link.springer.com/article/10.1007/s10107-015-0946-6},
  abstract = {A new algorithm is proposed to project, exactly and in finite time, a vector of arbitrary size onto a simplex or an \$\$l\_1\$\$l1-norm ball. It can be viewed as a Gauss–Seidel-like variant of Michelot’s v},
  number = {1-2},
  journaltitle = {Mathematical Programming},
  shortjournal = {Math. Program.},
  urldate = {2017-09-19},
  date = {2016-07-01},
  pages = {575-585},
  author = {Condat, Laurent},
  file = {/home/arthur/Dropbox/Zotero/Condat_2016_Fast projection onto the simplex and the l1 ball.pdf;/home/arthur/Zotero/storage/W2URTAPF/s10107-015-0946-6.html}
}

@article{azuma_algebraic_2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1702.06941},
  primaryClass = {cs},
  title = {An {{Algebraic Formalization}} of {{Forward}} and {{Forward}}-Backward {{Algorithms}}},
  url = {http://arxiv.org/abs/1702.06941},
  abstract = {In this paper, we propose an algebraic formalization of the two important classes of dynamic programming algorithms called forward and forward-backward algorithms. They are generalized extensively in this study so that a wide range of other existing algorithms is subsumed. Forward algorithms generalized in this study subsume the ordinary forward algorithm on trellises for sequence labeling, the inside algorithm on derivation forests for CYK parsing, a unidirectional message passing on acyclic factor graphs, the forward mode of automatic differentiation on computation graphs with addition and multiplication, and so on. In addition, we reveal algebraic structures underlying complicated computation with forward algorithms. By the aid of the revealed algebraic structures, we also propose a systematic framework to design complicated variants of forward algorithms. Forward-backward algorithms generalized in this study subsume the ordinary forward-backward algorithm on trellises for sequence labeling, the inside-outside algorithm on derivation forests for CYK parsing, the sum-product algorithm on acyclic factor graphs, the reverse mode of automatic differentiation (a.k.a. back propagation) on computation graphs with addition and multiplication, and so on. We also propose an algebraic characterization of what can be computed by forward-backward algorithms and elucidate the relationship between forward and forward-backward algorithms.},
  urldate = {2017-09-19},
  date = {2017-02-22},
  keywords = {Computer Science - Learning},
  author = {Azuma, Ai and Shimbo, Masashi and Matsumoto, Yuji},
  file = {/home/arthur/Dropbox/Zotero/Azuma et al_2017_An Algebraic Formalization of Forward and Forward-backward Algorithms.pdf;/home/arthur/Zotero/storage/UF44QXT6/1702.html}
}

@inproceedings{meshi_smooth_2015,
  title = {Smooth and Strong: {{Map}} Inference with Linear Convergence},
  url = {http://papers.nips.cc/paper/5710-smooth-and-strong-map-inference-with-linear-convergence},
  shorttitle = {Smooth and Strong},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  urldate = {2017-09-15},
  date = {2015},
  pages = {298--306},
  author = {Meshi, Ofer and Mahdavi, Mehrdad and Schwing, Alex},
  file = {/home/arthur/Dropbox/Zotero/Meshi et al_2015_Smooth and strong.pdf}
}

@inproceedings{genevay_stochastic_2016,
  title = {Stochastic Optimization for Large-Scale Optimal Transport},
  url = {http://papers.nips.cc/paper/6566-stochastic-optimization-for-large-scale-optimal-transport},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  urldate = {2017-09-15},
  date = {2016},
  pages = {3440--3448},
  author = {Genevay, Aude and Cuturi, Marco and Peyré, Gabriel and Bach, Francis},
  file = {/home/arthur/Dropbox/Zotero/Genevay et al_2016_Stochastic optimization for large-scale optimal transport.pdf}
}

@inproceedings{sawada_re-ranking_2014,
  title = {Re-Ranking of Spoken Term Detections Using {{CRF}}-Based Triphone Detection Models},
  doi = {10.1109/APSIPA.2014.7041550},
  abstract = {Conventional spoken term detection (STD) techniques, which use a text-based matching approach based on automatic speech recognition (ASR) systems, are not robust for speech recognition errors. This paper proposes a conditional random fields (CRF)-based re-ranking approach, which recomputes detection scores produced by a phoneme-based dynamic time warping (DTW) STD approach. In the re-ranking approach, we tackle STD as a sequence labeling problem. We use CRF-based triphone detection models based on features generated from multiple types of phoneme-based transcriptions. They train recognition error patterns such as phoneme-to-phoneme confusions on the CRF framework. Therefore, the models can detect a triphone, which is one of triphones composing a query term, with detection probability. In the experimental evaluation on the Japanese OOV test collection, the CRF-based approach alone could not outperform the conventional DTW-based approach we have already proposed; however, it worked well in the re-ranking (second-pass) process for the detections from the DTW-based approach. The CRF-based re-ranking approach made a 2.4\% improvement of F-measure in the STD performance.},
  eventtitle = {Signal and {{Information Processing Association Annual Summit}} and {{Conference}} ({{APSIPA}}), 2014 {{Asia}}-{{Pacific}}},
  booktitle = {Signal and {{Information Processing Association Annual Summit}} and {{Conference}} ({{APSIPA}}), 2014 {{Asia}}-{{Pacific}}},
  date = {2014-12},
  pages = {1-4},
  keywords = {Training,Hidden Markov models,speech recognition,Speech recognition,Indexes,conditional random field,CRF-based re-ranking approach,CRF-based triphone detection model,detection score recomputation,DTW-based approach,F-measure,Feature extraction,Japanese OOV test collection,pattern matching,phoneme-based dynamic time warping,phoneme-based transcriptions,Probability,random processes,recognition error patterns,sequence labeling problem,Speech,spoken term detection,text analysis,text-based matching approach},
  author = {Sawada, N. and Natori, S. and Nishizaki, H.},
  file = {/home/arthur/Dropbox/Zotero/Sawada et al_2014_Re-ranking of spoken term detections using CRF-based triphone detection models.pdf;/home/arthur/Zotero/storage/NLJMRZ3L/7041550.html}
}

@article{gramfort_fast_2015,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1503.08596},
  primaryClass = {cs},
  title = {Fast {{Optimal Transport Averaging}} of {{Neuroimaging Data}}},
  url = {http://arxiv.org/abs/1503.08596},
  abstract = {Knowing how the Human brain is anatomically and functionally organized at the level of a group of healthy individuals or patients is the primary goal of neuroimaging research. Yet computing an average of brain imaging data defined over a voxel grid or a triangulation remains a challenge. Data are large, the geometry of the brain is complex and the between subjects variability leads to spatially or temporally non-overlapping effects of interest. To address the problem of variability, data are commonly smoothed before group linear averaging. In this work we build on ideas originally introduced by Kantorovich to propose a new algorithm that can average efficiently non-normalized data defined over arbitrary discrete domains using transportation metrics. We show how Kantorovich means can be linked to Wasserstein barycenters in order to take advantage of an entropic smoothing approach. It leads to a smooth convex optimization problem and an algorithm with strong convergence guarantees. We illustrate the versatility of this tool and its empirical behavior on functional neuroimaging data, functional MRI and magnetoencephalography (MEG) source estimates, defined on voxel grids and triangulations of the folded cortical surface.},
  urldate = {2017-09-20},
  date = {2015-03-30},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  author = {Gramfort, Alexandre and Peyré, Gabriel and Cuturi, Marco},
  file = {/home/arthur/Dropbox/Zotero/Gramfort et al_2015_Fast Optimal Transport Averaging of Neuroimaging Data.pdf;/home/arthur/Zotero/storage/IC3H4K57/Gramfort et al. - 2015 - Fast Optimal Transport Averaging of Neuroimaging D.html}
}

@inproceedings{su_order-preserving_2017,
  title = {Order-Preserving {{Wasserstein Distance}} for {{Sequence Matching}}},
  url = {http://www.ganghua.org/publication/CVPR17d.pdf},
  eventtitle = {Conference on {{Computer Vison}} and {{Patterm Recognition}}},
  urldate = {2017-09-20},
  date = {2017},
  author = {Su, Bing and Hua, Gang},
  file = {/home/arthur/Dropbox/Zotero/Su_Hua_2017_Order-preserving Wasserstein Distance for Sequence Matching.pdf}
}

@article{ferradans_regularized_2013,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1307.5551},
  primaryClass = {cs, math},
  title = {Regularized {{Discrete Optimal Transport}}},
  url = {http://arxiv.org/abs/1307.5551},
  abstract = {This article introduces a generalization of the discrete optimal transport, with applications to color image manipulations. This new formulation includes a relaxation of the mass conservation constraint and a regularization term. These two features are crucial for image processing tasks, which necessitate to take into account families of multimodal histograms, with large mass variation across modes. The corresponding relaxed and regularized transportation problem is the solution of a convex optimization problem. Depending on the regularization used, this minimization can be solved using standard linear programming methods or first order proximal splitting schemes. The resulting transportation plan can be used as a color transfer map, which is robust to mass variation across images color palettes. Furthermore, the regularization of the transport plan helps to remove colorization artifacts due to noise amplification. We also extend this framework to the computation of barycenters of distributions. The barycenter is the solution of an optimization problem, which is separately convex with respect to the barycenter and the transportation plans, but not jointly convex. A block coordinate descent scheme converges to a stationary point of the energy. We show that the resulting algorithm can be used for color normalization across several images. The relaxed and regularized barycenter defines a common color palette for those images. Applying color transfer toward this average palette performs a color normalization of the input images.},
  urldate = {2017-09-25},
  date = {2013-07-21},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Mathematics - Optimization and Control,Computer Science - Discrete Mathematics},
  author = {Ferradans, Sira and Papadakis, Nicolas and Peyré, Gabriel and Aujol, Jean-François},
  file = {/home/arthur/Dropbox/Zotero/Ferradans et al_2013_Regularized Discrete Optimal Transport.pdf;/home/arthur/Zotero/storage/EJ6THQWZ/1307.html}
}

@article{chizat_unbalanced_2015,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1508.05216},
  primaryClass = {math},
  title = {Unbalanced {{Optimal Transport}}: {{Geometry}} and {{Kantorovich Formulation}}},
  url = {http://arxiv.org/abs/1508.05216},
  shorttitle = {Unbalanced {{Optimal Transport}}},
  abstract = {This article presents a new class of "optimal transportation"-like distances between arbitrary positive Radon measures. These distances are defined by two equivalent alternative formulations: (i) a "fluid dynamic" formulation defining the distance as a geodesic distance over the space of measures (ii) a static "Kantorovich" formulation where the distance is the minimum of an optimization program over pairs of couplings describing the transfer (transport, creation and destruction) of mass between two measures. Both formulations are convex optimization problems, and the ability to switch from one to the other depending on the targeted application is a crucial property of our models. Of particular interest is the Wasserstein-Fisher-Rao metric recently introduced independently by [CSPV15,KMV15]. Defined initially through a dynamic formulation, it belongs to this class of metrics and hence automatically benefits from a static Kantorovich formulation. Switching from the initial Eulerian expression of this metric to a Lagrangian point of view provides the generalization of Otto's Riemannian submersion to this new setting, where the group of diffeomorphisms is replaced by a semi-direct product of group. This Riemannian submersion enables a formal computation of the sectional curvature of the space of densities and the formulation of an equivalent Monge problem.},
  urldate = {2017-09-25},
  date = {2015-08-21},
  keywords = {Mathematics - Optimization and Control},
  author = {Chizat, Lenaic and Peyré, Gabriel and Schmitzer, Bernhard and Vialard, François-Xavier},
  file = {/home/arthur/Dropbox/Zotero/Chizat et al_2015_Unbalanced Optimal Transport.pdf;/home/arthur/Zotero/storage/4L2A9CZ8/1508.html}
}

@article{beiglbock_problem_2016,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1208.1509},
  title = {On a Problem of Optimal Transport under Marginal Martingale Constraints},
  volume = {44},
  issn = {0091-1798},
  url = {http://arxiv.org/abs/1208.1509},
  abstract = {The basic problem of optimal transportation consists in minimizing the expected costs \$$\backslash$mathbb \{E\}[c(X\_1,X\_2)]\$ by varying the joint distribution \$(X\_1,X\_2)\$ where the marginal distributions of the random variables \$X\_1\$ and \$X\_2\$ are fixed. Inspired by recent applications in mathematical finance and connections with the peacock problem, we study this problem under the additional condition that \$(X\_i)\_\{i=1,2\}\$ is a martingale, that is, \$$\backslash$mathbb \{E\}[X\_2|X\_1]=X\_1\$. We establish a variational principle for this problem which enables us to determine optimal martingale transport plans for specific cost functions. In particular, we identify a martingale coupling that resembles the classic monotone quantile coupling in several respects. In analogy with the celebrated theorem of Brenier, the following behavior can be observed: If the initial distribution is continuous, then this "monotone martingale" is supported by the graphs of two functions \$T\_1,T\_2:$\backslash$mathbb \{R\}$\backslash$to $\backslash$mathbb \{R\}\$.},
  number = {1},
  journaltitle = {The Annals of Probability},
  urldate = {2017-09-25},
  date = {2016-01},
  pages = {42-106},
  keywords = {Mathematics - Optimization and Control,Mathematics - Probability,Mathematics - Functional Analysis},
  author = {Beiglböck, Mathias and Juillet, Nicolas},
  file = {/home/arthur/Dropbox/Zotero/Beiglböck_Juillet_2016_On a problem of optimal transport under marginal martingale constraints.pdf;/home/arthur/Zotero/storage/NURH4PN8/1208.html}
}

@book{villani_optimal_2008,
  title = {Optimal {{Transport}}: {{Old}} and {{New}}},
  url = {http://cedricvillani.org/wp-content/uploads/2012/08/preprint-1.pdf},
  publisher = {{Springer}},
  urldate = {2017-09-25},
  date = {2008},
  author = {Villani, Cédric},
  file = {/home/arthur/Dropbox/Zotero/Villani_2008_Optimal Transport.pdf}
}

@article{thorpe_transportation_2017,
  langid = {english},
  title = {A {{Transportation L}}\^p {{Distance}} for {{Signal Analysis}}},
  volume = {59},
  issn = {0924-9907, 1573-7683},
  url = {https://link.springer.com/article/10.1007/s10851-017-0726-4},
  abstract = {Transport-based distances, such as the Wasserstein distance and earth mover’s distance, have been shown to be an effective tool in signal and image analysis. The success of transport-based distances i},
  number = {2},
  journaltitle = {Journal of Mathematical Imaging and Vision},
  shortjournal = {J Math Imaging Vis},
  urldate = {2017-09-25},
  date = {2017-10-01},
  pages = {187-210},
  author = {Thorpe, Matthew and Park, Serim and Kolouri, Soheil and Rohde, Gustavo K. and Slepčev, Dejan},
  file = {/home/arthur/Dropbox/Zotero/Thorpe et al_2017_A Transportation L^p Distance for Signal Analysis.pdf;/home/arthur/Zotero/storage/6SJ6DQFM/s10851-017-0726-4.html}
}

@book{brualdi_combinatorial_2006,
  location = {{Cambridge, UK ; New York}},
  title = {Combinatorial Matrix Classes},
  isbn = {978-0-521-86565-4},
  pagetotal = {544},
  number = {v. 108},
  series = {Encyclopedia of mathematics and its applications},
  publisher = {{Cambridge University Press}},
  date = {2006},
  keywords = {Combinatorial analysis,Matrices},
  author = {Brualdi, Richard A.},
  file = {/home/arthur/Dropbox/Zotero/Brualdi_2006_Combinatorial matrix classes.pdf},
  note = {OCLC: ocm69484289}
}

@article{genevay_sinkhorn-autodiff_2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1706.00292},
  primaryClass = {stat},
  title = {Sinkhorn-{{AutoDiff}}: {{Tractable Wasserstein Learning}} of {{Generative Models}}},
  url = {http://arxiv.org/abs/1706.00292},
  shorttitle = {Sinkhorn-{{AutoDiff}}},
  abstract = {The ability to compare two degenerate probability distributions (i.e. two probability distributions supported on two distinct low-dimensional manifolds living in a much higher-dimensional space) is a crucial problem arising in the estimation of generative models for high-dimensional observations such as those arising in computer vision or natural language. It is known that optimal transport metrics can represent a cure for this problem, since they were specifically designed as an alternative to information divergences to handle such problematic scenarios. Unfortunately, training generative machines using OT raises formidable computational and statistical challenges, because of (i) the computational burden of evaluating OT losses, (ii) the instability and lack of smoothness of these losses, (iii) the difficulty to estimate robustly these losses and their gradients in high dimension. This paper presents the first tractable computational method to train large scale generative models using an optimal transport loss, and tackles both these issues by relying on two key ideas: (a) entropic smoothing, which turns the original OT loss into one that can be computed using Sinkhorn fixed point iterations; (b) algorithmic (automatic) differentiation of these iterations. These two approximations result in a robust and differentiable approximation of the OT loss with streamlined GPU execution. Entropic smoothing generates a family of losses interpolating between Wasserstein (OT) and Maximum Mean Discrepancy (MMD), thus allowing to find a sweet spot leveraging the geometry of OT and the favorable high-dimensional sample complexity of MMD which comes with unbiased gradient estimates. The resulting computational architecture complements nicely standard deep network generative models by a stack of extra layers implementing the loss function.},
  urldate = {2017-09-21},
  date = {2017-06-01},
  keywords = {Statistics - Machine Learning},
  author = {Genevay, Aude and Peyré, Gabriel and Cuturi, Marco},
  file = {/home/arthur/Dropbox/Zotero/Genevay et al_2017_Sinkhorn-AutoDiff.pdf;/home/arthur/Zotero/storage/GK5YHHUG/1706.html}
}

@article{tjandra_local_2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1705.08091},
  primaryClass = {cs},
  title = {Local {{Monotonic Attention Mechanism}} for {{End}}-to-{{End Speech Recognition}}},
  url = {http://arxiv.org/abs/1705.08091},
  abstract = {Recently, sequence-to-sequence model by using encoder-decoder neural network has gained popularity for automatic speech recognition (ASR). The architecture commonly uses an attentional mechanism which allows the model to learn alignments between source speech sequence and target text sequence. Most attentional mechanisms used today is based on a global attention property which requires a computation of a weighted summarization of the whole input sequence generated by encoder states. However, it is computationally expensive and often produces misalignment on the longer input sequence. Furthermore, it does not fit with monotonous or left-to-right nature in speech recognition task. In this paper, we propose a novel attention mechanism that has local and monotonic properties. Various ways to control those properties are also explored. Experimental results demonstrate that encoder-decoder based ASR with local monotonic attention could achieve significant performance improvements and reduce the computational complexity in comparison with the one that used the standard global attention architecture.},
  urldate = {2017-09-21},
  date = {2017-05-23},
  keywords = {Computer Science - Computation and Language},
  author = {Tjandra, Andros and Sakti, Sakriani and Nakamura, Satoshi},
  file = {/home/arthur/Dropbox/Zotero/Tjandra et al_2017_Local Monotonic Attention Mechanism for End-to-End Speech Recognition.pdf;/home/arthur/Zotero/storage/9V65T6D8/1705.html}
}

@article{niculae_regularized_2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1705.07704},
  primaryClass = {cs, stat},
  title = {A {{Regularized Framework}} for {{Sparse}} and {{Structured Neural Attention}}},
  url = {http://arxiv.org/abs/1705.07704},
  abstract = {Modern neural networks are often augmented with an attention mechanism, which tells the network where to focus within the input. We propose in this paper a new framework for sparse and structured attention, building upon a max operator regularized with a strongly convex function. We show that this operator is differentiable and that its gradient defines a mapping from real values to probabilities, suitable as an attention mechanism. Our framework includes softmax and a slight generalization of the recently-proposed sparsemax as special cases. However, we also show how our framework can incorporate modern structured penalties, resulting in new attention mechanisms that focus on entire segments or groups of an input, encouraging parsimony and interpretability. We derive efficient algorithms to compute the forward and backward passes of these attention mechanisms, enabling their use in a neural network trained with backpropagation. To showcase their potential as a drop-in replacement for existing attention mechanisms, we evaluate them on three large-scale tasks: textual entailment, machine translation, and sentence summarization. Our attention mechanisms improve interpretability without sacrificing performance; notably, on textual entailment and summarization, we outperform the existing attention mechanisms based on softmax and sparsemax.},
  urldate = {2017-09-21},
  date = {2017-05-22},
  keywords = {Computer Science - Learning,Statistics - Machine Learning,Computer Science - Computation and Language},
  author = {Niculae, Vlad and Blondel, Mathieu},
  file = {/home/arthur/Dropbox/Zotero/Niculae_Blondel_2017_A Regularized Framework for Sparse and Structured Neural Attention.pdf;/home/arthur/Zotero/storage/G3ZY4TI9/1705.html}
}

@article{duan_soundprism_2011,
  title = {Soundprism: {{An}} Online System for Score-Informed Source Separation of Music Audio},
  volume = {5},
  url = {http://ieeexplore.ieee.org/abstract/document/5887382/},
  shorttitle = {Soundprism},
  number = {6},
  journaltitle = {IEEE Journal of Selected Topics in Signal Processing},
  urldate = {2017-10-19},
  date = {2011},
  pages = {1205--1215},
  author = {Duan, Zhiyao and Pardo, Bryan},
  file = {/home/arthur/Dropbox/Zotero/Duan_Pardo_2011_Soundprism.pdf;/home/arthur/Zotero/storage/84BW26UA/5887382.html}
}

@article{alonso_parallel_2017,
  langid = {english},
  title = {Parallel Online Time Warping for Real-Time Audio-to-Score Alignment in Multi-Core Systems},
  volume = {73},
  issn = {0920-8542, 1573-0484},
  url = {https://link.springer.com/article/10.1007/s11227-016-1647-5},
  abstract = {The audio-to-score framework consists of two separate stages: preprocessing and alignment. The alignment is commonly solved through offline dynamic time warping (DTW), which is a method to find the pa},
  number = {1},
  journaltitle = {The Journal of Supercomputing},
  shortjournal = {J Supercomput},
  urldate = {2017-10-19},
  date = {2017-01-01},
  pages = {126-138},
  author = {Alonso, Pedro and Cortina, Raquel and Rodríguez-Serrano, F. J. and Vera-Candeas, P. and Alonso-González, M. and Ranilla, José},
  file = {/home/arthur/Dropbox/Zotero/Alonso et al_2017_Parallel online time warping for real-time audio-to-score alignment in.pdf;/home/arthur/Zotero/storage/ZBP6X2QX/s11227-016-1647-5.html}
}

@article{nakamura_real-time_2016,
  title = {Real-{{Time Audio}}-to-{{Score Alignment}} of {{Music Performances Containing Errors}} and {{Arbitrary Repeats}} and {{Skips}}},
  volume = {24},
  issn = {2329-9290},
  doi = {10.1109/TASLP.2015.2507862},
  abstract = {This paper discusses real-time alignment of audio signals of music performance to the corresponding score (a.k.a. score following) which can handle tempo changes, errors and arbitrary repeats and/or skips (repeats/skips) in performances. This type of score following is particularly useful in automatic accompaniment for practices and rehearsals, where errors and repeats/skips are often made. Simple extensions of the algorithms previously proposed in the literature are not applicable in these situations for scores of practical length due to the problem of large computational complexity. To cope with this problem, we present two hidden Markov models of monophonic performance with errors and arbitrary repeats/skips, and derive efficient score-following algorithms with an assumption that the prior probability distributions of score positions before and after repeats/skips are independent from each other. We confirmed real-time operation of the algorithms with music scores of practical length (around 10000 notes) on a modern laptop and their tracking ability to the input performance within 0.7 s on average after repeats/skips in clarinet performance data. Further improvements and extension for polyphonic signals are also discussed.},
  number = {2},
  journaltitle = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  date = {2016-02},
  pages = {329-339},
  keywords = {computational complexity,Signal processing algorithms,Real-time systems,hidden Markov models,Hidden Markov models,Multiple signal classification,arbitrary repeats,Arbitrary repeats and skips,arbitrary skips handling,audio signal processing,audio signals,audio-to-score alignment,Computational complexity,Computational modeling,error handling,fast Viterbi algorithm,hidden Markov model,Instruments,monophonic performance,music,music performance,music scores,music signal processing,polyphonic signals,probability distributions,real-time audio-to-score alignment,score following,score-following algorithm,statistical distributions,tempo change handling},
  author = {Nakamura, T. and Nakamura, E. and Sagayama, S.},
  file = {/home/arthur/Dropbox/Zotero/Nakamura et al_2016_Real-Time Audio-to-Score Alignment of Music Performances Containing Errors and.pdf;/home/arthur/Zotero/storage/2PJZQN3H/7352338.html}
}

@article{conneau_word_2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1710.04087},
  primaryClass = {cs},
  title = {Word {{Translation Without Parallel Data}}},
  url = {http://arxiv.org/abs/1710.04087},
  abstract = {State-of-the-art methods for learning cross-lingual word embeddings have relied on bilingual dictionaries or parallel corpora. Recent works showed that the need for parallel data supervision can be alleviated with character-level information. While these methods showed encouraging results, they are not on par with their supervised counterparts and are limited to pairs of languages sharing a common alphabet. In this work, we show that we can build a bilingual dictionary between two languages without using any parallel corpora, by aligning monolingual word embedding spaces in an unsupervised way. Without using any character information, our model even outperforms existing supervised methods on cross-lingual tasks for some language pairs. Our experiments demonstrate that our method works very well also for distant language pairs, like English-Russian or English-Chinese. We finally show that our method is a first step towards fully unsupervised machine translation and describe experiments on the English-Esperanto language pair, on which there only exists a limited amount of parallel data.},
  urldate = {2017-10-16},
  date = {2017-10-11},
  keywords = {Computer Science - Computation and Language},
  author = {Conneau, Alexis and Lample, Guillaume and Ranzato, Marc'Aurelio and Denoyer, Ludovic and Jégou, Hervé},
  file = {/home/arthur/Dropbox/Zotero/Conneau et al_2017_Word Translation Without Parallel Data.pdf;/home/arthur/Zotero/storage/3N723JPG/1710.html}
}

@article{baydin_automatic_2015,
  title = {Automatic Differentiation in Machine Learning: A Survey},
  url = {https://arxiv.org/abs/1502.05767},
  shorttitle = {Automatic Differentiation in Machine Learning},
  journaltitle = {arXiv preprint arXiv:1502.05767},
  urldate = {2017-10-12},
  date = {2015},
  author = {Baydin, Atilim Gunes and Pearlmutter, Barak A. and Radul, Alexey Andreyevich and Siskind, Jeffrey Mark},
  file = {/home/arthur/Dropbox/Zotero/Baydin et al_2015_Automatic differentiation in machine learning.pdf;/home/arthur/Zotero/storage/F9ABSBYM/1502.html}
}

@article{martins_connection_2001,
  title = {The Connection between the Complex-Step Derivative Approximation and Algorithmic Differentiation},
  volume = {921},
  url = {https://arc.aiaa.org/doi/pdfplus/10.2514/6.2001-921},
  journaltitle = {AIAA paper},
  urldate = {2017-10-12},
  date = {2001},
  pages = {2001},
  author = {Martins, JRRA and Sturdza, Peter and Alonso, Juan J.},
  file = {/home/arthur/Dropbox/Zotero/Martins et al_2001_The connection between the complex-step derivative approximation and.pdf}
}

@article{amos_input_2016,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1609.07152},
  primaryClass = {cs, math},
  title = {Input {{Convex Neural Networks}}},
  url = {http://arxiv.org/abs/1609.07152},
  abstract = {This paper presents the input convex neural network architecture. These are scalar-valued (potentially deep) neural networks with constraints on the network parameters such that the output of the network is a convex function of (some of) the inputs. The networks allow for efficient inference via optimization over some inputs to the network given others, and can be applied to settings including structured prediction, data imputation, reinforcement learning, and others. In this paper we lay the basic groundwork for these models, proposing methods for inference, optimization and learning, and analyze their representational power. We show that many existing neural network architectures can be made input-convex with a minor modification, and develop specialized optimization algorithms tailored to this setting. Finally, we highlight the performance of the methods on multi-label prediction, image completion, and reinforcement learning problems, where we show improvement over the existing state of the art in many cases.},
  urldate = {2017-10-11},
  date = {2016-09-22},
  keywords = {Computer Science - Learning,Mathematics - Optimization and Control},
  author = {Amos, Brandon and Xu, Lei and Kolter, J. Zico},
  file = {/home/arthur/Dropbox/Zotero/Amos et al_2016_Input Convex Neural Networks.pdf;/home/arthur/Zotero/storage/HPRYYF5T/1609.html}
}

@article{amos_optnet_2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1703.00443},
  primaryClass = {cs, math, stat},
  title = {{{OptNet}}: {{Differentiable Optimization}} as a {{Layer}} in {{Neural Networks}}},
  url = {http://arxiv.org/abs/1703.00443},
  shorttitle = {{{OptNet}}},
  abstract = {This paper presents OptNet, a network architecture that integrates optimization problems (here, specifically in the form of quadratic programs) as individual layers in larger end-to-end trainable deep networks. These layers encode constraints and complex dependencies between the hidden states that traditional convolutional and fully-connected layers often cannot capture. In this paper, we explore the foundations for such an architecture: we show how techniques from sensitivity analysis, bilevel optimization, and implicit differentiation can be used to exactly differentiate through these layers and with respect to layer parameters; we develop a highly efficient solver for these layers that exploits fast GPU-based batch solves within a primal-dual interior point method, and which provides backpropagation gradients with virtually no additional cost on top of the solve; and we highlight the application of these approaches in several problems. In one notable example, we show that the method is capable of learning to play mini-Sudoku (4x4) given just input and output games, with no a priori information about the rules of the game; this highlights the ability of our architecture to learn hard constraints better than other neural architectures.},
  urldate = {2017-10-11},
  date = {2017-03-01},
  keywords = {Computer Science - Learning,Mathematics - Optimization and Control,Statistics - Machine Learning,Computer Science - Artificial Intelligence},
  author = {Amos, Brandon and Kolter, J. Zico},
  file = {/home/arthur/Dropbox/Zotero/Amos_Kolter_2017_OptNet.pdf}
}

@article{xu_learning_2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1710.01410},
  primaryClass = {stat},
  title = {Learning {{Registered Point Processes}} from {{Idiosyncratic Observations}}},
  url = {http://arxiv.org/abs/1710.01410},
  abstract = {A parametric point process model is developed, with modeling based on the assumption that sequential observations often share latent phenomena, while also possessing idiosyncratic effects. An alternating optimization method is proposed to learn a "registered" point process that accounts for shared structure, as well as "warping" functions that characterize idiosyncratic aspects of each observed sequence. Under reasonable constraints, in each iteration we update the sample-specific warping functions by solving a set of constrained nonlinear programming problems in parallel, and update the model by maximum likelihood estimation. The justifiability, complexity and robustness of the proposed method are investigated in detail. Experiments on both synthetic and real-world data demonstrate that the method yields explainable point process models, achieving encouraging results compared to state-of-the-art methods.},
  urldate = {2017-10-10},
  date = {2017-10-03},
  keywords = {Statistics - Machine Learning},
  author = {Xu, Hongteng and Carin, Lawrence and Zha, Hongyuan},
  file = {/home/arthur/Dropbox/Zotero/Xu et al_2017_Learning Registered Point Processes from Idiosyncratic Observations.pdf;/home/arthur/Zotero/storage/FF5V5655/1710.html}
}

@incollection{wang_graphical_2016,
  title = {Graphical {{Time Warping}} for {{Joint Alignment}} of {{Multiple Curves}}},
  url = {http://papers.nips.cc/paper/6269-graphical-time-warping-for-joint-alignment-of-multiple-curves.pdf},
  booktitle = {Advances in {{Neural Information Processing Systems}} 29},
  publisher = {{Curran Associates, Inc.}},
  urldate = {2017-10-10},
  date = {2016},
  pages = {3648--3656},
  author = {Wang, Yizhi and Miller, David J and Poskanzer, Kira and Wang, Yue and Tian, Lin and Yu, Guoqiang},
  editor = {Lee, D. D. and Sugiyama, M. and Luxburg, U. V. and Guyon, I. and Garnett, R.},
  file = {/home/arthur/Dropbox/Zotero/Wang et al_2016_Graphical Time Warping for Joint Alignment of Multiple Curves.pdf;/home/arthur/Zotero/storage/FSFGT3MH/6269-graphical-time-warping-for-joint-alignment-of-multiple-curves.html}
}

@article{rezende_stochastic_2014,
  title = {Stochastic Backpropagation and Approximate Inference in Deep Generative Models},
  url = {https://arxiv.org/abs/1401.4082},
  journaltitle = {arXiv preprint arXiv:1401.4082},
  urldate = {2017-10-10},
  date = {2014},
  author = {Rezende, Danilo Jimenez and Mohamed, Shakir and Wierstra, Daan},
  file = {/home/arthur/Dropbox/Zotero/Rezende et al_2014_Stochastic backpropagation and approximate inference in deep generative models.pdf;/home/arthur/Zotero/storage/WEBQ77FY/1401.html}
}

@inproceedings{joulain_efficient_2014,
  title = {Efficient {{Image}} and {{Video Co}}-Localization with {{Frank}}-{{Wolfe Algorithm}}},
  url = {http://ai.stanford.edu/~kdtang/papers/eccv14-vidcoloc.pdf},
  urldate = {2017-09-25},
  date = {2014},
  author = {Joulain, Armand and Tang, Kevin and Li, Fei Fei},
  file = {/home/arthur/Dropbox/Zotero/Joulain et al_2014_Efficient Image and Video Co-localization with Frank-Wolfe Algorithm.pdf}
}

@article{lacoste-julien_global_2015,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1511.05932},
  primaryClass = {cs, math, stat},
  title = {On the {{Global Linear Convergence}} of {{Frank}}-{{Wolfe Optimization Variants}}},
  url = {http://arxiv.org/abs/1511.05932},
  abstract = {The Frank-Wolfe (FW) optimization algorithm has lately re-gained popularity thanks in particular to its ability to nicely handle the structured constraints appearing in machine learning applications. However, its convergence rate is known to be slow (sublinear) when the solution lies at the boundary. A simple less-known fix is to add the possibility to take 'away steps' during optimization, an operation that importantly does not require a feasibility oracle. In this paper, we highlight and clarify several variants of the Frank-Wolfe optimization algorithm that have been successfully applied in practice: away-steps FW, pairwise FW, fully-corrective FW and Wolfe's minimum norm point algorithm, and prove for the first time that they all enjoy global linear convergence, under a weaker condition than strong convexity of the objective. The constant in the convergence rate has an elegant interpretation as the product of the (classical) condition number of the function with a novel geometric quantity that plays the role of a 'condition number' of the constraint set. We provide pointers to where these algorithms have made a difference in practice, in particular with the flow polytope, the marginal polytope and the base polytope for submodular optimization.},
  urldate = {2017-09-29},
  date = {2015-11-18},
  keywords = {Computer Science - Learning,Mathematics - Optimization and Control,Statistics - Machine Learning,G.1.6,I.2.6,90C52; 90C90; 68T05},
  author = {Lacoste-Julien, Simon and Jaggi, Martin},
  file = {/home/arthur/Dropbox/Zotero/Lacoste-Julien_Jaggi_2015_On the Global Linear Convergence of Frank-Wolfe Optimization Variants.pdf;/home/arthur/Zotero/storage/XQBLFFN8/1511.html}
}

@article{isola_image--image_2016,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1611.07004},
  primaryClass = {cs},
  title = {Image-to-{{Image Translation}} with {{Conditional Adversarial Networks}}},
  url = {http://arxiv.org/abs/1611.07004},
  abstract = {We investigate conditional adversarial networks as a general-purpose solution to image-to-image translation problems. These networks not only learn the mapping from input image to output image, but also learn a loss function to train this mapping. This makes it possible to apply the same generic approach to problems that traditionally would require very different loss formulations. We demonstrate that this approach is effective at synthesizing photos from label maps, reconstructing objects from edge maps, and colorizing images, among other tasks. As a community, we no longer hand-engineer our mapping functions, and this work suggests we can achieve reasonable results without hand-engineering our loss functions either.},
  urldate = {2017-09-28},
  date = {2016-11-21},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  author = {Isola, Phillip and Zhu, Jun-Yan and Zhou, Tinghui and Efros, Alexei A.},
  file = {/home/arthur/Dropbox/Zotero/Isola et al_2016_Image-to-Image Translation with Conditional Adversarial Networks.pdf;/home/arthur/Zotero/storage/TPF8MC53/1611.html}
}

@article{lacoste-julien_block-coordinate_2012,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1207.4747},
  primaryClass = {cs, math, stat},
  title = {Block-{{Coordinate Frank}}-{{Wolfe Optimization}} for {{Structural SVMs}}},
  url = {http://arxiv.org/abs/1207.4747},
  abstract = {We propose a randomized block-coordinate variant of the classic Frank-Wolfe algorithm for convex optimization with block-separable constraints. Despite its lower iteration cost, we show that it achieves a similar convergence rate in duality gap as the full Frank-Wolfe algorithm. We also show that, when applied to the dual structural support vector machine (SVM) objective, this yields an online algorithm that has the same low iteration complexity as primal stochastic subgradient methods. However, unlike stochastic subgradient methods, the block-coordinate Frank-Wolfe algorithm allows us to compute the optimal step-size and yields a computable duality gap guarantee. Our experiments indicate that this simple algorithm outperforms competing structural SVM solvers.},
  urldate = {2017-09-26},
  date = {2012-07-19},
  keywords = {Computer Science - Learning,Mathematics - Optimization and Control,Statistics - Machine Learning,90C52; 90C90; 90C06; 68T05,G.1.6,I.2.6},
  author = {Lacoste-Julien, Simon and Jaggi, Martin and Schmidt, Mark and Pletscher, Patrick},
  file = {/home/arthur/Dropbox/Zotero/Lacoste-Julien et al_2012_Block-Coordinate Frank-Wolfe Optimization for Structural SVMs.pdf}
}

@inproceedings{jaggi_revisiting_2013,
  title = {Revisiting {{Frank}}-{{Wolfe}}: {{Projection}}-{{Free Sparse Convex Optimization}}.},
  url = {http://www.jmlr.org/proceedings/papers/v28/jaggi13-supp.pdf},
  shorttitle = {Revisiting {{Frank}}-{{Wolfe}}},
  booktitle = {{{ICML}} (1)},
  urldate = {2017-09-26},
  date = {2013},
  pages = {427--435},
  author = {Jaggi, Martin},
  file = {/home/arthur/Dropbox/Zotero/Jaggi_2013_Revisiting Frank-Wolfe.pdf}
}

@article{ilic_entropy_2011,
  title = {Entropy {{Message Passing}}},
  volume = {57},
  issn = {0018-9448},
  doi = {10.1109/TIT.2010.2090235},
  abstract = {The paper proposes a new message passing algorithm for cycle-free factor graphs. The proposed "entropy message passing" (EMP) algorithm may be viewed as sum-product message passing over the entropy semiring, which has previously appeared in automata theory. The primary use of EMP is to compute the entropy of a model. However, EMP can also be used to compute expressions that appear in expectation maximization and in gradient-descent algorithms.},
  number = {1},
  journaltitle = {IEEE Transactions on Information Theory},
  date = {2011-01},
  pages = {375-380},
  keywords = {Signal processing algorithms,gradient methods,Hidden Markov models,Computational modeling,automata theory,Commutative semiring,cycle-free factor graphsg,entropy,Entropy,entropy message passing algorithm,entropy semiring,expectation maximization,expectation maximization algorithm,expectation-maximisation algorithm,factor graphs,gradient-descent algorithm,graph theory,graphical models,graphs,message passing,Message passing,Probabilistic logic,Sum product algorithm,sum-product message passing},
  author = {Ilic, V. M. and Stankovic, M. S. and Todorovic, B. T.},
  file = {/home/arthur/Dropbox/Zotero/Ilic et al_2011_Entropy Message Passing.pdf;/home/arthur/Zotero/storage/V8KTPEE8/5673956.html}
}

@inproceedings{vishwanathan_accelerated_2006,
  title = {Accelerated Training of Conditional Random Fields with Stochastic Gradient Methods},
  booktitle = {Proceedings of the 23rd International Conference on {{Machine}} Learning},
  publisher = {{ACM}},
  date = {2006},
  pages = {969--976},
  author = {Vishwanathan, S. V. N. and Schraudolph, Nicol N. and Schmidt, Mark W. and Murphy, Kevin P.},
  file = {/home/arthur/Dropbox/Zotero/Vishwanathan et al_2006_Accelerated training of conditional random fields with stochastic gradient.pdf}
}

@inproceedings{roth_integer_2005,
  title = {Integer Linear Programming Inference for Conditional Random Fields},
  booktitle = {Proceedings of the 22nd International Conference on {{Machine}} Learning},
  publisher = {{ACM}},
  date = {2005},
  pages = {736--743},
  author = {Roth, Dan and Yih, Wen-tau},
  file = {/home/arthur/Dropbox/Zotero/Roth_Yih_2005_Integer linear programming inference for conditional random fields.pdf}
}

@article{wainwright_new_2012,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1301.0610},
  primaryClass = {cs, stat},
  title = {A {{New Class}} of {{Upper Bounds}} on the {{Log Partition Function}}},
  url = {http://arxiv.org/abs/1301.0610},
  abstract = {Bounds on the log partition function are important in a variety of contexts, including approximate inference, model fitting, decision theory, and large deviations analysis. We introduce a new class of upper bounds on the log partition function, based on convex combinations of distributions in the exponential domain, that is applicable to an arbitrary undirected graphical model. In the special case of convex combinations of tree-structured distributions, we obtain a family of variational problems, similar to the Bethe free energy, but distinguished by the following desirable properties: i. they are cnvex, and have a unique global minimum; and ii. the global minimum gives an upper bound on the log partition function. The global minimum is defined by stationary conditions very similar to those defining fixed points of belief propagation or tree-based reparameterization Wainwright et al., 2001. As with BP fixed points, the elements of the minimizing argument can be used as approximations to the marginals of the original model. The analysis described here can be extended to structures of higher treewidth e.g., hypertrees, thereby making connections with more advanced approximations e.g., Kikuchi and variants Yedidia et al., 2001; Minka, 2001.},
  urldate = {2017-10-25},
  date = {2012-12-12},
  keywords = {Computer Science - Learning,Statistics - Machine Learning},
  author = {Wainwright, Martin and Jaakkola, Tommi S. and Willsky, Alan},
  file = {/home/arthur/Dropbox/Zotero/Wainwright et al_2012_A New Class of Upper Bounds on the Log Partition Function.pdf;/home/arthur/Zotero/storage/XMYEAPMM/1301.html}
}

@inproceedings{weiss_map_2007,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1206.5286},
  title = {{{MAP Estimation}}, {{Linear Programming}} and {{Belief Propagation}} with {{Convex Free Energies}}},
  url = {http://arxiv.org/abs/1206.5286},
  abstract = {Finding the most probable assignment (MAP) in a general graphical model is known to be NP hard but good approximations have been attained with max-product belief propagation (BP) and its variants. In particular, it is known that using BP on a single-cycle graph or tree reweighted BP on an arbitrary graph will give the MAP solution if the beliefs have no ties. In this paper we extend the setting under which BP can be used to provably extract the MAP. We define Convex BP as BP algorithms based on a convex free energy approximation and show that this class includes ordinary BP with single-cycle, tree reweighted BP and many other BP variants. We show that when there are no ties, fixed-points of convex max-product BP will provably give the MAP solution. We also show that convex sum-product BP at sufficiently small temperatures can be used to solve linear programs that arise from relaxing the MAP problem. Finally, we derive a novel condition that allows us to derive the MAP solution even if some of the convex BP beliefs have ties. In experiments, we show that our theorems allow us to find the MAP in many real-world instances of graphical models where exact inference using junction-tree is impossible.},
  booktitle = {Uncertainty in {{Artificial Intelligence}}},
  urldate = {2017-10-25},
  date = {2007},
  keywords = {Computer Science - Learning,Statistics - Machine Learning,Computer Science - Artificial Intelligence},
  author = {Weiss, Yair and Yanover, Chen and Meltzer, Talya},
  file = {/home/arthur/Dropbox/Zotero/Weiss et al_2007_MAP Estimation, Linear Programming and Belief Propagation with Convex Free.pdf;/home/arthur/Dropbox/Zotero/Weiss et al_2007_MAP Estimation, Linear Programming and Belief Propagation with Convex Free2.pdf;/home/arthur/Zotero/storage/B7UDKZ7Q/1206.html}
}

@article{wainwright_graphical_2008,
  title = {Graphical Models, Exponential Families, and Variational Inference},
  volume = {1},
  number = {1–2},
  journaltitle = {Foundations and Trends® in Machine Learning},
  date = {2008},
  pages = {1--305},
  author = {Wainwright, Martin J. and Jordan, Michael I.},
  file = {/home/arthur/Dropbox/Zotero/Wainwright_Jordan_2008_Graphical models, exponential families, and variational inference.pdf;/home/arthur/Zotero/storage/SZEVUIQU/MAL-001.html}
}

@unpublished{martins_linear_2014,
  venue = {{EMNLP}},
  title = {Linear {{Programming Decoders}} in {{NLP}}},
  url = {http://emnlp2014.org/tutorials/6_notes.pdf},
  urldate = {2017-10-25},
  date = {2014},
  author = {Martins, André},
  file = {/home/arthur/Dropbox/Zotero/Martins_2014_Linear Programming Decoders in NLP.pdf}
}

@incollection{jurafsky_hidden_2017,
  title = {Hidden {{Markov Models}}},
  url = {https://web.stanford.edu/~jurafsky/slp3/9.pdf},
  urldate = {2017-10-24},
  date = {2017},
  author = {Jurafsky, Daniel and Martins, James},
  file = {/home/arthur/Dropbox/Zotero/Jurafsky_Martins_2017_Hidden Markov Models.pdf}
}

@inproceedings{bloem_infinite_2014,
  title = {Infinite Time Horizon Maximum Causal Entropy Inverse Reinforcement Learning},
  doi = {10.1109/CDC.2014.7040156},
  abstract = {We extend the maximum causal entropy framework for inverse reinforcement learning to the infinite time horizon discounted reward setting. To do so, we maximize discounted future contributions to causal entropy subject to a discounted feature expectation matching constraint. A parameterized class of stochastic policies that solve this problem are referred to as soft Bellman policies because they can be specified in terms of values that satisfy an equation identical to the Bellman equation but with a softmax (the log of a sum of exponentials) instead of a max. Under some assumptions, algorithms that repeatedly solve for a soft Bellman policy, evaluate the policy, and then perform a gradient update on the parameters will find the optimal soft Bellman policy. For the first step, we extend techniques from dynamic programming and reinforcement learning so that they derive soft Bellman policies. For the second step, we can use policy evaluation techniques from dynamic programming or perform Monte Carlo simulations. We compare three algorithms of this type by applying them to a problem instance involving demonstration data from a simple controlled queuing network model inspired by problems in air traffic management.},
  eventtitle = {53rd {{IEEE Conference}} on {{Decision}} and {{Control}}},
  booktitle = {53rd {{IEEE Conference}} on {{Decision}} and {{Control}}},
  date = {2014-12},
  pages = {4911-4916},
  keywords = {Vectors,learning (artificial intelligence),stochastic processes,Context,Heuristic algorithms,Stochastic processes,dynamic programming,Dynamic programming,entropy,Entropy,air traffic,air traffic management,Bellman equation,controlled queuing network model,discounted feature expectation matching constraint,Finite element analysis,infinite time horizon discounted reward setting,inverse reinforcement learning,maximum causal entropy,Monte Carlo methods,Monte Carlo simulations,parameterized stochastic policies,policy evaluation techniques,soft Bellman policies},
  author = {Bloem, M. and Bambos, N.},
  file = {/home/arthur/Dropbox/Zotero/Bloem_Bambos_2014_Infinite time horizon maximum causal entropy inverse reinforcement learning.pdf;/home/arthur/Zotero/storage/G3X7JWKX/7040156.html}
}

@misc{bach_sum_2014,
  title = {Sum {{Product Algorithm}} and {{Hidden Markov Model}}},
  url = {http://imagine.enpc.fr/~obozinsg/teaching/mva_gm/lecture_notes/lecture7.pdf},
  urldate = {2017-10-24},
  date = {2014},
  author = {Bach, Francis},
  file = {/home/arthur/Dropbox/Zotero/Bach_2014_Sum Product Algorithm and Hidden Markov Model.pdf}
}

@thesis{passos_combinatorial_2013,
  title = {Combinatorial Algorithms and Linear Programming for Inference in Natural Language Processing},
  date = {2013},
  author = {Passos, Alexandre Tachard},
  file = {/home/arthur/Dropbox/Zotero/Passos_2013_Combinatorial algorithms and linear programming for inference in natural.pdf;/home/arthur/Zotero/storage/G83LHCSW/my-thesis-combinatorial-algorithms-and-linear.html}
}

@article{neu_unified_2017,
  title = {A Unified View of Entropy-Regularized {{Markov}} Decision Processes},
  journaltitle = {arXiv preprint arXiv:1705.07798},
  date = {2017},
  author = {Neu, Gergely and Jonsson, Anders and Gómez, Vicenç},
  file = {/home/arthur/Dropbox/Zotero/Neu et al_2017_A unified view of entropy-regularized Markov decision processes.pdf}
}

@unpublished{wainwright_marginal_2008,
  title = {Marginal Polytopes of Graphical Models: {{Linear}} Programs, Max-Product, and Variational Relaxation},
  url = {http://www.maths.dur.ac.uk/events/Meetings/LMS/2008/MAGM/Talks/wainwright.pdf},
  urldate = {2017-10-27},
  date = {2008},
  author = {Wainwright, Martin},
  file = {/home/arthur/Dropbox/Zotero/Wainwright_2008_Marginal polytopes of graphical models.pdf}
}

@article{domke_learning_2013,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1301.3193},
  title = {Learning {{Graphical Model Parameters}} with {{Approximate Marginal Inference}}},
  volume = {35},
  issn = {0162-8828, 2160-9292},
  url = {http://arxiv.org/abs/1301.3193},
  abstract = {Likelihood based-learning of graphical models faces challenges of computational-complexity and robustness to model mis-specification. This paper studies methods that fit parameters directly to maximize a measure of the accuracy of predicted marginals, taking into account both model and inference approximations at training time. Experiments on imaging problems suggest marginalization-based learning performs better than likelihood-based approximations on difficult problems where the model being fit is approximate in nature.},
  number = {10},
  journaltitle = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  urldate = {2017-10-27},
  date = {2013-10},
  pages = {2454-2467},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Learning,I.2.6,I.4.8},
  author = {Domke, Justin},
  file = {/home/arthur/Dropbox/Zotero/Domke_2013_Learning Graphical Model Parameters with Approximate Marginal Inference.pdf;/home/arthur/Zotero/storage/DLHI4PSV/1301.html}
}

@inproceedings{domke_implicit_2010,
  title = {Implicit Differentiation by Perturbation},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  date = {2010},
  pages = {523--531},
  author = {Domke, Justin},
  file = {/home/arthur/Dropbox/Zotero/Domke_2010_Implicit differentiation by perturbation.pdf}
}

@inproceedings{eaton_choosing_2009,
  langid = {english},
  title = {Choosing a {{Variable}} to {{Clamp}}},
  url = {http://proceedings.mlr.press/v5/eaton09a.html},
  abstract = {In this paper we propose an algorithm for  approximate inference on graphical models  based on belief propagation (BP). Our algorithm  is an approximate version of Cutset  Conditioning, in which a ...},
  eventtitle = {Artificial {{Intelligence}} and {{Statistics}}},
  booktitle = {{{PMLR}}},
  urldate = {2017-10-27},
  date = {2009-04-15},
  pages = {145-152},
  author = {Eaton, Frederik and Ghahramani, Zoubin},
  file = {/home/arthur/Dropbox/Zotero/Eaton_Ghahramani_2009_Choosing a Variable to Clamp.pdf;/home/arthur/Zotero/storage/27P67NQJ/eaton09a.html}
}

@inproceedings{yanover_finding_2004,
  title = {Finding the {{M}} Most Probable Configurations Using Loopy Belief Propagation},
  booktitle = {Advances in Neural Information Processing Systems},
  date = {2004},
  pages = {289--296},
  author = {Yanover, Chen and Weiss, Yair},
  file = {/home/arthur/Dropbox/Zotero/Yanover_Weiss_2004_Finding the M most probable configurations using loopy belief propagation.pdf}
}

@article{daskalakis_probabilistic_2008,
  title = {Probabilistic {{Analysis}} of {{Linear Programming Decoding}}},
  volume = {54},
  issn = {0018-9448},
  url = {http://ieeexplore.ieee.org/document/4567569/},
  number = {8},
  journaltitle = {IEEE Transactions on Information Theory},
  urldate = {2017-10-27},
  date = {2008-08},
  pages = {3565-3578},
  author = {Daskalakis, C. and Dimakis, A.G. and Karp, R.M. and Wainwright, M.J.},
  file = {/home/arthur/Dropbox/Zotero/Daskalakis et al_2008_Probabilistic Analysis of Linear Programming Decoding.pdf}
}

@article{ashburner_fast_2007,
  title = {A Fast Diffeomorphic Image Registration Algorithm},
  volume = {38},
  number = {1},
  journaltitle = {Neuroimage},
  date = {2007},
  pages = {95--113},
  author = {Ashburner, John},
  file = {/home/arthur/Dropbox/Zotero/Ashburner_2007_A fast diffeomorphic image registration algorithm.pdf;/home/arthur/Zotero/storage/SLQYTY4Y/S1053811907005848.html}
}

@article{gorgolewski_brain_2016,
  langid = {english},
  title = {The Brain Imaging Data Structure, a Format for Organizing and Describing Outputs of Neuroimaging Experiments},
  volume = {3},
  issn = {2052-4463},
  url = {https://www.nature.com/articles/sdata201644},
  abstract = {Article},
  journaltitle = {Scientific Data},
  urldate = {2017-10-27},
  date = {2016-06-21},
  pages = {sdata201644},
  author = {Gorgolewski, Krzysztof J. and Auer, Tibor and Calhoun, Vince D. and Craddock, R. Cameron and Das, Samir and Duff, Eugene P. and Flandin, Guillaume and Ghosh, Satrajit S. and Glatard, Tristan and Halchenko, Yaroslav O. and Handwerker, Daniel A. and Hanke, Michael and Keator, David and Li, Xiangrui and Michael, Zachary and Maumet, Camille and Nichols, B. Nolan and Nichols, Thomas E. and Pellman, John and Poline, Jean-Baptiste and Rokem, Ariel and Schaefer, Gunnar and Sochat, Vanessa and Triplett, William and Turner, Jessica A. and Varoquaux, Gaël and Poldrack, Russell A.},
  file = {/home/arthur/Dropbox/Zotero/Gorgolewski et al_2016_The brain imaging data structure, a format for organizing and describing.pdf;/home/arthur/Zotero/storage/MXCYMHNV/sdata201644.html}
}

@article{locatello_unified_2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1702.06457},
  primaryClass = {cs, stat},
  title = {A {{Unified Optimization View}} on {{Generalized Matching Pursuit}} and {{Frank}}-{{Wolfe}}},
  url = {http://arxiv.org/abs/1702.06457},
  abstract = {Two of the most fundamental prototypes of greedy optimization are the matching pursuit and Frank-Wolfe algorithms. In this paper, we take a unified view on both classes of methods, leading to the first explicit convergence rates of matching pursuit methods in an optimization sense, for general sets of atoms. We derive sublinear (\$1/t\$) convergence for both classes on general smooth objectives, and linear convergence on strongly convex objectives, as well as a clear correspondence of algorithm variants. Our presented algorithms and rates are affine invariant, and do not need any incoherence or sparsity assumptions.},
  urldate = {2017-10-30},
  date = {2017-02-21},
  keywords = {Computer Science - Learning,Statistics - Machine Learning},
  author = {Locatello, Francesco and Khanna, Rajiv and Tschannen, Michael and Jaggi, Martin},
  file = {/home/arthur/Dropbox/Zotero/Locatello et al_2017_A Unified Optimization View on Generalized Matching Pursuit and Frank-Wolfe.pdf;/home/arthur/Zotero/storage/VR5WQPVW/1702.html}
}

@article{locatello_greedy_2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1705.11041},
  primaryClass = {cs, stat},
  title = {Greedy {{Algorithms}} for {{Cone Constrained Optimization}} with {{Convergence Guarantees}}},
  url = {http://arxiv.org/abs/1705.11041},
  abstract = {Greedy optimization methods such as Matching Pursuit (MP) and Frank-Wolfe (FW) algorithms regained popularity in recent years due to their simplicity, effectiveness and theoretical guarantees. MP and FW address optimization over the linear span and the convex hull of a set of atoms, respectively. In this paper, we consider the intermediate case of optimization over the convex cone, parametrized as the conic hull of a generic atom set, leading to the first principled definitions of non-negative MP algorithms for which we give explicit convergence rates and demonstrate excellent empirical performance. In particular, we derive sublinear (\$$\backslash$mathcal\{O\}(1/t)\$) convergence on general smooth and convex objectives, and linear convergence (\$$\backslash$mathcal\{O\}(e\^\{-t\})\$) on strongly convex objectives, in both cases for general sets of atoms. Furthermore, we establish a clear correspondence of our algorithms to known algorithms from the MP and FW literature. Our novel algorithms and analyses target general atom sets and general objective functions, and hence are directly applicable to a large variety of learning settings.},
  urldate = {2017-10-30},
  date = {2017-05-31},
  keywords = {Computer Science - Learning,Statistics - Machine Learning},
  author = {Locatello, Francesco and Tschannen, Michael and Rätsch, Gunnar and Jaggi, Martin},
  file = {/home/arthur/Dropbox/Zotero/Locatello et al_2017_Greedy Algorithms for Cone Constrained Optimization with Convergence Guarantees.pdf;/home/arthur/Zotero/storage/4KUNMD2F/1705.html}
}

@article{gormley_approximation-aware_2015,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1508.02375},
  primaryClass = {cs},
  title = {Approximation-{{Aware Dependency Parsing}} by {{Belief Propagation}}},
  url = {http://arxiv.org/abs/1508.02375},
  abstract = {We show how to train the fast dependency parser of Smith and Eisner (2008) for improved accuracy. This parser can consider higher-order interactions among edges while retaining O(n\^3) runtime. It outputs the parse with maximum expected recall -- but for speed, this expectation is taken under a posterior distribution that is constructed only approximately, using loopy belief propagation through structured factors. We show how to adjust the model parameters to compensate for the errors introduced by this approximation, by following the gradient of the actual loss on training data. We find this gradient by back-propagation. That is, we treat the entire parser (approximations and all) as a differentiable circuit, as Stoyanov et al. (2011) and Domke (2010) did for loopy CRFs. The resulting trained parser obtains higher accuracy with fewer iterations of belief propagation than one trained by conditional log-likelihood.},
  urldate = {2017-10-30},
  date = {2015-08-10},
  keywords = {Computer Science - Learning,Computer Science - Computation and Language},
  author = {Gormley, Matthew R. and Dredze, Mark and Eisner, Jason},
  file = {/home/arthur/Dropbox/Zotero/Gormley et al_2015_Approximation-Aware Dependency Parsing by Belief Propagation.pdf;/home/arthur/Zotero/storage/FMALD7DX/1508.html}
}

@inproceedings{stoyanov_empirical_2011,
  title = {Empirical Risk Minimization of Graphical Model Parameters given Approximate Inference, Decoding, and Model Structure},
  booktitle = {Proceedings of the {{Fourteenth International Conference}} on {{Artificial Intelligence}} and {{Statistics}}},
  date = {2011},
  pages = {725--733},
  author = {Stoyanov, Veselin and Ropson, Alexander and Eisner, Jason},
  file = {/home/arthur/Dropbox/Zotero/Stoyanov et al_2011_Empirical risk minimization of graphical model parameters given approximate.pdf;/home/arthur/Dropbox/Zotero/Stoyanov et al_2011_Empirical risk minimization of graphical model parameters given approximate2.pdf}
}

@article{martins_softmax_2016,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1602.02068},
  title = {From {{Softmax}} to {{Sparsemax}}: {{A Sparse Model}} of {{Attention}} and {{Multi}}-{{Label Classification}}},
  shorttitle = {From {{Softmax}} to {{Sparsemax}}},
  abstract = {We propose sparsemax, a new activation function similar to the traditional softmax, but able to output sparse probabilities. After deriving its properties, we show how its Jacobian can be efficiently computed, enabling its use in a network trained with backpropagation. Then, we propose a new smooth and convex loss function which is the sparsemax analogue of the logistic loss. We reveal an unexpected connection between this new loss and the Huber classification loss. We obtain promising empirical results in multi-label classification problems and in attention-based neural networks for natural language inference. For the latter, we achieve a similar performance as the traditional softmax, but with a selective, more compact, attention focus.},
  journaltitle = {Proc. ICML},
  date = {2016},
  pages = {1614-1623},
  keywords = {Computer Science - Learning,Statistics - Machine Learning,Computer Science - Computation and Language},
  author = {Martins, André F. T. and Astudillo, Ramón Fernandez},
  file = {/home/arthur/Dropbox/Zotero/Martins_Astudillo_2016_From Softmax to Sparsemax.pdf;/home/arthur/Zotero/storage/Z9W9R7MU/1602.html}
}

@article{ravikumar_message-passing_2010,
  title = {Message-Passing for Graph-Structured Linear Programs: {{Proximal}} Methods and Rounding Schemes},
  volume = {11},
  shorttitle = {Message-Passing for Graph-Structured Linear Programs},
  issue = {Mar},
  journaltitle = {Journal of Machine Learning Research},
  date = {2010},
  pages = {1043--1080},
  author = {Ravikumar, Pradeep and Agarwal, Alekh and Wainwright, Martin J.},
  file = {/home/arthur/Dropbox/Zotero/Ravikumar et al_2010_Message-passing for graph-structured linear programs.pdf}
}

@article{weiss_map_2012,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1206.5286},
  primaryClass = {cs, stat},
  title = {{{MAP Estimation}}, {{Linear Programming}} and {{Belief Propagation}} with {{Convex Free Energies}}},
  url = {http://arxiv.org/abs/1206.5286},
  abstract = {Finding the most probable assignment (MAP) in a general graphical model is known to be NP hard but good approximations have been attained with max-product belief propagation (BP) and its variants. In particular, it is known that using BP on a single-cycle graph or tree reweighted BP on an arbitrary graph will give the MAP solution if the beliefs have no ties. In this paper we extend the setting under which BP can be used to provably extract the MAP. We define Convex BP as BP algorithms based on a convex free energy approximation and show that this class includes ordinary BP with single-cycle, tree reweighted BP and many other BP variants. We show that when there are no ties, fixed-points of convex max-product BP will provably give the MAP solution. We also show that convex sum-product BP at sufficiently small temperatures can be used to solve linear programs that arise from relaxing the MAP problem. Finally, we derive a novel condition that allows us to derive the MAP solution even if some of the convex BP beliefs have ties. In experiments, we show that our theorems allow us to find the MAP in many real-world instances of graphical models where exact inference using junction-tree is impossible.},
  urldate = {2017-10-31},
  date = {2012-06-20},
  keywords = {Computer Science - Learning,Statistics - Machine Learning,Computer Science - Artificial Intelligence},
  author = {Weiss, Yair and Yanover, Chen and Meltzer, Talya},
  file = {/home/arthur/Dropbox/Zotero/Weiss et al_2012_MAP Estimation, Linear Programming and Belief Propagation with Convex Free.pdf;/home/arthur/Zotero/storage/UN5ISGW8/1206.html}
}

@article{wainwright_map_2005,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {cs/0508070},
  title = {{{MAP}} Estimation via Agreement on (Hyper)Trees: {{Message}}-Passing and Linear Programming},
  url = {http://arxiv.org/abs/cs/0508070},
  shorttitle = {{{MAP}} Estimation via Agreement on (Hyper)Trees},
  abstract = {We develop and analyze methods for computing provably optimal \{$\backslash$em maximum a posteriori\} (MAP) configurations for a subclass of Markov random fields defined on graphs with cycles. By decomposing the original distribution into a convex combination of tree-structured distributions, we obtain an upper bound on the optimal value of the original problem (i.e., the log probability of the MAP assignment) in terms of the combined optimal values of the tree problems. We prove that this upper bound is tight if and only if all the tree distributions share an optimal configuration in common. An important implication is that any such shared configuration must also be a MAP configuration for the original distribution. Next we develop two approaches to attempting to obtain tight upper bounds: (a) a \{$\backslash$em tree-relaxed linear program\} (LP), which is derived from the Lagrangian dual of the upper bounds; and (b) a \{$\backslash$em tree-reweighted max-product message-passing algorithm\} that is related to but distinct from the max-product algorithm. In this way, we establish a connection between a certain LP relaxation of the mode-finding problem, and a reweighted form of the max-product (min-sum) message-passing algorithm.},
  urldate = {2017-10-31},
  date = {2005-08-15},
  keywords = {Computer Science - Information Theory,Computer Science - Artificial Intelligence},
  author = {Wainwright, Martin J. and Jaakkola, Tommi S. and Willsky, Alan S.},
  file = {/home/arthur/Dropbox/Zotero/Wainwright et al_2005_MAP estimation via agreement on (hyper)trees.pdf;/home/arthur/Zotero/storage/3KGMBZGC/0508070.html}
}

@inproceedings{huang_advanced_2008,
  title = {Advanced Dynamic Programming in Semiring and Hypergraph Frameworks},
  booktitle = {Proceedings of {{COLING}}},
  date = {2008},
  pages = {1--18},
  author = {Huang, Liang},
  file = {/home/arthur/Dropbox/Zotero/Huang_2008_Advanced dynamic programming in semiring and hypergraph frameworks.pdf;/home/arthur/Zotero/storage/8NEAFZZV/can-dynamic-programming-problems-always-be-represented-as-dag.html}
}

@article{eppstein_finding_1998,
  title = {Finding the k {{Shortest Paths}}},
  volume = {28},
  issn = {0097-5397},
  url = {http://epubs.siam.org/doi/10.1137/S0097539795290477},
  abstract = {We give algorithms for finding the k shortest paths (not required to be simple) connecting a pair of vertices in a digraph. Our algorithms output an implicit representation of these paths in a digraph with n vertices and m edges, in time O(m + n log n + k). We can also find the k shortest paths from a given source s to each vertex in the graph, in total time O(m + n log n + kn). We describe applications to dynamic programming problems including the knapsack problem, sequence alignment, maximum inscribed polygons, and genealogical relationship discovery.},
  number = {2},
  journaltitle = {SIAM Journal on Computing},
  shortjournal = {SIAM J. Comput.},
  urldate = {2017-11-02},
  date = {1998-01-01},
  pages = {652-673},
  author = {Eppstein, D.},
  file = {/home/arthur/Dropbox/Zotero/Eppstein_1998_Finding the k Shortest Paths.pdf;/home/arthur/Zotero/storage/5KWPWT38/S0097539795290477.html}
}

@article{seshadri_list_1994,
  title = {List {{Viterbi Decoding}} with {{Applications}}},
  volume = {42},
  url = {http://www2.ensc.sfu.ca/people/faculty/cavers/ENSC805/readings/42comm02-seshadri.pdf},
  number = {2},
  journaltitle = {IEEE Transactions on Communications},
  urldate = {2017-11-02},
  date = {1994},
  pages = {313-323},
  author = {Seshadri, Nambijaran and Sundberg, Carl-Erik W.},
  file = {/home/arthur/Dropbox/Zotero/Seshadri_Sundberg_1994_List Viterbi Decoding with Applications.pdf}
}

@article{aljazzar_k_2011,
  title = {K⁎: {{A}} Heuristic Search Algorithm for Finding the k Shortest Paths},
  volume = {175},
  issn = {0004-3702},
  url = {http://www.sciencedirect.com/science/article/pii/S0004370211000865},
  shorttitle = {K⁎},
  abstract = {We present a directed search algorithm, called K⁎, for finding the k shortest paths between a designated pair of vertices in a given directed weighted graph. K⁎ has two advantages compared to current k-shortest-paths algorithms. First, K⁎ operates on-the-fly, which means that it does not require the graph to be explicitly available and stored in main memory. Portions of the graph will be generated as needed. Second, K⁎ can be guided using heuristic functions. We prove the correctness of K⁎ and determine its asymptotic worst-case complexity when using a consistent heuristic to be the same as the state of the art, O(m+nlogn+k), with respect to both runtime and space, where n is the number of vertices and m is the number of edges of the graph. We present an experimental evaluation of K⁎ by applying it to route planning problems as well as counterexample generation for stochastic model checking. The experimental results illustrate that due to the use of heuristic, on-the-fly search K⁎ can use less time and memory compared to the most efficient k-shortest-paths algorithms known so far.},
  number = {18},
  journaltitle = {Artificial Intelligence},
  shortjournal = {Artificial Intelligence},
  urldate = {2017-11-02},
  date = {2011-12-01},
  pages = {2129-2154},
  keywords = {Heuristic search,K,k-Shortest-paths problem,On-the-fly search},
  author = {Aljazzar, Husain and Leue, Stefan},
  file = {/home/arthur/Dropbox/Zotero/Aljazzar_Leue_2011_K⁎.pdf;/home/arthur/Zotero/storage/INYAFDDZ/S0004370211000865.html}
}

@thesis{weller_methods_2014,
  title = {Methods for Inference in Graphical Models},
  institution = {{Columbia University}},
  date = {2014},
  author = {Weller, Adrian},
  file = {/home/arthur/Dropbox/Zotero/Weller_2014_Methods for inference in graphical models.pdf}
}

@online{ruozzi_message_2011,
  title = {Message {{Passing Algorithms}} for {{Optimization}}},
  url = {http://www.utdallas.edu/~nrr150130/Papers/thesis.pdf},
  urldate = {2017-11-14},
  date = {2011},
  author = {Ruozzi, Nicholas Robert},
  file = {/home/arthur/Dropbox/Zotero/Ruozzi_2011_Message Passing Algorithms for Optimization.pdf}
}

@article{gower_new_2012,
  title = {A New Framework for the Computation of {{Hessians}}},
  volume = {27},
  issn = {1055-6788},
  url = {http://dx.doi.org/10.1080/10556788.2011.580098},
  abstract = {We investigate the computation of Hessian matrices via Automatic Differentiation, using a graph model and an algebraic model. The graph model reveals the inherent symmetries involved in calculating the Hessian. The algebraic model, based on Griewank and Walther's [Evaluating derivatives, in Principles and Techniques of Algorithmic Differentiation, 2nd ed., Society for Industrial and Applied Mathematics (SIAM), Philadelphia, PA, 2008] state transformations synthesizes the calculation of the Hessian as a formula. These dual points of view, graphical and algebraic, lead to a new framework for Hessian computation. This is illustrated by developing edge\_pushing, a new truly reverse Hessian computation algorithm that fully exploits the Hessian's symmetry. Computational experiments compare the performance of edge\_pushing on 16 functions from the CUTE collection [I. Bongartz et al. Cute: constrained and unconstrained testing environment, ACM Trans. Math. Softw. 21(1) (1995), pp. 123–160] against two algorithms available as drivers of the software ADOL-C [A. Griewank et al. ADOL-C: A package for the automatic differentiation of algorithms written in C/C++, Technical report, Institute of Scientific Computing, Technical University Dresden, 1999. Updated version of the paper published in ACM Trans. Math. Softw. 22, 1996, pp. 131–167; A. Walther, Computing sparse Hessians with automatic differentiation, ACM Trans. Math. Softw. 34(1) (2008), pp. 1–15; A.H. Gebremedhin et al. Efficient computation of sparse Hessians using coloring and automatic differentiation, INFORMS J. Comput. 21(2) (2009), pp. 209–223], and the results are very promising.},
  number = {2},
  journaltitle = {Optimization Methods and Software},
  urldate = {2017-11-14},
  date = {2012-04-01},
  pages = {251-273},
  keywords = {65D25,65F50,65K10,automatic differentiation,Hessian matrix,reverse computation,sparse matrices},
  author = {Gower, R. M. and Mello, M. P.},
  file = {/home/arthur/Dropbox/Zotero/Gower_Mello_2012_A new framework for the computation of Hessians.pdf;/home/arthur/Zotero/storage/MSDNPCGJ/10556788.2011.html}
}

@inproceedings{krichene_efficient_2015,
  title = {Efficient {{Bregman}} Projections onto the Simplex},
  doi = {10.1109/CDC.2015.7402714},
  abstract = {We consider the problem of projecting a vector onto the simplex Δ = x ∈ ℝ+d : Σi=1d xi = 1, using a Bregman projection. This is a common problem in first-order methods for convex optimization and online-learning algorithms, such as mirror descent. We derive the KKT conditions of the projection problem, and show that for Bregman divergences induced by ω-potentials, one can efficiently compute the solution using a bisection method. More precisely, an ω-approximate projection can be obtained in O(d log 1/ω). We also consider a class of exponential potentials for which the exact solution can be computed efficiently, and give a O(d log d) deterministic algorithm and O(d) randomized algorithm to compute the projection. In particular, we show that one can generalize the KL divergence to a Bregman divergence which is bounded on the simplex (unlike the KL divergence), strongly convex with respect to the ℓ1 norm, and for which one can still solve the projection in expected linear time.},
  eventtitle = {2015 54th {{IEEE Conference}} on {{Decision}} and {{Control}} ({{CDC}})},
  booktitle = {2015 54th {{IEEE Conference}} on {{Decision}} and {{Control}} ({{CDC}})},
  date = {2015-12},
  pages = {3291-3298},
  keywords = {Approximation algorithms,computational complexity,Algorithm design and analysis,bisection method,Bregman divergences,Bregman projections,Complexity theory,Computers,Convex functions,convex optimization,deterministic algorithms,exponential potentials,Games,KKT conditions,KL divergence,mirror descent,Mirrors,O(d log d) deterministic algorithm,O(d) randomized algorithm,online-learning algorithms,randomised algorithms,simplex,vector,vectors,ω-approximate projection,ω-potentials},
  author = {Krichene, W. and Krichene, S. and Bayen, A.},
  file = {/home/arthur/Dropbox/Zotero/Krichene et al_2015_Efficient Bregman projections onto the simplex.pdf;/home/arthur/Zotero/storage/STYRE6XJ/7402714.html}
}

@article{sniedovich_dijkstras_2006,
  title = {Dijkstra’s Algorithm Revisited: The Dynamic Programming Connexion},
  volume = {35},
  url = {http://matwbn.icm.edu.pl/ksiazki/cc/cc35/cc3536.pdf},
  number = {3},
  journaltitle = {Control and Cybernetics},
  urldate = {2017-11-08},
  date = {2006},
  author = {Sniedovich, Moshe},
  file = {/home/arthur/Dropbox/Zotero/Sniedovich_2006_Dijkstra’s algorithm revisited.pdf}
}

@article{lauritzen_local_1988,
  title = {Local Computations with Probabilities on Graphical Structures and Their Application to Expert Systems},
  journaltitle = {Journal of the Royal Statistical Society. Series B (Methodological)},
  date = {1988},
  pages = {157--224},
  author = {Lauritzen, Steffen L. and Spiegelhalter, David J.},
  file = {/home/arthur/Dropbox/Zotero/Lauritzen_Spiegelhalter_1988_Local computations with probabilities on graphical structures and their.pdf;/home/arthur/Zotero/storage/QH26WC5S/2345762.html}
}

@book{pouly_generic_2011,
  location = {{Hoboken, New Jersey}},
  title = {Generic {{Inference}}: {{A Unifying Theory}} for {{Automated Reasoning}}},
  isbn = {978-0-470-52701-6 978-1-118-01087-7},
  shorttitle = {Generic {{Inference}}},
  pagetotal = {452},
  publisher = {{Wiley}},
  date = {2011},
  keywords = {Algorithms,Algebra; Abstract,TECHNOLOGY & ENGINEERING / Electronics / General,Valuation theory},
  author = {Pouly, Marc and Kohlas, Jürg},
  file = {/home/arthur/Dropbox/Zotero/Pouly_Kohlas_2011_Generic Inference.pdf}
}

@article{pearlmutter_fast_1994,
  title = {Fast Exact Multiplication by the {{Hessian}}},
  volume = {6},
  number = {1},
  journaltitle = {Neural computation},
  date = {1994},
  pages = {147--160},
  author = {Pearlmutter, Barak A.},
  file = {/home/arthur/Dropbox/Zotero/Pearlmutter_1994_Fast exact multiplication by the Hessian.pdf;/home/arthur/Zotero/storage/NN2A35R5/neco.1994.6.1.html}
}

@online{gormley_belief_2016,
  title = {Belief Propagation Tutorial},
  url = {http://www.cs.cmu.edu/~mgormley/bp-tutorial/},
  urldate = {2017-11-28},
  date = {2016},
  author = {Gormley, Matt},
  file = {/home/arthur/Zotero/storage/6MHXLLJX/bp-tutorial.html}
}

@inproceedings{eisner_compiling_2005,
  title = {Compiling {{Comp Ling}}: {{Practical}} Weighted Dynamic Programming and the {{Dyna}} Language},
  shorttitle = {Compiling {{Comp Ling}}},
  booktitle = {Proceedings of the Conference on {{Human Language Technology}} and {{Empirical Methods}} in {{Natural Language Processing}}},
  publisher = {{Association for Computational Linguistics}},
  date = {2005},
  pages = {281--290},
  author = {Eisner, Jason and Goldlust, Eric and Smith, Noah A.},
  file = {/home/arthur/Dropbox/Zotero/Eisner et al_2005_Compiling Comp Ling.pdf}
}

@article{collins_inside-outside_2013,
  title = {The {{Inside}}-{{Outside Algorithm}}},
  journaltitle = {Lecture Notes},
  date = {2013},
  author = {Collins, Michael},
  file = {/home/arthur/Dropbox/Zotero/Collins_2013_The Inside-Outside Algorithm.pdf}
}

@article{nesterov_smooth_2005,
  langid = {english},
  title = {Smooth Minimization of Non-Smooth Functions},
  volume = {103},
  issn = {0025-5610, 1436-4646},
  url = {http://link.springer.com/10.1007/s10107-004-0552-5},
  number = {1},
  journaltitle = {Mathematical Programming},
  urldate = {2017-11-29},
  date = {2005-05},
  pages = {127-152},
  author = {Nesterov, Yu.},
  file = {/home/arthur/Dropbox/Zotero/Nesterov_2005_Smooth minimization of non-smooth functions.pdf}
}

@article{sun_structure_2014,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1411.6243},
  primaryClass = {cs},
  title = {Structure {{Regularization}} for {{Structured Prediction}}: {{Theories}} and {{Experiments}}},
  url = {http://arxiv.org/abs/1411.6243},
  shorttitle = {Structure {{Regularization}} for {{Structured Prediction}}},
  abstract = {While there are many studies on weight regularization, the study on structure regularization is rare. Many existing systems on structured prediction focus on increasing the level of structural dependencies within the model. However, this trend could have been misdirected, because our study suggests that complex structures are actually harmful to generalization ability in structured prediction. To control structure-based overfitting, we propose a structure regularization framework via $\backslash$emph\{structure decomposition\}, which decomposes training samples into mini-samples with simpler structures, deriving a model with better generalization power. We show both theoretically and empirically that structure regularization can effectively control overfitting risk and lead to better accuracy. As a by-product, the proposed method can also substantially accelerate the training speed. The method and the theoretical results can apply to general graphical models with arbitrary structures. Experiments on well-known tasks demonstrate that our method can easily beat the benchmark systems on those highly-competitive tasks, achieving state-of-the-art accuracies yet with substantially faster training speed.},
  urldate = {2017-11-30},
  date = {2014-11-23},
  keywords = {Computer Science - Learning},
  author = {Sun, Xu},
  file = {/home/arthur/Dropbox/Zotero/Sun_2014_Structure Regularization for Structured Prediction.pdf;/home/arthur/Zotero/storage/WLWGDAR6/1411.html}
}

@inproceedings{stoyanov_minimum-risk_2012,
  title = {Minimum-Risk Training of Approximate {{CRF}}-Based {{NLP}} Systems},
  booktitle = {Proceedings of the 2012 {{Conference}} of the {{North American Chapter}} of the {{Association}} for {{Computational Linguistics}}: {{Human Language Technologies}}},
  publisher = {{Association for Computational Linguistics}},
  date = {2012},
  pages = {120--130},
  author = {Stoyanov, Veselin and Eisner, Jason},
  file = {/home/arthur/Dropbox/Zotero/Stoyanov_Eisner_2012_Minimum-risk training of approximate CRF-based NLP systems.pdf}
}

@article{bronstein_geometric_2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1611.08097},
  title = {Geometric Deep Learning: Going beyond {{Euclidean}} Data},
  volume = {34},
  issn = {1053-5888},
  url = {http://arxiv.org/abs/1611.08097},
  shorttitle = {Geometric Deep Learning},
  abstract = {Many scientific fields study data with an underlying structure that is a non-Euclidean space. Some examples include social networks in computational social sciences, sensor networks in communications, functional networks in brain imaging, regulatory networks in genetics, and meshed surfaces in computer graphics. In many applications, such geometric data are large and complex (in the case of social networks, on the scale of billions), and are natural targets for machine learning techniques. In particular, we would like to use deep neural networks, which have recently proven to be powerful tools for a broad range of problems from computer vision, natural language processing, and audio analysis. However, these tools have been most successful on data with an underlying Euclidean or grid-like structure, and in cases where the invariances of these structures are built into networks used to model them. Geometric deep learning is an umbrella term for emerging techniques attempting to generalize (structured) deep neural models to non-Euclidean domains such as graphs and manifolds. The purpose of this paper is to overview different examples of geometric deep learning problems and present available solutions, key difficulties, applications, and future research directions in this nascent field.},
  number = {4},
  journaltitle = {IEEE Signal Processing Magazine},
  urldate = {2017-12-06},
  date = {2017-07},
  pages = {18-42},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  author = {Bronstein, Michael M. and Bruna, Joan and LeCun, Yann and Szlam, Arthur and Vandergheynst, Pierre},
  file = {/home/arthur/Dropbox/Zotero/Bronstein et al_2017_Geometric deep learning.pdf;/home/arthur/Zotero/storage/UV2XHITD/1611.html}
}

@incollection{djolonga_differentiable_2017,
  title = {Differentiable {{Learning}} of {{Submodular Functions}}},
  url = {http://papers.nips.cc/paper/6702-differentiable-learning-of-submodular-functions.pdf},
  booktitle = {Advances in {{Neural Information Processing Systems}} 30},
  publisher = {{Curran Associates, Inc.}},
  urldate = {2017-12-07},
  date = {2017},
  pages = {1014--1024},
  author = {Djolonga, Josip and Krause, Andreas},
  editor = {Guyon, I. and Luxburg, U. V. and Bengio, S. and Wallach, H. and Fergus, R. and Vishwanathan, S. and Garnett, R.},
  file = {/home/arthur/Dropbox/Zotero/Djolonga_Krause_2017_Differentiable Learning of Submodular Functions.pdf;/home/arthur/Zotero/storage/WK39IS6W/6702-differentiable-learning-of-submodular-functions.html}
}

@article{lundberg_unified_2017,
  title = {A {{Unified Approach}} to {{Interpreting Model Predictions}}},
  url = {https://arxiv.org/abs/1705.07874},
  urldate = {2017-12-07},
  date = {2017-05-22},
  author = {Lundberg, Scott and Lee, Su-In},
  file = {/home/arthur/Dropbox/Zotero/Lundberg_Lee_2017_A Unified Approach to Interpreting Model Predictions.pdf;/home/arthur/Zotero/storage/FYA2QNEP/1705.html}
}

@article{gunasekar_implicit_2017,
  title = {Implicit {{Regularization}} in {{Matrix Factorization}}},
  url = {https://arxiv.org/abs/1705.09280},
  urldate = {2017-12-07},
  date = {2017-05-25},
  author = {Gunasekar, Suriya and Woodworth, Blake and Bhojanapalli, Srinadh and Neyshabur, Behnam and Srebro, Nathan},
  file = {/home/arthur/Dropbox/Zotero/Gunasekar et al_2017_Implicit Regularization in Matrix Factorization.pdf;/home/arthur/Zotero/storage/P7TNGIAB/1705.html}
}

@article{altschuler_near-linear_2017,
  title = {Near-Linear Time Approximation Algorithms for Optimal Transport via {{Sinkhorn}} Iteration},
  url = {https://arxiv.org/abs/1705.09634},
  urldate = {2017-12-07},
  date = {2017-05-26},
  author = {Altschuler, Jason and Weed, Jonathan and Rigollet, Philippe},
  file = {/home/arthur/Dropbox/Zotero/Altschuler et al_2017_Near-linear time approximation algorithms for optimal transport via Sinkhorn.pdf;/home/arthur/Zotero/storage/7VKFNSV5/1705.html}
}

@inproceedings{ge_optimization_2017,
  title = {On the {{Optimization Landscape}} of {{Tensor Decompositions}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  date = {2017},
  pages = {3655--3664},
  author = {Ge, Rong and Ma, Tengyu},
  file = {/home/arthur/Dropbox/Zotero/Ge_Ma_2017_On the Optimization Landscape of Tensor Decompositions.pdf}
}

@article{tian_elf_2017,
  title = {{{ELF}}: {{An Extensive}}, {{Lightweight}} and {{Flexible Research Platform}} for {{Real}}-Time {{Strategy Games}}},
  url = {https://arxiv.org/abs/1707.01067},
  shorttitle = {{{ELF}}},
  urldate = {2017-12-07},
  date = {2017-07-04},
  author = {Tian, Yuandong and Gong, Qucheng and Shang, Wenling and Wu, Yuxin and Zitnick, C. Lawrence},
  file = {/home/arthur/Dropbox/Zotero/Tian et al_2017_ELF.pdf;/home/arthur/Zotero/storage/XHN2FHP3/1707.html}
}

@article{rocktaschel_end--end_2017,
  title = {End-to-{{End Differentiable Proving}}},
  url = {https://arxiv.org/abs/1705.11040},
  urldate = {2017-12-07},
  date = {2017-05-31},
  author = {Rocktäschel, Tim and Riedel, Sebastian},
  file = {/home/arthur/Dropbox/Zotero/Rocktäschel_Riedel_2017_End-to-End Differentiable Proving.pdf;/home/arthur/Zotero/storage/ZD6JWB45/1705.html}
}

@article{louppe_adversarial_2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1707.07113},
  primaryClass = {cs, stat},
  title = {Adversarial {{Variational Optimization}} of {{Non}}-{{Differentiable Simulators}}},
  url = {http://arxiv.org/abs/1707.07113},
  abstract = {Complex computer simulators are increasingly used across fields of science as generative models tying parameters of an underlying theory to experimental observations. Inference in this setup is often difficult, as simulators rarely admit a tractable density or likelihood function. We introduce Adversarial Variational Optimization (AVO), a likelihood-free inference algorithm for fitting a non-differentiable generative model incorporating ideas from empirical Bayes and variational inference. We adapt the training procedure of generative adversarial networks by replacing the differentiable generative network with a domain-specific simulator. We solve the resulting non-differentiable minimax problem by minimizing variational upper bounds of the two adversarial objectives. Effectively, the procedure results in learning a proposal distribution over simulator parameters, such that the corresponding marginal distribution of the generated data matches the observations. We present results of the method with simulators producing both discrete and continuous data.},
  urldate = {2017-12-08},
  date = {2017-07-22},
  keywords = {Computer Science - Learning,Statistics - Machine Learning},
  author = {Louppe, Gilles and Cranmer, Kyle},
  file = {/home/arthur/Dropbox/Zotero/Louppe_Cranmer_2017_Adversarial Variational Optimization of Non-Differentiable Simulators.pdf;/home/arthur/Zotero/storage/BP5EERWI/1707.html}
}

@article{tolstikhin_wasserstein_2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1711.01558},
  primaryClass = {cs, stat},
  title = {Wasserstein {{Auto}}-{{Encoders}}},
  url = {http://arxiv.org/abs/1711.01558},
  abstract = {We propose the Wasserstein Auto-Encoder (WAE)---a new algorithm for building a generative model of the data distribution. WAE minimizes a penalized form of the Wasserstein distance between the model distribution and the target distribution, which leads to a different regularizer than the one used by the Variational Auto-Encoder (VAE). This regularizer encourages the encoded training distribution to match the prior. We compare our algorithm with several other techniques and show that it is a generalization of adversarial auto-encoders (AAE). Our experiments show that WAE shares many of the properties of VAEs (stable training, encoder-decoder architecture, nice latent manifold structure) while generating samples of better quality, as measured by the FID score.},
  urldate = {2017-12-22},
  date = {2017-11-05},
  keywords = {Computer Science - Learning,Statistics - Machine Learning},
  author = {Tolstikhin, Ilya and Bousquet, Olivier and Gelly, Sylvain and Schoelkopf, Bernhard},
  file = {/home/arthur/Dropbox/Zotero/Tolstikhin et al_2017_Wasserstein Auto-Encoders.pdf;/home/arthur/Zotero/storage/2M648IDZ/1711.html}
}

@article{xu_stacked_2016,
  title = {Stacked {{Sparse Autoencoder}} ({{SSAE}}) for {{Nuclei Detection}} on {{Breast Cancer Histopathology Images}}},
  volume = {35},
  issn = {0278-0062},
  url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4729702/},
  abstract = {Automated nuclear detection is a critical step for a number of computer assisted pathology related image analysis algorithms such as for automated grading of breast cancer tissue specimens. The Nottingham Histologic Score system is highly correlated with the shape and appearance of breast cancer nuclei in histopathological images. However, automated nucleus detection is complicated by () the large number of nuclei and the size of high resolution digitized pathology images, and () the variability in size, shape, appearance, and texture of the individual nuclei. Recently there has been interest in the application of “Deep Learning” strategies for classification and analysis of big image data. Histopathology, given its size and complexity, represents an excellent use case for application of deep learning strategies. In this paper, a Stacked Sparse Autoencoder (SSAE), an instance of a deep learning strategy, is presented for efficient nuclei detection on high-resolution histopathological images of breast cancer. The SSAE learns high-level features from just pixel intensities alone in order to identify distinguishing features of nuclei. A sliding window operation is applied to each image in order to represent image patches via high-level features obtained via the auto-encoder, which are then subsequently fed to a classifier which categorizes each image patch as nuclear or non-nuclear. Across a cohort of 500 histopathological images (2200 × 2200) and approximately 3500 manually segmented individual nuclei serving as the groundtruth, SSAE was shown to have an improved F-measure 84.49\% and an average area under Precision-Recall curve (AveP) 78.83\%. The SSAE approach also out-performed 9 other state of the art nuclear detection strategies.},
  number = {1},
  journaltitle = {IEEE transactions on medical imaging},
  shortjournal = {IEEE Trans Med Imaging},
  urldate = {2017-12-29},
  date = {2016-01},
  pages = {119-130},
  author = {Xu, Jun and Xiang, Lei and Liu, Qingshan and Gilmore, Hannah and Wu, Jianzhong and Tang, Jinghai and Madabhushi, Anant},
  file = {/home/arthur/Dropbox/Zotero/Xu et al_2016_Stacked Sparse Autoencoder (SSAE) for Nuclei Detection on Breast Cancer.pdf},
  eprinttype = {pmid},
  eprint = {26208307},
  pmcid = {PMC4729702}
}

@article{lyons_predicting_2014,
  langid = {english},
  title = {Predicting Backbone {{Cα}} Angles and Dihedrals from Protein Sequences by Stacked Sparse Auto-Encoder Deep Neural Network},
  volume = {35},
  issn = {1096-987X},
  url = {http://onlinelibrary.wiley.com/doi/10.1002/jcc.23718/abstract},
  abstract = {Because a nearly constant distance between two neighbouring Cα atoms, local backbone structure of proteins can be represented accurately by the angle between Cαi−1CαiCαi+1 (θ) and a dihedral angle rotated about the CαiCαi+1 bond (τ). θ and τ angles, as the representative of structural properties of three to four amino-acid residues, offer a description of backbone conformations that is complementary to φ and ψ angles (single residue) and secondary structures ($>$3 residues). Here, we report the first machine-learning technique for sequence-based prediction of θ and τ angles. Predicted angles based on an independent test have a mean absolute error of 9° for θ and 34° for τ with a distribution on the θ-τ plane close to that of native values. The average root-mean-square distance of 10-residue fragment structures constructed from predicted θ and τ angles is only 1.9Å from their corresponding native structures. Predicted θ and τ angles are expected to be complementary to predicted ϕ and ψ angles and secondary structures for using in model validation and template-based as well as template-free structure prediction. The deep neural network learning technique is available as an on-line server called Structural Property prediction with Integrated DEep neuRal network (SPIDER) at http://sparks-lab.org. © 2014 Wiley Periodicals, Inc.},
  number = {28},
  journaltitle = {Journal of Computational Chemistry},
  shortjournal = {J. Comput. Chem.},
  urldate = {2017-12-29},
  date = {2014-10-30},
  pages = {2040-2046},
  keywords = {neural network,deep learning,fold recognition,fragment structure prediction,local structure prediction,protein structure prediction,secondary structure prediction},
  author = {Lyons, James and Dehzangi, Abdollah and Heffernan, Rhys and Sharma, Alok and Paliwal, Kuldip and Sattar, Abdul and Zhou, Yaoqi and Yang, Yuedong},
  file = {/home/arthur/Zotero/storage/GRN8VE7V/abstract.html}
}

@article{roughan_spatio-temporal_2012,
  title = {Spatio-{{Temporal Compressive Sensing}} and {{Internet Traffic Matrices}} ({{Extended Version}})},
  volume = {20},
  issn = {1063-6692},
  doi = {10.1109/TNET.2011.2169424},
  abstract = {Summary form only given. Strong light-matter coupling has been recently successfully explored in the GHz and THz [1] range with on-chip platforms. New and intriguing quantum optical phenomena have been predicted in the ultrastrong coupling regime [2], when the coupling strength Ω becomes comparable to the unperturbed frequency of the system ω. We recently proposed a new experimental platform where we couple the inter-Landau level transition of an high-mobility 2DEG to the highly subwavelength photonic mode of an LC meta-atom [3] showing very large Ω/ωc = 0.87. Our system benefits from the collective enhancement of the light-matter coupling which comes from the scaling of the coupling Ω ∝ √n, were n is the number of optically active electrons. In our previous experiments [3] and in literature [4] this number varies from 104-103 electrons per meta-atom. We now engineer a new cavity, resonant at 290 GHz, with an extremely reduced effective mode surface Seff = 4 × 10-14 m2 (FE simulations, CST), yielding large field enhancements above 1500 and allowing to enter the few ($<$;100) electron regime. It consist of a complementary metasurface with two very sharp metallic tips separated by a 60 nm gap (Fig.1(a, b)) on top of a single triangular quantum well. THz-TDS transmission experiments as a function of the applied magnetic field reveal strong anticrossing of the cavity mode with linear cyclotron dispersion. Measurements for arrays of only 12 cavities are reported in Fig.1(c). On the top horizontal axis we report the number of electrons occupying the topmost Landau level as a function of the magnetic field. At the anticrossing field of B=0.73 T we measure approximately 60 electrons ultra strongly coupled (Ω/ω- ||},
  number = {3},
  journaltitle = {IEEE/ACM Transactions on Networking},
  date = {2012-06},
  pages = {662-676},
  keywords = {Approximation algorithms,Sparse matrices,matrix algebra,Internet,Matrix decomposition,compressed sensing,Compressed sensing,Internet traffic matrices,interpolation,Interpolation,large-scale network datasets,measurement systems,measurement technology,prediction methods,Redundancy,spatio-temporal compressive sensing,spatio-temporal structure,telecommunication traffic,tomography,traffic data},
  author = {Roughan, M. and Zhang, Y. and Willinger, W. and Qiu, L.},
  file = {/home/arthur/Zotero/storage/DNZQRZT2/6058636.html}
}

@article{ganin_unsupervised_2014,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1409.7495},
  primaryClass = {cs, stat},
  title = {Unsupervised {{Domain Adaptation}} by {{Backpropagation}}},
  url = {http://arxiv.org/abs/1409.7495},
  abstract = {Top-performing deep architectures are trained on massive amounts of labeled data. In the absence of labeled data for a certain task, domain adaptation often provides an attractive option given that labeled data of similar nature but from a different domain (e.g. synthetic images) are available. Here, we propose a new approach to domain adaptation in deep architectures that can be trained on large amount of labeled data from the source domain and large amount of unlabeled data from the target domain (no labeled target-domain data is necessary). As the training progresses, the approach promotes the emergence of "deep" features that are (i) discriminative for the main learning task on the source domain and (ii) invariant with respect to the shift between the domains. We show that this adaptation behaviour can be achieved in almost any feed-forward model by augmenting it with few standard layers and a simple new gradient reversal layer. The resulting augmented architecture can be trained using standard backpropagation. Overall, the approach can be implemented with little effort using any of the deep-learning packages. The method performs very well in a series of image classification experiments, achieving adaptation effect in the presence of big domain shifts and outperforming previous state-of-the-art on Office datasets.},
  urldate = {2017-12-29},
  date = {2014-09-26},
  keywords = {Computer Science - Learning,Statistics - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  author = {Ganin, Yaroslav and Lempitsky, Victor},
  file = {/home/arthur/Dropbox/Zotero/Ganin_Lempitsky_2014_Unsupervised Domain Adaptation by Backpropagation.pdf;/home/arthur/Zotero/storage/3EZI2G58/1409.html}
}

@article{louppe_learning_2016,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1611.01046},
  primaryClass = {physics, stat},
  title = {Learning to {{Pivot}} with {{Adversarial Networks}}},
  url = {http://arxiv.org/abs/1611.01046},
  abstract = {Several techniques for domain adaptation have been proposed to account for differences in the distribution of the data used for training and testing. The majority of this work focuses on a binary domain label. Similar problems occur in a scientific context where there may be a continuous family of plausible data generation processes associated to the presence of systematic uncertainties. Robust inference is possible if it is based on a pivot -- a quantity whose distribution does not depend on the unknown values of the nuisance parameters that parametrize this family of data generation processes. In this work, we introduce and derive theoretical results for a training procedure based on adversarial networks for enforcing the pivotal property (or, equivalently, fairness with respect to continuous attributes) on a predictive model. The method includes a hyperparameter to control the trade-off between accuracy and robustness. We demonstrate the effectiveness of this approach with a toy example and examples from particle physics.},
  urldate = {2017-12-29},
  date = {2016-11-03},
  keywords = {Computer Science - Learning,Statistics - Machine Learning,Statistics - Methodology,Computer Science - Neural and Evolutionary Computing,Physics - Data Analysis; Statistics and Probability},
  author = {Louppe, Gilles and Kagan, Michael and Cranmer, Kyle},
  file = {/home/arthur/Dropbox/Zotero/Louppe et al_2016_Learning to Pivot with Adversarial Networks.pdf;/home/arthur/Zotero/storage/MHV7WI8T/1611.html}
}

@article{courty_optimal_2017,
  title = {Optimal Transport for Domain Adaptation},
  volume = {39},
  number = {9},
  journaltitle = {IEEE transactions on pattern analysis and machine intelligence},
  date = {2017},
  pages = {1853--1865},
  author = {Courty, Nicolas and Flamary, Rémi and Tuia, Devis and Rakotomamonjy, Alain},
  file = {/home/arthur/Dropbox/Zotero/Courty et al_2017_Optimal transport for domain adaptation.pdf;/home/arthur/Zotero/storage/X7JYYXRW/7586038.html}
}

@article{liu_learning_2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1705.09207},
  primaryClass = {cs},
  title = {Learning {{Structured Text Representations}}},
  url = {http://arxiv.org/abs/1705.09207},
  abstract = {In this paper, we focus on learning structure-aware document representations from data without recourse to a discourse parser or additional annotations. Drawing inspiration from recent efforts to empower neural networks with a structural bias, we propose a model that can encode a document while automatically inducing rich structural dependencies. Specifically, we embed a differentiable non-projective parsing algorithm into a neural model and use attention mechanisms to incorporate the structural biases. Experimental evaluation across different tasks and datasets shows that the proposed model achieves state-of-the-art results on document modeling tasks while inducing intermediate structures which are both interpretable and meaningful.},
  urldate = {2018-01-02},
  date = {2017-05-25},
  keywords = {Computer Science - Computation and Language,Computer Science - Artificial Intelligence},
  author = {Liu, Yang and Lapata, Mirella},
  file = {/home/arthur/Dropbox/Zotero/Liu_Lapata_2017_Learning Structured Text Representations.pdf;/home/arthur/Zotero/storage/34ZANYL5/1705.html}
}

@article{yang_breaking_2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1711.03953},
  primaryClass = {cs},
  title = {Breaking the {{Softmax Bottleneck}}: {{A High}}-{{Rank RNN Language Model}}},
  url = {http://arxiv.org/abs/1711.03953},
  shorttitle = {Breaking the {{Softmax Bottleneck}}},
  abstract = {We formulate language modeling as a matrix factorization problem, and show that the expressiveness of Softmax-based models (including the majority of neural language models) is limited by a Softmax bottleneck. Given that natural language is highly context-dependent, this further implies that in practice Softmax with distributed word embeddings does not have enough capacity to model natural language. We propose a simple and effective method to address this issue, and improve the state-of-the-art perplexities on Penn Treebank and WikiText-2 to 47.69 and 40.68 respectively.},
  urldate = {2018-01-03},
  date = {2017-11-10},
  keywords = {Computer Science - Learning,Computer Science - Computation and Language},
  author = {Yang, Zhilin and Dai, Zihang and Salakhutdinov, Ruslan and Cohen, William W.},
  file = {/home/arthur/Dropbox/Zotero/Yang et al_2017_Breaking the Softmax Bottleneck.pdf;/home/arthur/Zotero/storage/Q2GXP8DM/1711.html}
}

@inproceedings{ramdas_online_2017,
  title = {Online Control of the False Discovery Rate with Decaying Memory},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  date = {2017},
  pages = {5655--5664},
  author = {Ramdas, Aaditya and Yang, Fanny and Wainwright, Martin J. and Jordan, Michael I.},
  file = {/home/arthur/Dropbox/Zotero/Ramdas et al_2017_Online control of the false discovery rate with decaying memory.pdf}
}

@article{graves_sequence_2012,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1211.3711},
  primaryClass = {cs, stat},
  title = {Sequence {{Transduction}} with {{Recurrent Neural Networks}}},
  url = {http://arxiv.org/abs/1211.3711},
  abstract = {Many machine learning tasks can be expressed as the transformation---or $\backslash$emph\{transduction\}---of input sequences into output sequences: speech recognition, machine translation, protein secondary structure prediction and text-to-speech to name but a few. One of the key challenges in sequence transduction is learning to represent both the input and output sequences in a way that is invariant to sequential distortions such as shrinking, stretching and translating. Recurrent neural networks (RNNs) are a powerful sequence learning architecture that has proven capable of learning such representations. However RNNs traditionally require a pre-defined alignment between the input and output sequences to perform transduction. This is a severe limitation since $\backslash$emph\{finding\} the alignment is the most difficult aspect of many sequence transduction problems. Indeed, even determining the length of the output sequence is often challenging. This paper introduces an end-to-end, probabilistic sequence transduction system, based entirely on RNNs, that is in principle able to transform any input sequence into any finite, discrete output sequence. Experimental results for phoneme recognition are provided on the TIMIT speech corpus.},
  urldate = {2018-01-11},
  date = {2012-11-14},
  keywords = {Computer Science - Learning,Statistics - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  author = {Graves, Alex},
  file = {/home/arthur/Dropbox/Zotero/Graves_2012_Sequence Transduction with Recurrent Neural Networks.pdf;/home/arthur/Zotero/storage/GMQI7KNA/1211.html}
}

@article{bahdanau_neural_2014,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1409.0473},
  primaryClass = {cs, stat},
  title = {Neural {{Machine Translation}} by {{Jointly Learning}} to {{Align}} and {{Translate}}},
  url = {http://arxiv.org/abs/1409.0473},
  abstract = {Neural machine translation is a recently proposed approach to machine translation. Unlike the traditional statistical machine translation, the neural machine translation aims at building a single neural network that can be jointly tuned to maximize the translation performance. The models proposed recently for neural machine translation often belong to a family of encoder-decoders and consists of an encoder that encodes a source sentence into a fixed-length vector from which a decoder generates a translation. In this paper, we conjecture that the use of a fixed-length vector is a bottleneck in improving the performance of this basic encoder-decoder architecture, and propose to extend this by allowing a model to automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly. With this new approach, we achieve a translation performance comparable to the existing state-of-the-art phrase-based system on the task of English-to-French translation. Furthermore, qualitative analysis reveals that the (soft-)alignments found by the model agree well with our intuition.},
  urldate = {2018-01-11},
  date = {2014-09-01},
  keywords = {Computer Science - Learning,Statistics - Machine Learning,Computer Science - Computation and Language,Computer Science - Neural and Evolutionary Computing},
  author = {Bahdanau, Dzmitry and Cho, Kyunghyun and Bengio, Yoshua},
  file = {/home/arthur/Dropbox/Zotero/Bahdanau et al_2014_Neural Machine Translation by Jointly Learning to Align and Translate.pdf;/home/arthur/Zotero/storage/KLRF4FJC/1409.html}
}

@article{leblond_searnn_2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1706.04499},
  primaryClass = {cs, stat},
  title = {{{SEARNN}}: {{Training RNNs}} with {{Global}}-{{Local Losses}}},
  url = {http://arxiv.org/abs/1706.04499},
  shorttitle = {{{SEARNN}}},
  abstract = {We propose SEARNN, a novel training algorithm for recurrent neural networks (RNNs) inspired by the "learning to search" (L2S) approach to structured prediction. RNNs have been widely successful in structured prediction applications such as machine translation or parsing, and are commonly trained using maximum likelihood estimation (MLE). Unfortunately, this training loss is not always an appropriate surrogate for the test error: by only maximizing the ground truth probability, it fails to exploit the wealth of information offered by structured losses. Further, it introduces discrepancies between training and predicting (such as exposure bias) that may hurt test performance. Instead, SEARNN leverages test-alike search space exploration to introduce global-local losses that are closer to the test error. We demonstrate improved performance over MLE on three different tasks: OCR, spelling correction and text chunking. Finally, we propose a subsampling strategy to enable SEARNN to scale to large vocabulary sizes.},
  urldate = {2018-01-16},
  date = {2017-06-14},
  keywords = {Computer Science - Learning,Statistics - Machine Learning},
  author = {Leblond, Rémi and Alayrac, Jean-Baptiste and Osokin, Anton and Lacoste-Julien, Simon},
  file = {/home/arthur/Dropbox/Zotero/Leblond et al_2017_SEARNN.pdf;/home/arthur/Zotero/storage/BMPXMZYA/1706.html}
}

@article{bellman_routing_1958,
  title = {On a Routing Problem},
  volume = {16},
  number = {1},
  journaltitle = {Quarterly of applied mathematics},
  date = {1958},
  pages = {87--90},
  author = {Bellman, Richard},
  file = {/home/arthur/Dropbox/Zotero/Bellman_1958_On a routing problem.pdf;/home/arthur/Dropbox/Zotero/Bellman_1958_On a routing problem2.pdf;/home/arthur/Zotero/storage/8EI2MTVE/S0033-569X-1958-0102435-2.html}
}

@book{danskin_theory_2012,
  title = {The Theory of Max-Min and Its Application to Weapons Allocation Problems},
  volume = {5},
  publisher = {{Springer Science \& Business Media}},
  date = {2012},
  author = {Danskin, John M.},
  file = {/home/arthur/Zotero/storage/V96IYBMV/books.html}
}

@article{sabour_dynamic_2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1710.09829},
  primaryClass = {cs},
  title = {Dynamic {{Routing Between Capsules}}},
  url = {http://arxiv.org/abs/1710.09829},
  abstract = {A capsule is a group of neurons whose activity vector represents the instantiation parameters of a specific type of entity such as an object or an object part. We use the length of the activity vector to represent the probability that the entity exists and its orientation to represent the instantiation parameters. Active capsules at one level make predictions, via transformation matrices, for the instantiation parameters of higher-level capsules. When multiple predictions agree, a higher level capsule becomes active. We show that a discrimininatively trained, multi-layer capsule system achieves state-of-the-art performance on MNIST and is considerably better than a convolutional net at recognizing highly overlapping digits. To achieve these results we use an iterative routing-by-agreement mechanism: A lower-level capsule prefers to send its output to higher level capsules whose activity vectors have a big scalar product with the prediction coming from the lower-level capsule.},
  urldate = {2018-01-19},
  date = {2017-10-26},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  author = {Sabour, Sara and Frosst, Nicholas and Hinton, Geoffrey E.},
  file = {/home/arthur/Dropbox/Zotero/Sabour et al_2017_Dynamic Routing Between Capsules.pdf;/home/arthur/Zotero/storage/UVGYMH9N/1710.html}
}

@book{wang_named_2017,
  location = {{Cham}},
  title = {Named {{Entity Recognition}} with {{Gated Convolutional Neural Networks}}},
  volume = {10565},
  isbn = {978-3-319-69004-9 978-3-319-69005-6},
  url = {http://link.springer.com/10.1007/978-3-319-69005-6},
  series = {Lecture Notes in Computer Science},
  publisher = {{Springer International Publishing}},
  urldate = {2018-01-30},
  date = {2017},
  author = {Wang, Chunqi and Chen, Wei and Xu, Bo},
  file = {/home/arthur/Dropbox/Zotero/Wang et al_2017_Named Entity Recognition with Gated Convolutional Neural Networks.pdf},
  doi = {10.1007/978-3-319-69005-6}
}

@article{dauphin_language_2016,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1612.08083},
  primaryClass = {cs},
  title = {Language {{Modeling}} with {{Gated Convolutional Networks}}},
  url = {http://arxiv.org/abs/1612.08083},
  abstract = {The pre-dominant approach to language modeling to date is based on recurrent neural networks. Their success on this task is often linked to their ability to capture unbounded context. In this paper we develop a finite context approach through stacked convolutions, which can be more efficient since they allow parallelization over sequential tokens. We propose a novel simplified gating mechanism that outperforms Oord et al (2016) and investigate the impact of key architectural decisions. The proposed approach achieves state-of-the-art on the WikiText-103 benchmark, even though it features long-term dependencies, as well as competitive results on the Google Billion Words benchmark. Our model reduces the latency to score a sentence by an order of magnitude compared to a recurrent baseline. To our knowledge, this is the first time a non-recurrent approach is competitive with strong recurrent models on these large scale language tasks.},
  urldate = {2018-01-30},
  date = {2016-12-23},
  keywords = {Computer Science - Computation and Language},
  author = {Dauphin, Yann N. and Fan, Angela and Auli, Michael and Grangier, David},
  file = {/home/arthur/Dropbox/Zotero/Dauphin et al_2016_Language Modeling with Gated Convolutional Networks.pdf;/home/arthur/Zotero/storage/VZ4AAMHY/1612.html}
}

@article{chiu_named_2015,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1511.08308},
  primaryClass = {cs},
  title = {Named {{Entity Recognition}} with {{Bidirectional LSTM}}-{{CNNs}}},
  url = {http://arxiv.org/abs/1511.08308},
  abstract = {Named entity recognition is a challenging task that has traditionally required large amounts of knowledge in the form of feature engineering and lexicons to achieve high performance. In this paper, we present a novel neural network architecture that automatically detects word- and character-level features using a hybrid bidirectional LSTM and CNN architecture, eliminating the need for most feature engineering. We also propose a novel method of encoding partial lexicon matches in neural networks and compare it to existing approaches. Extensive evaluation shows that, given only tokenized text and publicly available word embeddings, our system is competitive on the CoNLL-2003 dataset and surpasses the previously reported state of the art performance on the OntoNotes 5.0 dataset by 2.13 F1 points. By using two lexicons constructed from publicly-available sources, we establish new state of the art performance with an F1 score of 91.62 on CoNLL-2003 and 86.28 on OntoNotes, surpassing systems that employ heavy feature engineering, proprietary lexicons, and rich entity linking information.},
  urldate = {2018-01-31},
  date = {2015-11-26},
  keywords = {Computer Science - Learning,Computer Science - Computation and Language,Computer Science - Neural and Evolutionary Computing,68T50,I.2.7},
  author = {Chiu, Jason P. C. and Nichols, Eric},
  file = {/home/arthur/Dropbox/Zotero/Chiu_Nichols_2015_Named Entity Recognition with Bidirectional LSTM-CNNs.pdf;/home/arthur/Zotero/storage/SNWXWYRA/1511.html}
}

@article{viterbi_error_1967,
  title = {Error Bounds for Convolutional Codes and an Asymptotically Optimum Decoding Algorithm},
  volume = {13},
  issn = {0018-9448},
  doi = {10.1109/TIT.1967.1054010},
  abstract = {The probability of error in decoding an optimal convolutional code transmitted over a memoryless channel is bounded from above and below as a function of the constraint length of the code. For all but pathological channels the bounds are asymptotically (exponentially) tight for rates aboveR\_0, the computational cutoff rate of sequential decoding. As a function of constraint length the performance of optimal convolutional codes is shown to be superior to that of block codes of the same length, the relative improvement increasing with rate. The upper bound is obtained for a specific probabilistic nonsequential decoding algorithm which is shown to be asymptotically optimum for rates aboveR\_0and whose performance bears certain similarities to that of sequential decoding algorithms.},
  number = {2},
  journaltitle = {IEEE Transactions on Information Theory},
  date = {1967-04},
  pages = {260-269},
  keywords = {Convolutional codes},
  author = {Viterbi, A.},
  file = {/home/arthur/Zotero/storage/J2U9ZU65/1054010.html}
}

@book{pearl_probabilistic_1991,
  langid = {english},
  title = {Probabilistic {{Reasoning}} in {{Intelligent Systems}}: {{Networks}} of {{Plausible Inference}}},
  isbn = {978-0-08-051489-5},
  shorttitle = {Probabilistic {{Reasoning}} in {{Intelligent Systems}}},
  abstract = {Probabilistic Reasoning in Intelligent Systems is a complete and accessible account of the theoretical foundations and computational methods that underlie plausible reasoning under uncertainty. The author provides a coherent explication of probability as a language for reasoning with partial belief and offers a unifying perspective on other AI approaches to uncertainty, such as the Dempster-Shafer formalism, truth maintenance systems, and nonmonotonic logic.The author distinguishes syntactic and semantic approaches to uncertainty--and offers techniques, based on belief networks, that provide a mechanism for making semantics-based systems operational. Specifically, network-propagation techniques serve as a mechanism for combining the theoretical coherence of probability theory with modern demands of reasoning-systems technology: modular declarative inputs, conceptually meaningful inferences, and parallel distributed computation. Application areas include diagnosis, forecasting, image interpretation, multi-sensor fusion, decision support systems, plan recognition, planning, speech recognition--in short, almost every task requiring that conclusions be drawn from uncertain clues and incomplete information.Probabilistic Reasoning in Intelligent Systems will be of special interest to scholars and researchers in AI, decision theory, statistics, logic, philosophy, cognitive psychology, and the management sciences. Professionals in the areas of knowledge-based systems, operations research, engineering, and statistics will find theoretical and computational tools of immediate practical use. The book can also be used as an excellent text for graduate-level courses in AI, operations research, or applied probability.},
  pagetotal = {573},
  publisher = {{Elsevier}},
  date = {1991},
  keywords = {Computers / Intelligence (AI) & Semantics},
  author = {Pearl, Judea},
  eprinttype = {googlebooks}
}

@inproceedings{tjong_kim_sang_introduction_2003,
  title = {Introduction to the {{CoNLL}}-2003 Shared Task: {{Language}}-Independent Named Entity Recognition},
  shorttitle = {Introduction to the {{CoNLL}}-2003 Shared Task},
  booktitle = {Proceedings of the Seventh Conference on {{Natural}} Language Learning at {{HLT}}-{{NAACL}} 2003-{{Volume}} 4},
  publisher = {{Association for Computational Linguistics}},
  date = {2003},
  pages = {142--147},
  author = {Tjong Kim Sang, Erik F. and De Meulder, Fien},
  file = {/home/arthur/Dropbox/Zotero/Tjong Kim Sang_De Meulder_2003_Introduction to the CoNLL-2003 shared task.pdf;/home/arthur/Zotero/storage/MG35C7BD/citation.html}
}

@article{gselmann_entropy_2011,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1307.0650},
  title = {Entropy Functions and Functional Equations},
  abstract = {The purpose of this note is to give the general solution of two functional equations connected to the Shannon entropy and also to the Tsallis entropy. As a result of this, we present the regular solution of these equations, as well. Furthermore, we point out that the regularity assumptions used in previous works can substantially be weakened.},
  number = {16},
  journaltitle = {Mathematical Communications},
  date = {2011},
  pages = {347-357},
  keywords = {Mathematics - Classical Analysis and ODEs,Primary 39B22; Secondary 94A17},
  author = {Gselmann, Eszter},
  file = {/home/arthur/Dropbox/Zotero/Gselmann_2011_Entropy functions and functional equations.pdf;/home/arthur/Zotero/storage/FTJ7K56Z/1307.html}
}

@article{horibe_entropy_1988,
  title = {Entropy of Terminal Distributions and the {{Fibonnacci}} Trees},
  url = {https://www.fq.math.ca/Scanned/26-2/horibe.pdf},
  number = {26},
  journaltitle = {The Fibonacci Quarterly},
  urldate = {2018-02-05},
  date = {1988},
  pages = {135-140},
  author = {Horibe, Yasuichi},
  file = {/home/arthur/Dropbox/Zotero/Horibe_1988_Entropy of terminal distributions and the Fibonnacci trees.pdf}
}

@inproceedings{luong_effective_2015,
  title = {Effective {{Approaches}} to {{Attention}}-Based {{Neural Machine Translation}}},
  booktitle = {Proc. {{EMNLP}}},
  date = {2015},
  pages = {1412--1421},
  author = {Luong, Thang and Pham, Hieu and Manning, Christopher D.},
  file = {/home/arthur/Dropbox/Zotero/Luong et al_2015_Effective Approaches to Attention-based Neural Machine Translation.pdf;/home/arthur/Dropbox/Zotero/Luong et al_2015_Effective Approaches to Attention-based Neural Machine Translation2.pdf}
}

@article{moreau_proximite_1965,
  title = {Proximité et Dualité Dans Un Espace Hilbertien},
  volume = {93},
  number = {2},
  journaltitle = {Bull. Soc. Math. France},
  date = {1965},
  pages = {273--299},
  author = {Moreau, Jean-Jacques},
  file = {/home/arthur/Dropbox/Zotero/Moreau_1965_Proximité et dualité dans un espace hilbertien.pdf;/home/arthur/Dropbox/Zotero/Moreau_1965_Proximité et dualité dans un espace hilbertien2.pdf}
}

@book{rall_automatic_1981,
  title = {Automatic {{Differentiation}}: {{Techniques}} and {{Applications}}},
  volume = {120},
  shorttitle = {Automatic {{Differentiation}}},
  abstract = {Rall gives complete details on algorithms for the generation and differentiation of code lists for expressions. Recurrence relations for Taylor coefficients are given. Applications to ordinary differential equations, quadrature, optimization, and other problems are given.},
  publisher = {{Springer}},
  date = {1981},
  keywords = {automatic,differentiation},
  author = {Rall, Louis}
}

@article{verdu_abstract_1987,
  title = {Abstract Dynamic Programming Models under Coomutativity Conditions},
  volume = {25},
  url = {http://sci-hub.la/http://epubs.siam.org/doi/abs/10.1137/0325054?journalCode=sjcodc},
  number = {4},
  journaltitle = {SIAM Journal on Control and Optimization},
  urldate = {2018-02-07},
  date = {1987},
  author = {Verdu, Sergio and Poor, Vincent},
  file = {/home/arthur/Dropbox/Zotero/Verdu_Poor_1987_Abstract dynamic programming models under coomutativity conditions.pdf;/home/arthur/Zotero/storage/LZUMQZ4N/0325054.html}
}

@article{beck_smoothing_2012,
  title = {Smoothing and {{First Order Methods}}: {{A Unified Framework}}},
  volume = {22},
  issn = {1052-6234},
  url = {http://epubs.siam.org/doi/abs/10.1137/100818327},
  shorttitle = {Smoothing and {{First Order Methods}}},
  abstract = {We propose a unifying framework that combines smoothing approximation with fast first order algorithms for solving nonsmooth convex minimization problems. We prove that independently of the structure of the convex nonsmooth function involved,  and of the given fast first order iterative scheme, it is always possible to improve the complexity rate and reach an \$O($\backslash$varepsilon\^\{-1\})\$ efficiency estimate by solving an adequately smoothed approximation counterpart. Our approach relies on the combination of the notion of smoothable functions that we introduce with a natural extension of the Moreau-infimal convolution technique along with its connection to the smoothing mechanism via asymptotic functions. This allows for clarification and unification of several issues on the design, analysis, and potential applications of smoothing methods when combined with fast first order algorithms.},
  number = {2},
  journaltitle = {SIAM Journal on Optimization},
  shortjournal = {SIAM J. Optim.},
  urldate = {2018-02-07},
  date = {2012-01-01},
  pages = {557-580},
  author = {Beck, A. and Teboulle, M.},
  file = {/home/arthur/Dropbox/Zotero/Beck_Teboulle_2012_Smoothing and First Order Methods.pdf;/home/arthur/Zotero/storage/YP7CJZMS/100818327.html}
}

@article{pham_efficient_2018,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1802.03268},
  primaryClass = {cs, stat},
  title = {Efficient {{Neural Architecture Search}} via {{Parameter Sharing}}},
  url = {http://arxiv.org/abs/1802.03268},
  abstract = {We propose Efficient Neural Architecture Search (ENAS), a fast and inexpensive approach for automatic model design. In ENAS, a controller learns to discover neural network architectures by searching for an optimal subgraph within a large computational graph. The controller is trained with policy gradient to select a subgraph that maximizes the expected reward on the validation set. Meanwhile the model corresponding to the selected subgraph is trained to minimize a canonical cross entropy loss. Thanks to parameter sharing between child models, ENAS is fast: it delivers strong empirical performances using much fewer GPU-hours than all existing automatic model design approaches, and notably, 1000x less expensive than standard Neural Architecture Search. On the Penn Treebank dataset, ENAS discovers a novel architecture that achieves a test perplexity of 55.8, establishing a new state-of-the-art among all methods without post-training processing. On the CIFAR-10 dataset, ENAS designs novel architectures that achieve a test error of 2.89\%, which is on par with NASNet (Zoph et al., 2018), whose test error is 2.65\%.},
  urldate = {2018-02-13},
  date = {2018-02-09},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Learning,Statistics - Machine Learning,Computer Science - Computation and Language,Computer Science - Neural and Evolutionary Computing},
  author = {Pham, Hieu and Guan, Melody Y. and Zoph, Barret and Le, Quoc V. and Dean, Jeff},
  file = {/home/arthur/Dropbox/Zotero/Pham et al_2018_Efficient Neural Architecture Search via Parameter Sharing.pdf;/home/arthur/Zotero/storage/M7CQYDKU/1802.html}
}

@article{liang_fisher-rao_2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1711.01530},
  primaryClass = {cs, stat},
  title = {Fisher-{{Rao Metric}}, {{Geometry}}, and {{Complexity}} of {{Neural Networks}}},
  url = {http://arxiv.org/abs/1711.01530},
  abstract = {We study the relationship between geometry and capacity measures for deep neural networks from an invariance viewpoint. We introduce a new notion of capacity --- the Fisher-Rao norm --- that possesses desirable invariance properties and is motivated by Information Geometry. We discover an analytical characterization of the new capacity measure, through which we establish norm-comparison inequalities and further show that the new measure serves as an umbrella for several existing norm-based complexity measures. We discuss upper bounds on the generalization error induced by the proposed measure. Extensive numerical experiments on CIFAR-10 support our theoretical findings. Our theoretical analysis rests on a key structural lemma about partial derivatives of multi-layer rectifier networks.},
  urldate = {2018-02-20},
  date = {2017-11-05},
  keywords = {Computer Science - Learning,Statistics - Machine Learning,Computer Science - Artificial Intelligence},
  author = {Liang, Tengyuan and Poggio, Tomaso and Rakhlin, Alexander and Stokes, James},
  file = {/home/arthur/Dropbox/Zotero/Liang et al_2017_Fisher-Rao Metric, Geometry, and Complexity of Neural Networks.pdf;/home/arthur/Zotero/storage/P9GWTPNI/1711.html}
}

@article{blier_deep_2018,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1802.07044},
  primaryClass = {cs},
  title = {Do {{Deep Learning Models Have Too Many Parameters}}? {{An Information Theory Viewpoint}}},
  url = {http://arxiv.org/abs/1802.07044},
  shorttitle = {Do {{Deep Learning Models Have Too Many Parameters}}?},
  abstract = {Deep learning models often have more parameters than observations, and still perform well. This is sometimes described as a paradox. In this work, we show experimentally that despite their huge number of parameters, deep neural networks can compress the data losslessly even when taking the cost of encoding the parameters into account. Such a compression viewpoint originally motivated the use of variational methods in neural networks. However, we show that these variational methods provide surprisingly poor compression bounds, despite being explicitly built to minimize such bounds. This might explain the relatively poor practical performance of variational methods in deep learning. Better encoding methods, imported from the Minimum Description Length (MDL) toolbox, yield much better compression values on deep networks, corroborating the hypothesis that good compression on the training set correlates with good test performance.},
  urldate = {2018-02-22},
  date = {2018-02-20},
  keywords = {Computer Science - Learning},
  author = {Blier, Léonard and Ollivier, Yann},
  file = {/home/arthur/Dropbox/Zotero/Blier_Ollivier_2018_Do Deep Learning Models Have Too Many Parameters.pdf;/home/arthur/Zotero/storage/RD65KKGS/1802.html}
}

@article{theis_faster_2018,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1801.05787},
  primaryClass = {cs, stat},
  title = {Faster Gaze Prediction with Dense Networks and {{Fisher}} Pruning},
  url = {http://arxiv.org/abs/1801.05787},
  abstract = {Predicting human fixations from images has recently seen large improvements by leveraging deep representations which were pretrained for object recognition. However, as we show in this paper, these networks are highly overparameterized for the task of fixation prediction. We first present a simple yet principled greedy pruning method which we call Fisher pruning. Through a combination of knowledge distillation and Fisher pruning, we obtain much more runtime-efficient architectures for saliency prediction, achieving a 10x speedup for the same AUC performance as a state of the art network on the CAT2000 dataset. Speeding up single-image gaze prediction is important for many real-world applications, but it is also a crucial step in the development of video saliency models, where the amount of data to be processed is substantially larger.},
  urldate = {2018-02-22},
  date = {2018-01-17},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Statistics - Machine Learning},
  author = {Theis, Lucas and Korshunova, Iryna and Tejani, Alykhan and Huszár, Ferenc},
  file = {/home/arthur/Dropbox/Zotero/Theis et al_2018_Faster gaze prediction with dense networks and Fisher pruning.pdf;/home/arthur/Zotero/storage/XA3AXAQP/1801.html}
}

@article{ge_no_2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1704.00708},
  primaryClass = {cs, math, stat},
  title = {No {{Spurious Local Minima}} in {{Nonconvex Low Rank Problems}}: {{A Unified Geometric Analysis}}},
  url = {http://arxiv.org/abs/1704.00708},
  shorttitle = {No {{Spurious Local Minima}} in {{Nonconvex Low Rank Problems}}},
  abstract = {In this paper we develop a new framework that captures the common landscape underlying the common non-convex low-rank matrix problems including matrix sensing, matrix completion and robust PCA. In particular, we show for all above problems (including asymmetric cases): 1) all local minima are also globally optimal; 2) no high-order saddle points exists. These results explain why simple algorithms such as stochastic gradient descent have global converge, and efficiently optimize these non-convex objective functions in practice. Our framework connects and simplifies the existing analyses on optimization landscapes for matrix sensing and symmetric matrix completion. The framework naturally leads to new results for asymmetric matrix completion and robust PCA.},
  urldate = {2018-02-22},
  date = {2017-04-03},
  keywords = {Computer Science - Learning,Mathematics - Optimization and Control,Statistics - Machine Learning},
  author = {Ge, Rong and Jin, Chi and Zheng, Yi},
  file = {/home/arthur/Dropbox/Zotero/Ge et al_2017_No Spurious Local Minima in Nonconvex Low Rank Problems.pdf;/home/arthur/Zotero/storage/AMSRAFRA/1704.html}
}

@article{bach_self-concordant_2010,
  langid = {english},
  title = {Self-Concordant Analysis for Logistic Regression},
  volume = {4},
  issn = {1935-7524},
  url = {http://projecteuclid.org/euclid.ejs/1271941980},
  number = {0},
  journaltitle = {Electronic Journal of Statistics},
  urldate = {2018-02-22},
  date = {2010},
  pages = {384-414},
  author = {Bach, Francis},
  file = {/home/arthur/Dropbox/Zotero/Bach_2010_Self-concordant analysis for logistic regression.pdf}
}

@online{ruder_overview_2017,
  title = {An {{Overview}} of {{Multi}}-{{Task Learning}} for {{Deep Learning}}},
  url = {u=http://ruder.io/multi-task/},
  abstract = {This blog post gives an overview of multi-task learning in deep neural networks. It discusses existing approaches as well as recent advances.},
  journaltitle = {Sebastian Ruder},
  urldate = {2018-02-26},
  date = {2017-05-29T13:00:00.000Z},
  author = {Ruder, Sebastian},
  file = {/home/arthur/Zotero/storage/IWJ8LCLG/index.html}
}

@inproceedings{jalali_dirty_2010,
  title = {A Dirty Model for Multi-Task Learning},
  booktitle = {Advances in Neural Information Processing Systems},
  date = {2010},
  pages = {964--972},
  author = {Jalali, Ali and Sanghavi, Sujay and Ruan, Chao and Ravikumar, Pradeep K.},
  file = {/home/arthur/Dropbox/Zotero/Jalali et al_2010_A dirty model for multi-task learning.pdf}
}

@article{ruder_learning_2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1705.08142},
  primaryClass = {cs, stat},
  title = {Learning What to Share between Loosely Related Tasks},
  url = {http://arxiv.org/abs/1705.08142},
  abstract = {Multi-task learning is motivated by the observation that humans bring to bear what they know about related problems when solving new ones. Similarly, deep neural networks can profit from related tasks by sharing parameters with other networks. However, humans do not consciously decide to transfer knowledge between tasks. In Natural Language Processing (NLP), it is hard to predict if sharing will lead to improvements, particularly if tasks are only loosely related. To overcome this, we introduce Sluice Networks, a general framework for multi-task learning where trainable parameters control the amount of sharing. Our framework generalizes previous proposals in enabling sharing of all combinations of subspaces, layers, and skip connections. We perform experiments on three task pairs, and across seven different domains, using data from OntoNotes 5.0, and achieve up to 15\% average error reductions over common approaches to multi-task learning. We show that a) label entropy is predictive of gains in sluice networks, confirming findings for hard parameter sharing and b) while sluice networks easily fit noise, they are robust across domains in practice.},
  urldate = {2018-02-26},
  date = {2017-05-23},
  keywords = {Computer Science - Learning,Statistics - Machine Learning,Computer Science - Computation and Language,Computer Science - Neural and Evolutionary Computing,Computer Science - Artificial Intelligence},
  author = {Ruder, Sebastian and Bingel, Joachim and Augenstein, Isabelle and Søgaard, Anders},
  file = {/home/arthur/Dropbox/Zotero/Ruder et al_2017_Learning what to share between loosely related tasks.pdf;/home/arthur/Zotero/storage/67CF7GID/1705.html}
}

@article{bingel_identifying_2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1702.08303},
  primaryClass = {cs},
  title = {Identifying Beneficial Task Relations for Multi-Task Learning in Deep Neural Networks},
  url = {http://arxiv.org/abs/1702.08303},
  abstract = {Multi-task learning (MTL) in deep neural networks for NLP has recently received increasing interest due to some compelling benefits, including its potential to efficiently regularize models and to reduce the need for labeled data. While it has brought significant improvements in a number of NLP tasks, mixed results have been reported, and little is known about the conditions under which MTL leads to gains in NLP. This paper sheds light on the specific task relations that can lead to gains from MTL models over single-task setups.},
  urldate = {2018-02-26},
  date = {2017-02-27},
  keywords = {Computer Science - Computation and Language,I.2.7},
  author = {Bingel, Joachim and Søgaard, Anders},
  file = {/home/arthur/Dropbox/Zotero/Bingel_Søgaard_2017_Identifying beneficial task relations for multi-task learning in deep neural.pdf;/home/arthur/Zotero/storage/5EU8J8YQ/1702.html}
}

@article{bousmalis_domain_2016,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1608.06019},
  primaryClass = {cs},
  title = {Domain {{Separation Networks}}},
  url = {http://arxiv.org/abs/1608.06019},
  abstract = {The cost of large scale data collection and annotation often makes the application of machine learning algorithms to new tasks or datasets prohibitively expensive. One approach circumventing this cost is training models on synthetic data where annotations are provided automatically. Despite their appeal, such models often fail to generalize from synthetic to real images, necessitating domain adaptation algorithms to manipulate these models before they can be successfully applied. Existing approaches focus either on mapping representations from one domain to the other, or on learning to extract features that are invariant to the domain from which they were extracted. However, by focusing only on creating a mapping or shared representation between the two domains, they ignore the individual characteristics of each domain. We suggest that explicitly modeling what is unique to each domain can improve a model's ability to extract domain-invariant features. Inspired by work on private-shared component analysis, we explicitly learn to extract image representations that are partitioned into two subspaces: one component which is private to each domain and one which is shared across domains. Our model is trained not only to perform the task we care about in the source domain, but also to use the partitioned representation to reconstruct the images from both domains. Our novel architecture results in a model that outperforms the state-of-the-art on a range of unsupervised domain adaptation scenarios and additionally produces visualizations of the private and shared representations enabling interpretation of the domain adaptation process.},
  urldate = {2018-02-26},
  date = {2016-08-21},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  author = {Bousmalis, Konstantinos and Trigeorgis, George and Silberman, Nathan and Krishnan, Dilip and Erhan, Dumitru},
  file = {/home/arthur/Dropbox/Zotero/Bousmalis et al_2016_Domain Separation Networks.pdf;/home/arthur/Zotero/storage/SWLVBK96/1608.html}
}

@article{lee_deep_2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1708.00260},
  primaryClass = {cs, stat},
  title = {Deep {{Asymmetric Multi}}-Task {{Feature Learning}}},
  url = {http://arxiv.org/abs/1708.00260},
  abstract = {We propose Deep Asymmetric Multitask Feature Learning (Deep-AMTFL) which can learn deep representations shared across multiple tasks while effectively preventing negative transfer that may happen in the feature sharing process. Specifically, we introduce an asymmetric autoencoder term that allows reliable predictors for the easy tasks to have high contribution to the feature learning while suppressing the influences of unreliable predictors for more difficult tasks. This allows the learning of less noisy representations, and enables unreliable predictors to exploit knowledge from the reliable predictors via the shared latent features. Such asymmetric knowledge transfer through shared features is also more scalable and efficient than inter-task asymmetric transfer. We validate our Deep-AMTFL model on multiple benchmark datasets for multitask learning and image classification, on which it significantly outperforms existing symmetric and asymmetric multitask learning models, by effectively preventing negative transfer in deep feature learning.},
  urldate = {2018-02-28},
  date = {2017-08-01},
  keywords = {Computer Science - Learning,Statistics - Machine Learning},
  author = {Lee, Hae Beom and Yang, Eunho and Hwang, Sung Ju},
  file = {/home/arthur/Dropbox/Zotero/Lee et al_2017_Deep Asymmetric Multi-task Feature Learning.pdf;/home/arthur/Zotero/storage/B4UVJKUR/1708.html}
}

@article{loshchilov_sgdr_2016,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1608.03983},
  primaryClass = {cs, math},
  title = {{{SGDR}}: {{Stochastic Gradient Descent}} with {{Warm Restarts}}},
  url = {http://arxiv.org/abs/1608.03983},
  shorttitle = {{{SGDR}}},
  abstract = {Restart techniques are common in gradient-free optimization to deal with multimodal functions. Partial warm restarts are also gaining popularity in gradient-based optimization to improve the rate of convergence in accelerated gradient schemes to deal with ill-conditioned functions. In this paper, we propose a simple warm restart technique for stochastic gradient descent to improve its anytime performance when training deep neural networks. We empirically study its performance on the CIFAR-10 and CIFAR-100 datasets, where we demonstrate new state-of-the-art results at 3.14\% and 16.21\%, respectively. We also demonstrate its advantages on a dataset of EEG recordings and on a downsampled version of the ImageNet dataset. Our source code is available at https://github.com/loshchil/SGDR},
  urldate = {2018-02-28},
  date = {2016-08-13},
  keywords = {Computer Science - Learning,Mathematics - Optimization and Control,Computer Science - Neural and Evolutionary Computing},
  author = {Loshchilov, Ilya and Hutter, Frank},
  file = {/home/arthur/Dropbox/Zotero/Loshchilov_Hutter_2016_SGDR.pdf;/home/arthur/Zotero/storage/ULFGN8PK/1608.html}
}

@inproceedings{bousmalis_unsupervised_2017,
  title = {Unsupervised Pixel-Level Domain Adaptation with Generative Adversarial Networks},
  volume = {1},
  booktitle = {The {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  date = {2017},
  pages = {7},
  author = {Bousmalis, Konstantinos and Silberman, Nathan and Dohan, David and Erhan, Dumitru and Krishnan, Dilip},
  file = {/home/arthur/Dropbox/Zotero/Bousmalis et al_2017_Unsupervised pixel-level domain adaptation with generative adversarial networks.pdf;/home/arthur/Dropbox/Zotero/Bousmalis et al_2017_Unsupervised pixel-level domain adaptation with generative adversarial networks2.pdf}
}

@article{ganin_domain-adversarial_2016,
  title = {Domain-Adversarial Training of Neural Networks},
  volume = {17},
  number = {1},
  journaltitle = {The Journal of Machine Learning Research},
  date = {2016},
  pages = {2096--2030},
  author = {Ganin, Yaroslav and Ustinova, Evgeniya and Ajakan, Hana and Germain, Pascal and Larochelle, Hugo and Laviolette, François and Marchand, Mario and Lempitsky, Victor},
  file = {/home/arthur/Dropbox/Zotero/Ganin et al_2016_Domain-adversarial training of neural networks.pdf;/home/arthur/Dropbox/Zotero/Ganin et al_2016_Domain-adversarial training of neural networks2.pdf}
}

@online{hardt_can_2018,
  title = {Can Increasing Depth Serve to Accelerate Optimization?},
  url = {http://offconvex.github.io/2018/03/02/acceleration-overparameterization/},
  abstract = {Algorithms off the convex path.},
  journaltitle = {Off the convex path},
  urldate = {2018-03-05},
  date = {2018},
  author = {Hardt, Moritz},
  file = {/home/arthur/Zotero/storage/CPGRIV2U/acceleration-overparameterization.html}
}

@article{lecun_tutorial_2006,
  title = {A Tutorial on Energy-Based Learning},
  volume = {1},
  number = {0},
  journaltitle = {Predicting structured data},
  date = {2006},
  author = {LeCun, Yann and Chopra, Sumit and Hadsell, Raia and Ranzato, M. and Huang, F.},
  file = {/home/arthur/Dropbox/Zotero/LeCun et al_2006_A tutorial on energy-based learning.pdf}
}

@article{huang_snapshot_2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1704.00109},
  primaryClass = {cs},
  title = {Snapshot {{Ensembles}}: {{Train}} 1, Get {{M}} for Free},
  url = {http://arxiv.org/abs/1704.00109},
  shorttitle = {Snapshot {{Ensembles}}},
  abstract = {Ensembles of neural networks are known to be much more robust and accurate than individual networks. However, training multiple deep networks for model averaging is computationally expensive. In this paper, we propose a method to obtain the seemingly contradictory goal of ensembling multiple neural networks at no additional training cost. We achieve this goal by training a single neural network, converging to several local minima along its optimization path and saving the model parameters. To obtain repeated rapid convergence, we leverage recent work on cyclic learning rate schedules. The resulting technique, which we refer to as Snapshot Ensembling, is simple, yet surprisingly effective. We show in a series of experiments that our approach is compatible with diverse network architectures and learning tasks. It consistently yields lower error rates than state-of-the-art single models at no additional training cost, and compares favorably with traditional network ensembles. On CIFAR-10 and CIFAR-100 our DenseNet Snapshot Ensembles obtain error rates of 3.4\% and 17.4\% respectively.},
  urldate = {2018-03-09},
  date = {2017-03-31},
  keywords = {Computer Science - Learning},
  author = {Huang, Gao and Li, Yixuan and Pleiss, Geoff and Liu, Zhuang and Hopcroft, John E. and Weinberger, Kilian Q.},
  file = {/home/arthur/Dropbox/Zotero/Huang et al_2017_Snapshot Ensembles.pdf;/home/arthur/Zotero/storage/KHY5T2J6/1704.html}
}

@article{oord_neural_2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1711.00937},
  primaryClass = {cs},
  title = {Neural {{Discrete Representation Learning}}},
  url = {http://arxiv.org/abs/1711.00937},
  abstract = {Learning useful representations without supervision remains a key challenge in machine learning. In this paper, we propose a simple yet powerful generative model that learns such discrete representations. Our model, the Vector Quantised-Variational AutoEncoder (VQ-VAE), differs from VAEs in two key ways: the encoder network outputs discrete, rather than continuous, codes; and the prior is learnt rather than static. In order to learn a discrete latent representation, we incorporate ideas from vector quantisation (VQ). Using the VQ method allows the model to circumvent issues of "posterior collapse" -- where the latents are ignored when they are paired with a powerful autoregressive decoder -- typically observed in the VAE framework. Pairing these representations with an autoregressive prior, the model can generate high quality images, videos, and speech as well as doing high quality speaker conversion and unsupervised learning of phonemes, providing further evidence of the utility of the learnt representations.},
  urldate = {2018-03-19},
  date = {2017-11-02},
  keywords = {Computer Science - Learning},
  author = {van den Oord, Aaron and Vinyals, Oriol and Kavukcuoglu, Koray},
  file = {/home/arthur/Dropbox/Zotero/Oord et al_2017_Neural Discrete Representation Learning.pdf;/home/arthur/Zotero/storage/95XI7FZA/1711.html}
}

@inproceedings{pezeshki_deconstructing_2016,
  title = {Deconstructing the Ladder Network Architecture},
  date = {2016},
  pages = {2368--2376},
  author = {Pezeshki, Mohammad and Fan, Linxi and Brakel, Philemon and Courville, Aaron and Bengio, Yoshua},
  file = {/home/arthur/Zotero/storage/MWZVDUKU/Pezeshki et al. - 2016 - Deconstructing the ladder network architecture.html;/home/arthur/Zotero/storage/QZGWSET8/Pezeshki et al. - 2016 - Deconstructing the ladder network architecture.html}
}

@article{duchi_adaptive_2011,
  title = {Adaptive Subgradient Methods for Online Learning and Stochastic Optimization},
  volume = {12},
  issue = {Jul},
  journaltitle = {Journal of Machine Learning Research},
  date = {2011},
  pages = {2121--2159},
  author = {Duchi, John and Hazan, Elad and Singer, Yoram},
  file = {/home/arthur/Dropbox/Zotero/Duchi et al_2011_Adaptive subgradient methods for online learning and stochastic optimization.pdf}
}

@article{toussaint_lecture_2012,
  title = {Lecture {{Notes}}: {{Some}} Notes on Gradient Descent},
  shorttitle = {Lecture {{Notes}}},
  date = {2012},
  author = {Toussaint, Marc},
  file = {/home/arthur/Dropbox/Zotero/Toussaint_2012_Lecture Notes.pdf}
}

@article{frankle_lottery_2018,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1803.03635},
  primaryClass = {cs},
  title = {The {{Lottery Ticket Hypothesis}}: {{Training Pruned Neural Networks}}},
  url = {http://arxiv.org/abs/1803.03635},
  shorttitle = {The {{Lottery Ticket Hypothesis}}},
  abstract = {Recent work on neural network pruning indicates that, at training time, neural networks need to be significantly larger in size than is necessary to represent the eventual functions that they learn. This paper articulates a new hypothesis to explain this phenomenon. This conjecture, which we term the "lottery ticket hypothesis," proposes that successful training depends on lucky random initialization of a smaller subcomponent of the network. Larger networks have more of these "lottery tickets," meaning they are more likely to luck out with a subcomponent initialized in a configuration amenable to successful optimization. This paper conducts a series of experiments with XOR and MNIST that support the lottery ticket hypothesis. In particular, we identify these fortuitously-initialized subcomponents by pruning low-magnitude weights from trained networks. We then demonstrate that these subcomponents can be successfully retrained in isolation so long as the subnetworks are given the same initializations as they had at the beginning of the training process. Initialized as such, these small networks reliably converge successfully, often faster than the original network at the same level of accuracy. However, when these subcomponents are randomly reinitialized or rearranged, they perform worse than the original network. In other words, large networks that train successfully contain small subnetworks with initializations conducive to optimization. The lottery ticket hypothesis and its connection to pruning are a step toward developing architectures, initializations, and training strategies that make it possible to solve the same problems with much smaller networks.},
  urldate = {2018-03-22},
  date = {2018-03-09},
  keywords = {Computer Science - Learning,Computer Science - Neural and Evolutionary Computing,Computer Science - Artificial Intelligence},
  author = {Frankle, Jonathan and Carbin, Michael},
  file = {/home/arthur/Dropbox/Zotero/Frankle_Carbin_2018_The Lottery Ticket Hypothesis.pdf;/home/arthur/Zotero/storage/355GQYTY/1803.html}
}

@article{zhao_multiple_2018,
  title = {Multiple {{Source Domain Adaptation}} with {{Adversarial Learning}}},
  url = {https://openreview.net/forum?id=ryDNZZZAW},
  abstract = {While domain adaptation has been actively researched in recent years, most theoretical results and algorithms focus on the single-source-single-target adaptation setting. Naive application of such...},
  urldate = {2018-03-22},
  date = {2018-02-15},
  author = {Zhao, Han and Zhang, Shanghang and Wu, Guanhang and Costeira, Jo$\backslash$\textasciitilde\{a\}o P. and Moura, Jos$\backslash$'\{e\} M. F. and Gordon, Geoffrey J.},
  file = {/home/arthur/Dropbox/Zotero/Zhao et al_2018_Multiple Source Domain Adaptation with Adversarial Learning.pdf;/home/arthur/Zotero/storage/ISH72JND/forum.html}
}

@article{biau_theoretical_2018,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1803.07819},
  primaryClass = {cs, stat},
  title = {Some {{Theoretical Properties}} of {{GANs}}},
  url = {http://arxiv.org/abs/1803.07819},
  abstract = {Generative Adversarial Networks (GANs) are a class of generative algorithms that have been shown to produce state-of-the art samples, especially in the domain of image creation. The fundamental principle of GANs is to approximate the unknown distribution of a given data set by optimizing an objective function through an adversarial game between a family of generators and a family of discriminators. In this paper, we offer a better theoretical understanding of GANs by analyzing some of their mathematical and statistical properties. We study the deep connection between the adversarial principle underlying GANs and the Jensen-Shannon divergence, together with some optimality characteristics of the problem. An analysis of the role of the discriminator family via approximation arguments is also provided. In addition, taking a statistical point of view, we study the large sample properties of the estimated distribution and prove in particular a central limit theorem. Some of our results are illustrated with simulated examples.},
  urldate = {2018-03-23},
  date = {2018-03-21},
  keywords = {Computer Science - Learning,Statistics - Machine Learning},
  author = {Biau, G. and Cadre, B. and Sangnier, M. and Tanielian, U.},
  file = {/home/arthur/Dropbox/Zotero/Biau et al_2018_Some Theoretical Properties of GANs.pdf;/home/arthur/Zotero/storage/G2LYZXDW/1803.html}
}

@article{graham_efficient_2015,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1502.02478},
  primaryClass = {cs},
  title = {Efficient Batchwise Dropout Training Using Submatrices},
  url = {http://arxiv.org/abs/1502.02478},
  abstract = {Dropout is a popular technique for regularizing artificial neural networks. Dropout networks are generally trained by minibatch gradient descent with a dropout mask turning off some of the units---a different pattern of dropout is applied to every sample in the minibatch. We explore a very simple alternative to the dropout mask. Instead of masking dropped out units by setting them to zero, we perform matrix multiplication using a submatrix of the weight matrix---unneeded hidden units are never calculated. Performing dropout batchwise, so that one pattern of dropout is used for each sample in a minibatch, we can substantially reduce training times. Batchwise dropout can be used with fully-connected and convolutional neural networks.},
  urldate = {2018-03-27},
  date = {2015-02-09},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Neural and Evolutionary Computing},
  author = {Graham, Ben and Reizenstein, Jeremy and Robinson, Leigh},
  file = {/home/arthur/Dropbox/Zotero/Graham et al_2015_Efficient batchwise dropout training using submatrices.pdf;/home/arthur/Zotero/storage/TWB3Y83U/1502.html}
}

@article{fadili_model_2018,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1803.08381},
  primaryClass = {math},
  title = {Model {{Consistency}} for {{Learning}} with {{Mirror}}-{{Stratifiable Regularizers}}},
  url = {http://arxiv.org/abs/1803.08381},
  abstract = {Low-complexity non-smooth convex regularizers are routinely used to impose some structure (such as sparsity or low-rank) on the coefficients for linear methods in supervised learning.Model consistency (selecting the correct structure, for instance support or rank) is known to hold only if some non-degeneracy condition is imposed. This condition typically fails to hold for highly correlated designs and regularization methods tend to select larger models. In this work, we provide the theoretical underpinning of this behavior using the notion of mirror-stratifiable regularizers. This class of regularizers encompasses most well-known ones in the literature including the \$$\backslash$ell\_1\$ or trace norms. It enjoys a strong primal-dual relation between the models, which in turn allows one to locate the structure of the solution using a specific dual certificate. We also show how this analysis is applicable not only to solutions of an optimization problem, but also to the iterates computed by a certain class of stochastic proximal-gradient algorithms.},
  urldate = {2018-03-28},
  date = {2018-03-22},
  keywords = {Mathematics - Optimization and Control},
  author = {Fadili, Jalal and Garrigos, Guillaume and Malick, Jérome and Peyré, Gabriel},
  file = {/home/arthur/Dropbox/Zotero/Fadili et al_2018_Model Consistency for Learning with Mirror-Stratifiable Regularizers.pdf;/home/arthur/Zotero/storage/F8KX6MDM/1803.html}
}

@article{day_survey_2017,
  langid = {english},
  title = {A Survey on Heterogeneous Transfer Learning},
  volume = {4},
  issn = {2196-1115},
  url = {http://journalofbigdata.springeropen.com/articles/10.1186/s40537-017-0089-0},
  abstract = {Transfer learning has been demonstrated to be effective for many real-world applications as it exploits knowledge present in labeled training data from a source domain to enhance a model’s performance in a target domain, which has little or no labeled target training data. Utilizing a labeled source, or auxiliary, domain for aiding a target task can greatly reduce the cost and effort of collecting sufficient training labels to create an effective model in the new target distribution. Currently, most transfer learning methods assume the source and target domains consist of the same feature spaces which greatly limits their applications. This is because it may be difficult to collect auxiliary labeled source domain data that shares the same feature space as the target domain. Recently, heterogeneous transfer learning methods have been developed to address such limitations. This, in effect, expands the application of transfer learning to many other real-world tasks such as cross-language text categorization, text-to-image classification, and many others. Heterogeneous transfer learning is characterized by the source and target domains having differing feature spaces, but may also be combined with other issues such as differing data distributions and label spaces. These can present significant challenges, as one must develop a method to bridge the feature spaces, data distributions, and other gaps which may be present in these cross-domain learning tasks. This paper contributes a comprehensive survey and analysis of current methods designed for performing heterogeneous transfer learning tasks to provide an updated, centralized outlook into current methodologies.},
  number = {1},
  journaltitle = {Journal of Big Data},
  urldate = {2018-03-28},
  date = {2017-12},
  author = {Day, Oscar and Khoshgoftaar, Taghi M.},
  file = {/home/arthur/Dropbox/Zotero/Day_Khoshgoftaar_2017_A survey on heterogeneous transfer learning.pdf}
}

@article{morcos_importance_2018,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1803.06959},
  primaryClass = {cs, stat},
  title = {On the Importance of Single Directions for Generalization},
  url = {http://arxiv.org/abs/1803.06959},
  abstract = {Despite their ability to memorize large datasets, deep neural networks often achieve good generalization performance. However, the differences between the learned solutions of networks which generalize and those which do not remain unclear. Additionally, the tuning properties of single directions (defined as the activation of a single unit or some linear combination of units in response to some input) have been highlighted, but their importance has not been evaluated. Here, we connect these lines of inquiry to demonstrate that a network's reliance on single directions is a good predictor of its generalization performance, across networks trained on datasets with different fractions of corrupted labels, across ensembles of networks trained on datasets with unmodified labels, across different hyperparameters, and over the course of training. While dropout only regularizes this quantity up to a point, batch normalization implicitly discourages single direction reliance, in part by decreasing the class selectivity of individual units. Finally, we find that class selectivity is a poor predictor of task importance, suggesting not only that networks which generalize well minimize their dependence on individual units by reducing their selectivity, but also that individually selective units may not be necessary for strong network performance.},
  urldate = {2018-04-03},
  date = {2018-03-19},
  keywords = {Computer Science - Learning,Statistics - Machine Learning,Computer Science - Neural and Evolutionary Computing,Computer Science - Artificial Intelligence},
  author = {Morcos, Ari S. and Barrett, David G. T. and Rabinowitz, Neil C. and Botvinick, Matthew},
  file = {/home/arthur/Dropbox/Zotero/Morcos et al_2018_On the importance of single directions for generalization.pdf;/home/arthur/Zotero/storage/7N9P52GP/1803.html}
}

@article{cavazza_analysis_2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1710.03487},
  primaryClass = {cs, stat},
  title = {An {{Analysis}} of {{Dropout}} for {{Matrix Factorization}}},
  url = {http://arxiv.org/abs/1710.03487},
  abstract = {Dropout is a simple yet effective algorithm for regularizing neural networks by randomly dropping out units through Bernoulli multiplicative noise, and for some restricted problem classes, such as linear or logistic regression, several theoretical studies have demonstrated the equivalence between dropout and a fully deterministic optimization problem with data-dependent Tikhonov regularization. This work presents a theoretical analysis of dropout for matrix factorization, where Bernoulli random variables are used to drop a factor, thereby attempting to control the size of the factorization. While recent work has demonstrated the empirical effectiveness of dropout for matrix factorization, a theoretical understanding of the regularization properties of dropout in this context remains elusive. This work demonstrates the equivalence between dropout and a fully deterministic model for matrix factorization in which the factors are regularized by the sum of the product of the norms of the columns. While the resulting regularizer is closely related to a variational form of the nuclear norm, suggesting that dropout may limit the size of the factorization, we show that it is possible to trivially lower the objective value by doubling the size of the factorization. We show that this problem is caused by the use of a fixed dropout rate, which motivates the use of a rate that increases with the size of the factorization. Synthetic experiments validate our theoretical findings.},
  urldate = {2018-04-05},
  date = {2017-10-10},
  keywords = {Computer Science - Learning,Statistics - Machine Learning},
  author = {Cavazza, Jacopo and Lane, Connor and Haeffele, Benjamin D. and Murino, Vittorio and Vidal, René},
  file = {/home/arthur/Dropbox/Zotero/Cavazza et al_2017_An Analysis of Dropout for Matrix Factorization.pdf;/home/arthur/Zotero/storage/WWGDUX4H/1710.html}
}

@article{strub_hybrid_2016,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1603.00806},
  primaryClass = {cs},
  title = {Hybrid {{Collaborative Filtering}} with {{Autoencoders}}},
  url = {http://arxiv.org/abs/1603.00806},
  abstract = {Collaborative Filtering aims at exploiting the feedback of users to provide personalised recommendations. Such algorithms look for latent variables in a large sparse matrix of ratings. They can be enhanced by adding side information to tackle the well-known cold start problem. While Neu-ral Networks have tremendous success in image and speech recognition, they have received less attention in Collaborative Filtering. This is all the more surprising that Neural Networks are able to discover latent variables in large and heterogeneous datasets. In this paper, we introduce a Collaborative Filtering Neural network architecture aka CFN which computes a non-linear Matrix Factorization from sparse rating inputs and side information. We show experimentally on the MovieLens and Douban dataset that CFN outper-forms the state of the art and benefits from side information. We provide an implementation of the algorithm as a reusable plugin for Torch, a popular Neural Network framework.},
  urldate = {2018-04-05},
  date = {2016-03-02},
  keywords = {Computer Science - Neural and Evolutionary Computing,Computer Science - Artificial Intelligence,Computer Science - Information Retrieval},
  author = {Strub, Florian and Mary, Jeremie and Gaudel, Romaric},
  file = {/home/arthur/Dropbox/Zotero/Strub et al_2016_Hybrid Collaborative Filtering with Autoencoders.pdf;/home/arthur/Zotero/storage/DFCFVP8Q/1603.html}
}

@article{helmbold_surprising_nodate,
  langid = {english},
  title = {Surprising Properties of Dropout in Deep Networks},
  abstract = {We analyze dropout in deep networks with rectiﬁed linear units and the quadratic loss. Our results expose surprising differences between the behavior of dropout and more traditional regularizers like weight decay. For example, on some simple data sets dropout training produces negative weights even though the output is the sum of the inputs. This provides a counterpoint to the suggestion that dropout discourages co-adaptation of weights. We also show that the dropout penalty can grow exponentially in the depth of the network while the weight-decay penalty remains essentially linear, and that dropout is insensitive to various re-scalings of the input features, outputs, and network weights. This last insensitivity implies that there are no isolated local minima of the dropout training criterion. Our work uncovers new properties of dropout, extends our understanding of why dropout succeeds, and lays the foundation for further progress.},
  pages = {24},
  author = {Helmbold, David P},
  file = {/home/arthur/Dropbox/Zotero/Helmbold_Surprising properties of dropout in deep networks.pdf}
}

@incollection{baldi_understanding_2013,
  title = {Understanding {{Dropout}}},
  url = {http://papers.nips.cc/paper/4878-understanding-dropout.pdf},
  booktitle = {Advances in {{Neural Information Processing Systems}} 26},
  publisher = {{Curran Associates, Inc.}},
  urldate = {2018-04-05},
  date = {2013},
  pages = {2814--2822},
  author = {Baldi, Pierre and Sadowski, Peter J},
  editor = {Burges, C. J. C. and Bottou, L. and Welling, M. and Ghahramani, Z. and Weinberger, K. Q.},
  file = {/home/arthur/Dropbox/Zotero/Baldi_Sadowski_2013_Understanding Dropout.pdf;/home/arthur/Zotero/storage/IVKGYECY/4878-understanding-dropout.html}
}

@article{tzeng_adversarial_2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1702.05464},
  primaryClass = {cs},
  title = {Adversarial {{Discriminative Domain Adaptation}}},
  url = {http://arxiv.org/abs/1702.05464},
  abstract = {Adversarial learning methods are a promising approach to training robust deep networks, and can generate complex samples across diverse domains. They also can improve recognition despite the presence of domain shift or dataset bias: several adversarial approaches to unsupervised domain adaptation have recently been introduced, which reduce the difference between the training and test domain distributions and thus improve generalization performance. Prior generative approaches show compelling visualizations, but are not optimal on discriminative tasks and can be limited to smaller shifts. Prior discriminative approaches could handle larger domain shifts, but imposed tied weights on the model and did not exploit a GAN-based loss. We first outline a novel generalized framework for adversarial adaptation, which subsumes recent state-of-the-art approaches as special cases, and we use this generalized view to better relate the prior approaches. We propose a previously unexplored instance of our general framework which combines discriminative modeling, untied weight sharing, and a GAN loss, which we call Adversarial Discriminative Domain Adaptation (ADDA). We show that ADDA is more effective yet considerably simpler than competing domain-adversarial methods, and demonstrate the promise of our approach by exceeding state-of-the-art unsupervised adaptation results on standard cross-domain digit classification tasks and a new more difficult cross-modality object classification task.},
  urldate = {2018-04-06},
  date = {2017-02-17},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  author = {Tzeng, Eric and Hoffman, Judy and Saenko, Kate and Darrell, Trevor},
  file = {/home/arthur/Dropbox/Zotero/Tzeng et al_2017_Adversarial Discriminative Domain Adaptation.pdf;/home/arthur/Zotero/storage/SCKBPYIE/1702.html}
}

@article{li_revisiting_2016,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1603.04779},
  primaryClass = {cs},
  title = {Revisiting {{Batch Normalization For Practical Domain Adaptation}}},
  url = {http://arxiv.org/abs/1603.04779},
  abstract = {Deep neural networks (DNN) have shown unprecedented success in various computer vision applications such as image classification and object detection. However, it is still a common annoyance during the training phase, that one has to prepare at least thousands of labeled images to fine-tune a network to a specific domain. Recent study (Tommasi et al. 2015) shows that a DNN has strong dependency towards the training dataset, and the learned features cannot be easily transferred to a different but relevant task without fine-tuning. In this paper, we propose a simple yet powerful remedy, called Adaptive Batch Normalization (AdaBN) to increase the generalization ability of a DNN. By modulating the statistics in all Batch Normalization layers across the network, our approach achieves deep adaptation effect for domain adaptation tasks. In contrary to other deep learning domain adaptation methods, our method does not require additional components, and is parameter-free. It archives state-of-the-art performance despite its surprising simplicity. Furthermore, we demonstrate that our method is complementary with other existing methods. Combining AdaBN with existing domain adaptation treatments may further improve model performance.},
  urldate = {2018-04-06},
  date = {2016-03-15},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Learning},
  author = {Li, Yanghao and Wang, Naiyan and Shi, Jianping and Liu, Jiaying and Hou, Xiaodi},
  file = {/home/arthur/Dropbox/Zotero/Li et al_2016_Revisiting Batch Normalization For Practical Domain Adaptation.pdf;/home/arthur/Zotero/storage/2KG8CXTL/1603.html}
}

@article{chouzenoux_block_2016,
  langid = {english},
  title = {A Block Coordinate Variable Metric Forward–Backward Algorithm},
  volume = {66},
  issn = {0925-5001, 1573-2916},
  url = {https://link.springer.com/article/10.1007/s10898-016-0405-9},
  abstract = {A number of recent works have emphasized the prominent role played by the Kurdyka-Łojasiewicz inequality for proving the convergence of iterative algorithms solving possibly nonsmooth/nonconvex optimization problems. In this work, we consider the minimization of an objective function satisfying this property, which is a sum of two terms: (i) a differentiable, but not necessarily convex, function and (ii) a function that is not necessarily convex, nor necessarily differentiable. The latter function is expressed as a separable sum of functions of blocks of variables. Such an optimization problem can be addressed with the Forward–Backward algorithm which can be accelerated thanks to the use of variable metrics derived from the Majorize–Minimize principle. We propose to combine the latter acceleration technique with an alternating minimization strategy which relies upon a flexible update rule. We give conditions under which the sequence generated by the resulting Block Coordinate Variable Metric Forward–Backward algorithm converges to a critical point of the objective function. An application example to a nonconvex phase retrieval problem encountered in signal/image processing shows the efficiency of the proposed optimization method.},
  number = {3},
  journaltitle = {Journal of Global Optimization},
  shortjournal = {J Glob Optim},
  urldate = {2018-04-07},
  date = {2016-11-01},
  pages = {457-485},
  author = {Chouzenoux, Emilie and Pesquet, Jean-Christophe and Repetti, Audrey},
  file = {/home/arthur/Zotero/storage/28NXI9VN/s10898-016-0405-9.html}
}

@article{goyal_continuous_2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1708.00111},
  primaryClass = {cs},
  title = {A {{Continuous Relaxation}} of {{Beam Search}} for {{End}}-to-End {{Training}} of {{Neural Sequence Models}}},
  url = {http://arxiv.org/abs/1708.00111},
  abstract = {Beam search is a desirable choice of test-time decoding algorithm for neural sequence models because it potentially avoids search errors made by simpler greedy methods. However, typical cross entropy training procedures for these models do not directly consider the behaviour of the final decoding method. As a result, for cross-entropy trained models, beam decoding can sometimes yield reduced test performance when compared with greedy decoding. In order to train models that can more effectively make use of beam search, we propose a new training procedure that focuses on the final loss metric (e.g. Hamming loss) evaluated on the output of beam search. While well-defined, this "direct loss" objective is itself discontinuous and thus difficult to optimize. Hence, in our approach, we form a sub-differentiable surrogate objective by introducing a novel continuous approximation of the beam search decoding procedure. In experiments, we show that optimizing this new training objective yields substantially better results on two sequence tasks (Named Entity Recognition and CCG Supertagging) when compared with both cross entropy trained greedy decoding and cross entropy trained beam decoding baselines.},
  urldate = {2018-04-11},
  date = {2017-07-31},
  keywords = {Computer Science - Learning,Computer Science - Computation and Language,Computer Science - Neural and Evolutionary Computing,I.2.6,I.2.7},
  author = {Goyal, Kartik and Neubig, Graham and Dyer, Chris and Berg-Kirkpatrick, Taylor},
  file = {/home/arthur/Dropbox/Zotero/Goyal et al_2017_A Continuous Relaxation of Beam Search for End-to-end Training of Neural.pdf;/home/arthur/Zotero/storage/YZ3VCZ5A/1708.html}
}

@article{bengio_practical_2012,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1206.5533},
  primaryClass = {cs},
  title = {Practical Recommendations for Gradient-Based Training of Deep Architectures},
  url = {http://arxiv.org/abs/1206.5533},
  abstract = {Learning algorithms related to artificial neural networks and in particular for Deep Learning may seem to involve many bells and whistles, called hyper-parameters. This chapter is meant as a practical guide with recommendations for some of the most commonly used hyper-parameters, in particular in the context of learning algorithms based on back-propagated gradient and gradient-based optimization. It also discusses how to deal with the fact that more interesting results can be obtained when allowing one to adjust many hyper-parameters. Overall, it describes elements of the practice used to successfully and efficiently train and debug large-scale and often deep multi-layer neural networks. It closes with open questions about the training difficulties observed with deeper architectures.},
  urldate = {2018-04-12},
  date = {2012-06-24},
  keywords = {Computer Science - Learning},
  author = {Bengio, Yoshua},
  file = {/home/arthur/Dropbox/Zotero/Bengio_2012_Practical recommendations for gradient-based training of deep architectures.pdf;/home/arthur/Zotero/storage/YEU22Y32/1206.html}
}

@article{scardapane_group_2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1607.00485},
  title = {Group {{Sparse Regularization}} for {{Deep Neural Networks}}},
  volume = {241},
  issn = {09252312},
  url = {http://arxiv.org/abs/1607.00485},
  abstract = {In this paper, we consider the joint task of simultaneously optimizing (i) the weights of a deep neural network, (ii) the number of neurons for each hidden layer, and (iii) the subset of active input features (i.e., feature selection). While these problems are generally dealt with separately, we present a simple regularized formulation allowing to solve all three of them in parallel, using standard optimization routines. Specifically, we extend the group Lasso penalty (originated in the linear regression literature) in order to impose group-level sparsity on the network's connections, where each group is defined as the set of outgoing weights from a unit. Depending on the specific case, the weights can be related to an input variable, to a hidden neuron, or to a bias unit, thus performing simultaneously all the aforementioned tasks in order to obtain a compact network. We perform an extensive experimental evaluation, by comparing with classical weight decay and Lasso penalties. We show that a sparse version of the group Lasso penalty is able to achieve competitive performances, while at the same time resulting in extremely compact networks with a smaller number of input features. We evaluate both on a toy dataset for handwritten digit recognition, and on multiple realistic large-scale classification problems.},
  journaltitle = {Neurocomputing},
  urldate = {2018-04-12},
  date = {2017-06},
  pages = {81-89},
  keywords = {Computer Science - Learning,Statistics - Machine Learning},
  author = {Scardapane, Simone and Comminiello, Danilo and Hussain, Amir and Uncini, Aurelio},
  file = {/home/arthur/Dropbox/Zotero/Scardapane et al_2017_Group Sparse Regularization for Deep Neural Networks.pdf;/home/arthur/Zotero/storage/NZBCLEJP/1607.html}
}

@article{blei_variational_2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1601.00670},
  title = {Variational {{Inference}}: {{A Review}} for {{Statisticians}}},
  volume = {112},
  issn = {0162-1459, 1537-274X},
  url = {http://arxiv.org/abs/1601.00670},
  shorttitle = {Variational {{Inference}}},
  abstract = {One of the core problems of modern statistics is to approximate difficult-to-compute probability densities. This problem is especially important in Bayesian statistics, which frames all inference about unknown quantities as a calculation involving the posterior density. In this paper, we review variational inference (VI), a method from machine learning that approximates probability densities through optimization. VI has been used in many applications and tends to be faster than classical methods, such as Markov chain Monte Carlo sampling. The idea behind VI is to first posit a family of densities and then to find the member of that family which is close to the target. Closeness is measured by Kullback-Leibler divergence. We review the ideas behind mean-field variational inference, discuss the special case of VI applied to exponential family models, present a full example with a Bayesian mixture of Gaussians, and derive a variant that uses stochastic optimization to scale up to massive data. We discuss modern research in VI and highlight important open problems. VI is powerful, but it is not yet well understood. Our hope in writing this paper is to catalyze statistical research on this class of algorithms.},
  number = {518},
  journaltitle = {Journal of the American Statistical Association},
  urldate = {2018-04-20},
  date = {2017-04-03},
  pages = {859-877},
  keywords = {Computer Science - Learning,Statistics - Computation,Statistics - Machine Learning},
  author = {Blei, David M. and Kucukelbir, Alp and McAuliffe, Jon D.},
  file = {/home/arthur/Dropbox/Zotero/Blei et al_2017_Variational Inference.pdf;/home/arthur/Zotero/storage/KZTI284Y/1601.html}
}

@article{blundell_weight_2015,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1505.05424},
  primaryClass = {cs, stat},
  title = {Weight {{Uncertainty}} in {{Neural Networks}}},
  url = {http://arxiv.org/abs/1505.05424},
  abstract = {We introduce a new, efficient, principled and backpropagation-compatible algorithm for learning a probability distribution on the weights of a neural network, called Bayes by Backprop. It regularises the weights by minimising a compression cost, known as the variational free energy or the expected lower bound on the marginal likelihood. We show that this principled kind of regularisation yields comparable performance to dropout on MNIST classification. We then demonstrate how the learnt uncertainty in the weights can be used to improve generalisation in non-linear regression problems, and how this weight uncertainty can be used to drive the exploration-exploitation trade-off in reinforcement learning.},
  urldate = {2018-04-20},
  date = {2015-05-20},
  keywords = {Computer Science - Learning,Statistics - Machine Learning},
  author = {Blundell, Charles and Cornebise, Julien and Kavukcuoglu, Koray and Wierstra, Daan},
  file = {/home/arthur/Dropbox/Zotero/Blundell et al_2015_Weight Uncertainty in Neural Networks.pdf;/home/arthur/Zotero/storage/WTQ4TAUS/1505.html}
}

@article{neklyudov_variance_2018,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1803.03764},
  primaryClass = {stat},
  title = {Variance {{Networks}}: {{When Expectation Does Not Meet Your Expectations}}},
  url = {http://arxiv.org/abs/1803.03764},
  shorttitle = {Variance {{Networks}}},
  abstract = {In this paper, we propose variance networks, a new model that stores the learned information in the variances of the network weights. Surprisingly, no information gets stored in the expectations of the weights, therefore if we replace these weights with their expectations, we would obtain a random guess quality prediction. We provide a numerical criterion that uses the loss curvature to determine which random variables can be replaced with their expected values, and find that only a small fraction of weights is needed for ensembling. Variance networks represent a diverse ensemble that is more robust to adversarial attacks than conventional low-variance ensembles. The success of this model raises several counter-intuitive implications for the training and application of Deep Learning models.},
  urldate = {2018-04-20},
  date = {2018-03-10},
  keywords = {Statistics - Machine Learning},
  author = {Neklyudov, Kirill and Molchanov, Dmitry and Ashukha, Arsenii and Vetrov, Dmitry},
  file = {/home/arthur/Dropbox/Zotero/Neklyudov et al_2018_Variance Networks.pdf;/home/arthur/Zotero/storage/ZDHJVYF9/1803.html}
}

@article{hron_variational_2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1711.02989},
  primaryClass = {stat},
  title = {Variational {{Gaussian Dropout}} Is Not {{Bayesian}}},
  url = {http://arxiv.org/abs/1711.02989},
  abstract = {Gaussian multiplicative noise is commonly used as a stochastic regularisation technique in training of deterministic neural networks. A recent paper reinterpreted the technique as a specific algorithm for approximate inference in Bayesian neural networks; several extensions ensued. We show that the log-uniform prior used in all the above publications does not generally induce a proper posterior, and thus Bayesian inference in such models is ill-posed. Independent of the log-uniform prior, the correlated weight noise approximation has further issues leading to either infinite objective or high risk of overfitting. The above implies that the reported sparsity of obtained solutions cannot be explained by Bayesian or the related minimum description length arguments. We thus study the objective from a non-Bayesian perspective, provide its previously unknown analytical form which allows exact gradient evaluation, and show that the later proposed additive reparametrisation introduces minima not present in the original multiplicative parametrisation. Implications and future research directions are discussed.},
  urldate = {2018-04-20},
  date = {2017-11-08},
  keywords = {Statistics - Machine Learning},
  author = {Hron, Jiri and Matthews, Alexander G. de G. and Ghahramani, Zoubin},
  file = {/home/arthur/Dropbox/Zotero/Hron et al_2017_Variational Gaussian Dropout is not Bayesian.pdf;/home/arthur/Zotero/storage/94SJJQEL/1711.html}
}

@article{sonderby_how_2016,
  title = {How to Train Deep Variational Autoencoders and Probabilistic Ladder Networks},
  journaltitle = {arXiv preprint arXiv:1602.02282},
  date = {2016},
  author = {Sønderby, Casper Kaae and Raiko, Tapani and Maaløe, Lars and Sønderby, Søren Kaae and Winther, Ole},
  file = {/home/arthur/Dropbox/Zotero/Sønderby et al_2016_How to train deep variational autoencoders and probabilistic ladder networks.pdf}
}

@article{chris_hans_elastic_2011,
  eprinttype = {jstor},
  eprint = {23239545},
  langid = {english},
  title = {Elastic {{Net Regression Modeling With}} the {{Orthant Normal Prior}}},
  volume = {106},
  number = {496},
  journaltitle = {Journal of the American Statistical Association},
  date = {2011},
  author = {Chris Hans},
  file = {/home/arthur/Dropbox/Zotero/Chris Hans_2011_Elastic Net Regression Modeling With the Orthant Normal Prior.pdf}
}

@book{garwin_century_2010,
  langid = {english},
  title = {A {{Century}} of {{Nature}}: {{Twenty}}-{{One Discoveries}} That {{Changed Science}} and the {{World}}},
  isbn = {978-0-226-28416-3},
  shorttitle = {A {{Century}} of {{Nature}}},
  abstract = {Many of the scientific breakthroughs of the twentieth century were first reported in the journal Nature. A Century of Nature brings together in one volume Nature's greatest hits—reproductions of seminal contributions that changed science and the world, accompanied by essays written by leading scientists (including four Nobel laureates) that provide historical context for each article, explain its insights in graceful, accessible prose, and celebrate the serendipity of discovery and the rewards of searching for needles in haystacks.},
  pagetotal = {382},
  publisher = {{University of Chicago Press}},
  date = {2010-03-15},
  keywords = {Science / General,Science / History},
  author = {Garwin, Laura and Lincoln, Tim},
  eprinttype = {googlebooks}
}

@article{lauterbur_image_1973,
  title = {Image Formation by Induced Local Interactions: Examples Employing Nuclear Magnetic Resonance},
  shorttitle = {Image Formation by Induced Local Interactions},
  date = {1973},
  author = {Lauterbur, P. C.},
  file = {/home/arthur/Zotero/storage/5JE89AIX/books.html}
}

@article{laruterbur_image_1973,
  title = {Image {{Formation}} by {{Induced Local Interactions}}: {{Examples Employing Nuclear Magnetic Resonance}}},
  volume = {242},
  shorttitle = {Image {{Formation}} by {{Induced Local Interactions}}},
  number = {5394},
  journaltitle = {Nature},
  date = {1973},
  pages = {190},
  author = {Laruterbur, PC},
  file = {/home/arthur/Zotero/storage/LT858HQM/242190a0.html}
}

@article{ogawa_brain_1990,
  title = {Brain Magnetic Resonance Imaging with Contrast Dependent on Blood Oxygenation},
  volume = {87},
  number = {24},
  journaltitle = {Proceedings of the National Academy of Sciences},
  date = {1990},
  pages = {9868--9872},
  author = {Ogawa, Seiji and Lee, Tso-Ming and Kay, Alan R. and Tank, David W.},
  file = {/home/arthur/Zotero/storage/R4LHIMEY/9868.html}
}

@article{pedregosa_data-driven_2015,
  langid = {english},
  title = {Data-Driven {{HRF}} Estimation for Encoding and Decoding Models},
  volume = {104},
  issn = {1095-9572},
  doi = {10.1016/j.neuroimage.2014.09.060},
  abstract = {Despite the common usage of a canonical, data-independent, hemodynamic response function (HRF), it is known that the shape of the HRF varies across brain regions and subjects. This suggests that a data-driven estimation of this function could lead to more statistical power when modeling BOLD fMRI data. However, unconstrained estimation of the HRF can yield highly unstable results when the number of free parameters is large. We develop a method for the joint estimation of activation and HRF by means of a rank constraint, forcing the estimated HRF to be equal across events or experimental conditions, yet permitting it to differ across voxels. Model estimation leads to an optimization problem that we propose to solve with an efficient quasi-Newton method, exploiting fast gradient computations. This model, called GLM with Rank-1 constraint (R1-GLM), can be extended to the setting of GLM with separate designs which has been shown to improve decoding accuracy in brain activity decoding experiments. We compare 10 different HRF modeling methods in terms of encoding and decoding scores on two different datasets. Our results show that the R1-GLM model outperforms competing methods in both encoding and decoding settings, positioning it as an attractive method both from the points of view of accuracy and computational efficiency.},
  journaltitle = {NeuroImage},
  shortjournal = {Neuroimage},
  date = {2015-01-01},
  pages = {209-220},
  keywords = {Brain,Brain Mapping,Humans,Image Processing; Computer-Assisted,Magnetic Resonance Imaging,Models; Neurological,Optimization,Decoding,Encoding,BOLD,Finite impulse response (FIR),Functional MRI (fMRI),Hemodynamic response function (HRF),Machine learning,Machine Learning,Neurovascular Coupling,Regression Analysis,Visual Perception},
  author = {Pedregosa, Fabian and Eickenberg, Michael and Ciuciu, Philippe and Thirion, Bertrand and Gramfort, Alexandre},
  eprinttype = {pmid},
  eprint = {25304775}
}

@article{lindquist_modeling_2009,
  title = {Modeling the Hemodynamic Response Function in {{fMRI}}: Efficiency, Bias and Mis-Modeling},
  volume = {45},
  shorttitle = {Modeling the Hemodynamic Response Function in {{fMRI}}},
  number = {1},
  journaltitle = {Neuroimage},
  date = {2009},
  pages = {S187--S198},
  author = {Lindquist, Martin A. and Loh, Ji Meng and Atlas, Lauren Y. and Wager, Tor D.},
  file = {/home/arthur/Zotero/storage/3F7Y62LF/PMC3318970.html;/home/arthur/Zotero/storage/ZJQBBX9Q/S1053811908012056.html}
}

@article{wu_changes_2009,
  title = {Changes of Functional Connectivity of the Motor Network in the Resting State in {{Parkinson}}'s Disease},
  volume = {460},
  number = {1},
  journaltitle = {Neuroscience letters},
  date = {2009},
  pages = {6--10},
  author = {Wu, Tao and Wang, Liang and Chen, Yi and Zhao, Cheng and Li, Kuncheng and Chan, Piu},
  file = {/home/arthur/Dropbox/Zotero/Wu et al_2009_Changes of functional connectivity of the motor network in the resting state in.pdf;/home/arthur/Zotero/storage/NK9HHKY6/S0304394009006703.html}
}

@article{weng_alterations_2010,
  title = {Alterations of Resting State Functional Connectivity in the Default Network in Adolescents with Autism Spectrum Disorders},
  volume = {1313},
  journaltitle = {Brain research},
  date = {2010},
  pages = {202--214},
  author = {Weng, Shih-Jen and Wiggins, Jillian Lee and Peltier, Scott J. and Carrasco, Melisa and Risi, Susan and Lord, Catherine and Monk, Christopher S.},
  file = {/home/arthur/Zotero/storage/3H6DGVDT/PMC2818723.html;/home/arthur/Zotero/storage/JG9FF4IX/S0006899309025736.html}
}

@article{yu-feng_altered_2007,
  title = {Altered Baseline Brain Activity in Children with {{ADHD}} Revealed by Resting-State Functional {{MRI}}},
  volume = {29},
  number = {2},
  journaltitle = {Brain and Development},
  date = {2007},
  pages = {83--91},
  author = {Yu-Feng, Zang and Yong, He and Chao-Zhe, Zhu and Qing-Jiu, Cao and Man-Qiu, Sui and Meng, Liang and Li-Xia, Tian and Tian-Zi, Jiang and Yu-Feng, Wang},
  file = {/home/arthur/Dropbox/Zotero/Yu-Feng et al_2007_Altered baseline brain activity in children with ADHD revealed by resting-state.pdf;/home/arthur/Zotero/storage/NI93TRQ8/Yu-Feng et al. - 2007 - Altered baseline brain activity in children with A}
}

@inproceedings{rahim_population-shrinkage_2017,
  title = {Population-Shrinkage of Covariance to Estimate Better Brain Functional Connectivity},
  booktitle = {International {{Conference}} on {{Medical Image Computing}} and {{Computer}}-{{Assisted Intervention}}},
  publisher = {{Springer}},
  date = {2017},
  pages = {460--468},
  author = {Rahim, Mehdi and Thirion, Bertrand and Varoquaux, Gaël},
  file = {/home/arthur/Dropbox/Zotero/Rahim et al_2017_Population-shrinkage of covariance to estimate better brain functional.pdf;/home/arthur/Zotero/storage/UPH2JCQV/978-3-319-66182-7_53.html}
}

@inproceedings{xu_regularized_2012,
  title = {Regularized Hyperalignment of Multi-Set Fmri Data},
  booktitle = {Statistical {{Signal Processing Workshop}} ({{SSP}}), 2012 {{IEEE}}},
  publisher = {{IEEE}},
  date = {2012},
  pages = {229--232},
  author = {Xu, Hao and Lorbert, Alexander and Ramadge, Peter J. and Guntupalli, J. Swaroop and Haxby, James V.},
  file = {/home/arthur/Zotero/storage/AR7R4VEI/6319668.html}
}

@article{fox_spontaneous_2007,
  title = {Spontaneous Fluctuations in Brain Activity Observed with Functional Magnetic Resonance Imaging},
  volume = {8},
  number = {9},
  journaltitle = {Nature reviews neuroscience},
  date = {2007},
  pages = {700},
  author = {Fox, Michael D. and Raichle, Marcus E.},
  file = {/home/arthur/Dropbox/Zotero/Fox_Raichle_2007_Spontaneous fluctuations in brain activity observed with functional magnetic.pdf;/home/arthur/Zotero/storage/EVTMZJW4/nrn2201.html}
}

@article{sabuncu_function-based_2009,
  title = {Function-Based Intersubject Alignment of Human Cortical Anatomy},
  volume = {20},
  number = {1},
  journaltitle = {Cerebral cortex},
  date = {2009},
  pages = {130--140},
  author = {Sabuncu, Mert R. and Singer, Benjamin D. and Conroy, Bryan and Bryan, Ronald E. and Ramadge, Peter J. and Haxby, James V.},
  file = {/home/arthur/Zotero/storage/9SWBIFQX/416240.html;/home/arthur/Zotero/storage/ZFC78EMM/416240.html}
}

@article{greicius_functional_2003,
  title = {Functional Connectivity in the Resting Brain: A Network Analysis of the Default Mode Hypothesis},
  volume = {100},
  shorttitle = {Functional Connectivity in the Resting Brain},
  number = {1},
  journaltitle = {Proceedings of the National Academy of Sciences},
  date = {2003},
  pages = {253--258},
  author = {Greicius, Michael D. and Krasnow, Ben and Reiss, Allan L. and Menon, Vinod},
  file = {/home/arthur/Zotero/storage/BHCRWBKI/253.html;/home/arthur/Zotero/storage/N8YP2FVL/253.html}
}

@article{roy_functional_2009,
  title = {Functional Connectivity of the Human Amygdala Using Resting State {{fMRI}}},
  volume = {45},
  number = {2},
  journaltitle = {Neuroimage},
  date = {2009},
  pages = {614--626},
  author = {Roy, Amy Krain and Shehzad, Zarrar and Margulies, Daniel S. and Kelly, AM Clare and Uddin, Lucina Q. and Gotimer, Kristin and Biswal, Bharat B. and Castellanos, F. Xavier and Milham, Michael P.},
  file = {/home/arthur/Zotero/storage/APVCQC6M/PMC2735022.html;/home/arthur/Zotero/storage/QJVGA2A4/S1053811908012214.html}
}

@article{fonov_unbiased_2011,
  title = {Unbiased Average Age-Appropriate Atlases for Pediatric Studies},
  volume = {54},
  issn = {1053-8119},
  url = {http://www.sciencedirect.com/science/article/pii/S1053811910010062},
  abstract = {Spatial normalization, registration, and segmentation techniques for Magnetic Resonance Imaging (MRI) often use a target or template volume to facilitate processing, take advantage of prior information, and define a common coordinate system for analysis. In the neuroimaging literature, the MNI305 Talairach-like coordinate system is often used as a standard template. However, when studying pediatric populations, variation from the adult brain makes the MNI305 suboptimal for processing brain images of children. Morphological changes occurring during development render the use of age-appropriate templates desirable to reduce potential errors and minimize bias during processing of pediatric data. This paper presents the methods used to create unbiased, age-appropriate MRI atlas templates for pediatric studies that represent the average anatomy for the age range of 4.5–18.5years, while maintaining a high level of anatomical detail and contrast. The creation of anatomical T1-weighted, T2-weighted, and proton density-weighted templates for specific developmentally important age-ranges, used data derived from the largest epidemiological, representative (healthy and normal) sample of the U.S. population, where each subject was carefully screened for medical and psychiatric factors and characterized using established neuropsychological and behavioral assessments. Use of these age-specific templates was evaluated by computing average tissue maps for gray matter, white matter, and cerebrospinal fluid for each specific age range, and by conducting an exemplar voxel-wise deformation-based morphometry study using 66 young (4.5–6.9years) participants to demonstrate the benefits of using the age-appropriate templates. The public availability of these atlases/templates will facilitate analysis of pediatric MRI data and enable comparison of results between studies in a common standardized space specific to pediatric research.},
  number = {1},
  journaltitle = {NeuroImage},
  shortjournal = {NeuroImage},
  urldate = {2018-05-08},
  date = {2011-01-01},
  pages = {313-327},
  keywords = {Atlas template,Pediatric image analysis,Registration},
  author = {Fonov, Vladimir and Evans, Alan C. and Botteron, Kelly and Almli, C. Robert and McKinstry, Robert C. and Collins, D. Louis},
  file = {/home/arthur/Zotero/storage/QMIIFQKZ/S1053811910010062.html}
}

@article{friston_statistical_1994,
  title = {Statistical Parametric Maps in Functional Imaging: A General Linear Approach},
  volume = {2},
  shorttitle = {Statistical Parametric Maps in Functional Imaging},
  number = {4},
  journaltitle = {Human brain mapping},
  date = {1994},
  pages = {189--210},
  author = {Friston, Karl J. and Holmes, Andrew P. and Worsley, Keith J. and Poline, J.-P. and Frith, Chris D. and Frackowiak, Richard SJ},
  file = {/home/arthur/Dropbox/Zotero/Friston et al_1994_Statistical parametric maps in functional imaging.pdf;/home/arthur/Zotero/storage/EIDUIM5R/hbm.html}
}

@article{naselaris_encoding_2011,
  langid = {english},
  title = {Encoding and Decoding in {{fMRI}}},
  volume = {56},
  issn = {10538119},
  url = {http://linkinghub.elsevier.com/retrieve/pii/S1053811910010657},
  abstract = {Over the past decade fMRI researchers have developed increasingly sensitive techniques for analyzing the information represented in BOLD activity. The most popular of these techniques is linear classiﬁcation, a simple technique for decoding information about experimental stimuli or tasks from patterns of activity across an array of voxels. A more recent development is the voxel-based encoding model, which describes the information about the stimulus or task that is represented in the activity of single voxels. Encoding and decoding are complementary operations: encoding uses stimuli to predict activity while decoding uses activity to predict information about the stimuli. However, in practice these two operations are often confused, and their respective strengths and weaknesses have not been made clear. Here we use the concept of a linearizing feature space to clarify the relationship between encoding and decoding. We show that encoding and decoding operations can both be used to investigate some of the most common questions about how information is represented in the brain. However, focusing on encoding models offers two important advantages over decoding. First, an encoding model can in principle provide a complete functional description of a region of interest, while a decoding model can provide only a partial description. Second, while it is straightforward to derive an optimal decoding model from an encoding model it is much more difﬁcult to derive an encoding model from a decoding model. We propose a systematic modeling approach that begins by estimating an encoding model for every voxel in a scan and ends by using the estimated encoding models to perform decoding.},
  number = {2},
  journaltitle = {NeuroImage},
  urldate = {2018-05-08},
  date = {2011-05},
  pages = {400-410},
  author = {Naselaris, Thomas and Kay, Kendrick N. and Nishimoto, Shinji and Gallant, Jack L.},
  file = {/home/arthur/Dropbox/Zotero/Naselaris et al_2011_Encoding and decoding in fMRI.pdf}
}

@article{cox_functional_2003,
  langid = {english},
  title = {Functional Magnetic Resonance Imaging ({{fMRI}}) "Brain Reading": Detecting and Classifying Distributed Patterns of {{fMRI}} Activity in Human Visual Cortex},
  volume = {19},
  issn = {1053-8119},
  shorttitle = {Functional Magnetic Resonance Imaging ({{fMRI}}) "Brain Reading"},
  abstract = {Traditional (univariate) analysis of functional MRI (fMRI) data relies exclusively on the information contained in the time course of individual voxels. Multivariate analyses can take advantage of the information contained in activity patterns across space, from multiple voxels. Such analyses have the potential to greatly expand the amount of information extracted from fMRI data sets. In the present study, multivariate statistical pattern recognition methods, including linear discriminant analysis and support vector machines, were used to classify patterns of fMRI activation evoked by the visual presentation of various categories of objects. Classifiers were trained using data from voxels in predefined regions of interest during a subset of trials for each subject individually. Classification of subsequently collected fMRI data was attempted according to the similarity of activation patterns to prior training examples. Classification was done using only small amounts of data (20 s worth) at a time, so such a technique could, in principle, be used to extract information about a subject's percept on a near real-time basis. Classifiers trained on data acquired during one session were equally accurate in classifying data collected within the same session and across sessions separated by more than a week, in the same subject. Although the highest classification accuracies were obtained using patterns of activity including lower visual areas as input, classification accuracies well above chance were achieved using regions of interest restricted to higher-order object-selective visual areas. In contrast to typical fMRI data analysis, in which hours of data across many subjects are averaged to reveal slight differences in activation, the use of pattern recognition methods allows a subtle 10-way discrimination to be performed on an essentially trial-by-trial basis within individuals, demonstrating that fMRI data contain far more information than is typically appreciated.},
  issue = {2 Pt 1},
  journaltitle = {NeuroImage},
  shortjournal = {Neuroimage},
  date = {2003-06},
  pages = {261-270},
  keywords = {Adult,Brain Mapping,Female,Humans,Image Processing; Computer-Assisted,Magnetic Resonance Imaging,Male,Sensitivity and Specificity,Discrimination Learning,Mathematical Computing,Middle Aged,Multivariate Analysis,Pattern Recognition; Visual,Regional Blood Flow,Visual Cortex},
  author = {Cox, David D. and Savoy, Robert L.},
  file = {/home/arthur/Dropbox/Zotero/Cox_Savoy_2003_Functional magnetic resonance imaging (fMRI) brain reading.pdf},
  eprinttype = {pmid},
  eprint = {12814577}
}

@article{benjamini_controlling_1995,
  title = {Controlling the False Discovery Rate: A Practical and Powerful Approach to Multiple Testing},
  shorttitle = {Controlling the False Discovery Rate},
  journaltitle = {Journal of the royal statistical society. Series B (Methodological)},
  date = {1995},
  pages = {289--300},
  author = {Benjamini, Yoav and Hochberg, Yosef},
  file = {/home/arthur/Dropbox/Zotero/Benjamini_Hochberg_1995_Controlling the false discovery rate.pdf;/home/arthur/Zotero/storage/IH9K72YM/2346101.html}
}

@article{nichols_controlling_2003,
  langid = {english},
  title = {Controlling the Familywise Error Rate in Functional Neuroimaging: A                 Comparative Review},
  volume = {12},
  issn = {0962-2802},
  url = {https://doi.org/10.1191/0962280203sm341ra},
  shorttitle = {Controlling the Familywise Error Rate in Functional Neuroimaging},
  abstract = {Functional neuroimaging data embodies a massive multiple testing problem, where 100                 000 correlated test statistics must be assessed. The familywise error rate, the                 chance of any false positives is the standard measure of Type I errors in multiple                 testing. In this paper we review and evaluate three approaches to thresholding                 images of test statistics: Bonferroni, random field and the permutation test. Owing                 to recent developments, improved Bonferroni procedures, such as Hochberg’s                 methods, are now applicable to dependent data. Continuous random field methods use                 the smoothness of the image to adapt to the severity of the multiple testing                 problem. Also, increased computing power has made both permutation and bootstrap                 methods applicable to functional neuroimaging. We evaluate these approaches on t                 images using simulations and a collection of real datasets. We find that                 Bonferroni-related tests offer little improvement over Bonferroni, while the                 permutation method offers substantial improvement over the random field method for                 low smoothness and low degrees of freedom. We also show the limitations of trying to                 find an equivalent number of independent tests for an image of correlated test statistics.},
  number = {5},
  journaltitle = {Statistical Methods in Medical Research},
  shortjournal = {Stat Methods Med Res},
  urldate = {2018-05-08},
  date = {2003-10-01},
  pages = {419-446},
  author = {Nichols, Thomas and Hayasaka, Satoru},
  file = {/home/arthur/Dropbox/Zotero/Nichols_Hayasaka_2003_Controlling the familywise error rate in functional neuroimaging.pdf}
}

@article{genovese_thresholding_2002,
  langid = {english},
  title = {Thresholding of Statistical Maps in Functional Neuroimaging Using the False Discovery Rate},
  volume = {15},
  issn = {1053-8119},
  doi = {10.1006/nimg.2001.1037},
  abstract = {Finding objective and effective thresholds for voxelwise statistics derived from neuroimaging data has been a long-standing problem. With at least one test performed for every voxel in an image, some correction of the thresholds is needed to control the error rates, but standard procedures for multiple hypothesis testing (e.g., Bonferroni) tend to not be sensitive enough to be useful in this context. This paper introduces to the neuroscience literature statistical procedures for controlling the false discovery rate (FDR). Recent theoretical work in statistics suggests that FDR-controlling procedures will be effective for the analysis of neuroimaging data. These procedures operate simultaneously on all voxelwise test statistics to determine which tests should be considered statistically significant. The innovation of the procedures is that they control the expected proportion of the rejected hypotheses that are falsely rejected. We demonstrate this approach using both simulations and functional magnetic resonance imaging data from two simple experiments.},
  number = {4},
  journaltitle = {NeuroImage},
  shortjournal = {Neuroimage},
  date = {2002-04},
  pages = {870-878},
  keywords = {Adult,Artifacts,Brain Mapping,Computer Simulation,Female,Humans,Image Processing; Computer-Assisted,Magnetic Resonance Imaging,Male,Reference Values,Mathematical Computing,Cerebral Cortex,Motor Activity},
  author = {Genovese, Christopher R. and Lazar, Nicole A. and Nichols, Thomas},
  eprinttype = {pmid},
  eprint = {11906227}
}

@article{haxby_distributed_2001,
  langid = {english},
  title = {Distributed and Overlapping Representations of Faces and Objects in Ventral Temporal Cortex},
  volume = {293},
  issn = {0036-8075},
  doi = {10.1126/science.1063736},
  abstract = {The functional architecture of the object vision pathway in the human brain was investigated using functional magnetic resonance imaging to measure patterns of response in ventral temporal cortex while subjects viewed faces, cats, five categories of man-made objects, and nonsense pictures. A distinct pattern of response was found for each stimulus category. The distinctiveness of the response to a given category was not due simply to the regions that responded maximally to that category, because the category being viewed also could be identified on the basis of the pattern of response when those regions were excluded from the analysis. Patterns of response that discriminated among all categories were found even within cortical regions that responded maximally to only one category. These results indicate that the representations of faces and objects in ventral temporal cortex are widely distributed and overlapping.},
  number = {5539},
  journaltitle = {Science (New York, N.Y.)},
  shortjournal = {Science},
  date = {2001-09-28},
  pages = {2425-2430},
  keywords = {Brain Mapping,Face,Female,Humans,Magnetic Resonance Imaging,Male,Pattern Recognition; Visual,Form Perception,Recognition (Psychology),Temporal Lobe,Visual Pathways},
  author = {Haxby, J. V. and Gobbini, M. I. and Furey, M. L. and Ishai, A. and Schouten, J. L. and Pietrini, P.},
  eprinttype = {pmid},
  eprint = {11577229}
}

@article{hyvarinen_independent_2000,
  langid = {english},
  title = {Independent Component Analysis: Algorithms and Applications},
  volume = {13},
  issn = {08936080},
  url = {http://linkinghub.elsevier.com/retrieve/pii/S0893608000000265},
  shorttitle = {Independent Component Analysis},
  abstract = {A fundamental problem in neural network research, as well as in many other disciplines, is ﬁnding a suitable representation of multivariate data, i.e. random vectors. For reasons of computational and conceptual simplicity, the representation is often sought as a linear transformation of the original data. In other words, each component of the representation is a linear combination of the original variables. Well-known linear transformation methods include principal component analysis, factor analysis, and projection pursuit. Independent component analysis (ICA) is a recently developed method in which the goal is to ﬁnd a linear representation of nongaussian data so that the components are statistically independent, or as independent as possible. Such a representation seems to capture the essential structure of the data in many applications, including feature extraction and signal separation. In this paper, we present the basic theory and applications of ICA, and our recent work on the subject.},
  number = {4-5},
  journaltitle = {Neural Networks},
  urldate = {2018-05-08},
  date = {2000-06},
  pages = {411-430},
  author = {Hyvärinen, A. and Oja, E.},
  file = {/home/arthur/Dropbox/Zotero/Hyvärinen_Oja_2000_Independent component analysis.pdf}
}

@article{bell_information-maximization_1995,
  title = {An Information-Maximization Approach to Blind Separation and Blind Deconvolution},
  volume = {7},
  number = {6},
  journaltitle = {Neural computation},
  date = {1995},
  pages = {1129--1159},
  author = {Bell, Anthony J. and Sejnowski, Terrence J.},
  file = {/home/arthur/Dropbox/Zotero/Bell_Sejnowski_1995_An information-maximization approach to blind separation and blind deconvolution.pdf;/home/arthur/Zotero/storage/VFBN986L/neco.1995.7.6.html}
}

@article{calhoun_method_2001,
  langid = {english},
  title = {A {{Method}} for {{Making Group Inferences}} from {{Functional MRI Data Using Independent Component Analysis}}},
  volume = {14},
  abstract = {Independent component analysis (ICA) is a promising analysis method that is being increasingly applied to fMRI data. A principal advantage of this approach is its applicability to cognitive paradigms for which detailed models of brain activity are not available. Independent component analysis has been successfully utilized to analyze single-subject fMRI data sets, and an extension of this work would be to provide for group inferences. However, unlike univariate methods (e.g., regression analysis, Kolmogorov–Smirnov statistics), ICA does not naturally generalize to a method suitable for drawing inferences about groups of subjects. We introduce a novel approach for drawing group inferences using ICA of fMRI data, and present its application to a simple visual paradigm that alternately stimulates the left or right visual ﬁeld. Our group ICA analysis revealed task-related components in left and right visual cortex, a transiently task-related component in bilateral occipital/parietal cortex, and a non-task-related component in bilateral visual association cortex. We address issues involved in the use of ICA as an fMRI analysis method such as: (1) How many components should be calculated? (2) How are these components to be combined across subjects? (3) How should the ﬁnal results be thresholded and/or presented? We show that the methodology we present provides answers to these questions and lay out a process for making group inferences from fMRI data using independent component analysis. Hum. Brain Mapping 14: 140 –151, 2001. © 2001 Wiley-Liss, Inc.},
  journaltitle = {Human Brain Mapping},
  date = {2001},
  pages = {140--151},
  author = {Calhoun, V D and Adali, T and Pearlson, G D and Pekar, J J},
  file = {/home/arthur/Dropbox/Zotero/Calhoun et al_2001_A Method for Making Group Inferences from Functional MRI Data Using Independent.pdf}
}

@article{biswal_functional_1995,
  langid = {english},
  title = {Functional Connectivity in the Motor Cortex of Resting Human Brain Using Echo-Planar Mri},
  volume = {34},
  issn = {07403194, 15222594},
  url = {http://doi.wiley.com/10.1002/mrm.1910340409},
  number = {4},
  journaltitle = {Magnetic Resonance in Medicine},
  urldate = {2018-05-08},
  date = {1995-10},
  pages = {537-541},
  author = {Biswal, Bharat and Zerrin Yetkin, F. and Haughton, Victor M. and Hyde, James S.},
  file = {/home/arthur/Dropbox/Zotero/Biswal et al_1995_Functional connectivity in the motor cortex of resting human brain using.pdf}
}

@article{eckart_approximation_1936,
  title = {The Approximation of One Matrix by Another of Lower Rank},
  volume = {1},
  number = {3},
  journaltitle = {Psychometrika},
  date = {1936},
  pages = {211--218},
  author = {Eckart, Carl and Young, Gale},
  file = {/home/arthur/Zotero/storage/XVXKBM42/BF02288367.html}
}

@article{duchi_efficient_2009,
  title = {Efficient Online and Batch Learning Using Forward Backward Splitting},
  volume = {10},
  issue = {Dec},
  journaltitle = {Journal of Machine Learning Research},
  date = {2009},
  pages = {2899--2934},
  author = {Duchi, John and Singer, Yoram},
  file = {/home/arthur/Dropbox/Zotero/Duchi_Singer_2009_Efficient online and batch learning using forward backward splitting.pdf;/home/arthur/Zotero/storage/8CGQ8RR3/duchi09a.html}
}

@article{candes_robust_2009,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {0912.3599},
  primaryClass = {cs, math},
  title = {Robust {{Principal Component Analysis}}?},
  url = {http://arxiv.org/abs/0912.3599},
  abstract = {This paper is about a curious phenomenon. Suppose we have a data matrix, which is the superposition of a low-rank component and a sparse component. Can we recover each component individually? We prove that under some suitable assumptions, it is possible to recover both the low-rank and the sparse components exactly by solving a very convenient convex program called Principal Component Pursuit; among all feasible decompositions, simply minimize a weighted combination of the nuclear norm and of the L1 norm. This suggests the possibility of a principled approach to robust principal component analysis since our methodology and results assert that one can recover the principal components of a data matrix even though a positive fraction of its entries are arbitrarily corrupted. This extends to the situation where a fraction of the entries are missing as well. We discuss an algorithm for solving this optimization problem, and present applications in the area of video surveillance, where our methodology allows for the detection of objects in a cluttered background, and in the area of face recognition, where it offers a principled way of removing shadows and specularities in images of faces.},
  urldate = {2018-05-14},
  date = {2009-12-18},
  keywords = {Computer Science - Information Theory},
  author = {Candes, Emmanuel J. and Li, Xiaodong and Ma, Yi and Wright, John},
  file = {/home/arthur/Dropbox/Zotero/Candes et al_2009_Robust Principal Component Analysis.pdf;/home/arthur/Zotero/storage/I2F3BHEN/0912.html}
}

@article{nowak_divide_2016,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1611.02401},
  primaryClass = {cs, stat},
  title = {Divide and {{Conquer Networks}}},
  url = {http://arxiv.org/abs/1611.02401},
  abstract = {We consider the learning of algorithmic tasks by mere observation of input-output pairs. Rather than studying this as a black-box discrete regression problem with no assumption whatsoever on the input-output mapping, we concentrate on tasks that are amenable to the principle of divide and conquer, and study what are its implications in terms of learning. This principle creates a powerful inductive bias that we leverage with neural archi- tectures that are defined recursively and dynamically, by learning two scale-invariant atomic operations: how to split a given input into smaller sets, and how to merge two partially solved tasks into a larger partial solution. Our model can be trained in weakly supervised environments, namely by just observing input-output pairs, and in even weaker environments, using a non-differentiable reward signal. Moreover, thanks to the dynamic aspect of our architecture, we can incorporate the computational complexity as a regularization term that can be optimized by backpropagation. We demonstrate the flexibility and efficiency of the Divide-and-Conquer Network on three combinatorial and geometric tasks: sorting, clustering and convex hulls. Thanks to the dynamic program- ming nature of our model, we show significant improvements in terms of generalization error and computational complexity},
  urldate = {2018-05-14},
  date = {2016-11-08},
  keywords = {Computer Science - Learning,Statistics - Machine Learning},
  author = {Nowak, Alex and Bruna, Joan},
  file = {/home/arthur/Dropbox/Zotero/Nowak_Bruna_2016_Divide and Conquer Networks.pdf;/home/arthur/Zotero/storage/FFX6G92Y/1611.html}
}

@article{wang_learning_2016,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1611.05763},
  primaryClass = {cs, stat},
  title = {Learning to Reinforcement Learn},
  url = {http://arxiv.org/abs/1611.05763},
  abstract = {In recent years deep reinforcement learning (RL) systems have attained superhuman performance in a number of challenging task domains. However, a major limitation of such applications is their demand for massive amounts of training data. A critical present objective is thus to develop deep RL methods that can adapt rapidly to new tasks. In the present work we introduce a novel approach to this challenge, which we refer to as deep meta-reinforcement learning. Previous work has shown that recurrent networks can support meta-learning in a fully supervised context. We extend this approach to the RL setting. What emerges is a system that is trained using one RL algorithm, but whose recurrent dynamics implement a second, quite separate RL procedure. This second, learned RL algorithm can differ from the original one in arbitrary ways. Importantly, because it is learned, it is configured to exploit structure in the training domain. We unpack these points in a series of seven proof-of-concept experiments, each of which examines a key aspect of deep meta-RL. We consider prospects for extending and scaling up the approach, and also point out some potentially important implications for neuroscience.},
  urldate = {2018-05-14},
  date = {2016-11-17},
  keywords = {Computer Science - Learning,Statistics - Machine Learning,Computer Science - Artificial Intelligence},
  author = {Wang, Jane X. and Kurth-Nelson, Zeb and Tirumala, Dhruva and Soyer, Hubert and Leibo, Joel Z. and Munos, Remi and Blundell, Charles and Kumaran, Dharshan and Botvinick, Matt},
  file = {/home/arthur/Dropbox/Zotero/Wang et al_2016_Learning to reinforcement learn.pdf;/home/arthur/Zotero/storage/5JIZCZ7K/1611.html}
}

@inproceedings{fazel_rank_2001,
  langid = {english},
  title = {A Rank Minimization Heuristic with Application to Minimum Order System Approximation},
  isbn = {978-0-7803-6495-0},
  url = {http://ieeexplore.ieee.org/document/945730/},
  abstract = {Several problems arising in control system analysis and design,such as reduced order controller synthesis, involve minimizing the rank of a matrix variable subject to linear matrix inequality (LMI) constraints. Except in some special cases, solving this rank minimization probiem (globally) is very difficult. One simple and surprisingly effective heuristic, applicable when the matrix variable is symmetric and positive semidefinite, is to minimize its trace in place of its rank. This results in a semidefinite program (SDP)which can be efficiently solved.},
  publisher = {{IEEE}},
  urldate = {2018-05-15},
  date = {2001},
  pages = {4734-4739 vol.6},
  author = {Fazel, M. and Hindi, H. and Boyd, S.P.},
  file = {/home/arthur/Dropbox/Zotero/Fazel et al_2001_A rank minimization heuristic with application to minimum order system.pdf}
}

@inproceedings{leblond_asaga_2017,
  langid = {english},
  title = {{{ASAGA}}: {{Asynchronous Parallel SAGA}}},
  url = {http://proceedings.mlr.press/v54/leblond17a.html},
  shorttitle = {{{ASAGA}}},
  abstract = {We describe ASAGA, an asynchronous parallel version of the incremental gradient algorithm SAGA that enjoys fast linear convergence rates. Through a novel perspective, we revisit and clarify a subtl...},
  eventtitle = {Artificial {{Intelligence}} and {{Statistics}}},
  booktitle = {Artificial {{Intelligence}} and {{Statistics}}},
  urldate = {2018-05-15},
  date = {2017-04-10},
  pages = {46-54},
  author = {Leblond, Rémi and Pedregosa, Fabian and Lacoste-Julien, Simon},
  file = {/home/arthur/Dropbox/Zotero/Leblond et al_2017_ASAGA.pdf;/home/arthur/Zotero/storage/ET5SWY9J/leblond17a.html}
}

@article{kuhn_variants_1956,
  title = {Variants of the {{Hungarian}} Method for Assignment Problems},
  volume = {3},
  number = {4},
  journaltitle = {Naval Research Logistics (NRL)},
  date = {1956},
  pages = {253--258},
  author = {Kuhn, Harold W.},
  file = {/home/arthur/Zotero/storage/P24UMDKP/nav.html}
}

@article{haynes_primer_2015,
  title = {A {{Primer}} on {{Pattern}}-{{Based Approaches}} to {{fMRI}}: {{Principles}}, {{Pitfalls}}, and {{Perspectives}}},
  volume = {87},
  issn = {0896-6273},
  url = {http://www.sciencedirect.com/science/article/pii/S0896627315004328},
  shorttitle = {A {{Primer}} on {{Pattern}}-{{Based Approaches}} to {{fMRI}}},
  abstract = {Human fMRI signals exhibit a spatial patterning that contains detailed information about a person’s mental states. Using classifiers it is possible to access this information and study brain processes at the level of individual mental representations. The precise link between fMRI signals and neural population signals still needs to be unraveled. Also, the interpretation of classification studies needs to be handled with care. Nonetheless, pattern-based analyses make it possible to investigate human representational spaces in unprecedented ways, especially when combined with computational modeling.},
  number = {2},
  journaltitle = {Neuron},
  shortjournal = {Neuron},
  urldate = {2018-05-16},
  date = {2015-07-15},
  pages = {257-270},
  author = {Haynes, John-Dylan},
  file = {/home/arthur/Dropbox/Zotero/Haynes_2015_A Primer on Pattern-Based Approaches to fMRI.pdf;/home/arthur/Zotero/storage/BJ4R62YE/S0896627315004328.html}
}

@article{mourao-miranda_classifying_2005,
  title = {Classifying Brain States and Determining the Discriminating Activation Patterns: {{Support Vector Machine}} on Functional {{MRI}} Data},
  volume = {28},
  issn = {1053-8119},
  url = {http://www.sciencedirect.com/science/article/pii/S1053811905004787},
  shorttitle = {Classifying Brain States and Determining the Discriminating Activation Patterns},
  abstract = {In the present study, we applied the Support Vector Machine (SVM) algorithm to perform multivariate classification of brain states from whole functional magnetic resonance imaging (fMRI) volumes without prior selection of spatial features. In addition, we did a comparative analysis between the SVM and the Fisher Linear Discriminant (FLD) classifier. We applied the methods to two multisubject attention experiments: a face matching and a location matching task. We demonstrate that SVM outperforms FLD in classification performance as well as in robustness of the spatial maps obtained (i.e. discriminating volumes). In addition, the SVM discrimination maps had greater overlap with the general linear model (GLM) analysis compared to the FLD. The analysis presents two phases: during the training, the classifier algorithm finds the set of regions by which the two brain states can be best distinguished from each other. In the next phase, the test phase, given an fMRI volume from a new subject, the classifier predicts the subject's instantaneous brain state.},
  number = {4},
  journaltitle = {NeuroImage},
  shortjournal = {NeuroImage},
  series = {Special Section: Social Cognitive Neuroscience},
  urldate = {2018-05-16},
  date = {2005-12-01},
  pages = {980-995},
  keywords = {Classifiers,Functional magnetic resonance imaging data analysis,Machine learning methods,Support Vector Machine},
  author = {Mourão-Miranda, Janaina and Bokde, Arun L. W. and Born, Christine and Hampel, Harald and Stetter, Martin},
  file = {/home/arthur/Zotero/storage/HGGI268Q/S1053811905004787.html}
}

@book{bishop_pattern_1995,
  title = {Pattern {{Recognition}} and {{Machine Learning}}},
  publisher = {{Springer}},
  date = {1995},
  author = {Bishop, Christopher M},
  file = {/home/arthur/Dropbox/Zotero/Bishop_1995_Pattern Recognition and Machine Learning.pdf}
}

@article{fisher_use_1936,
  title = {The Use of Multiple Measurements in Taxonomic Problems},
  volume = {7},
  number = {2},
  journaltitle = {Annals of human genetics},
  date = {1936},
  pages = {179--188},
  author = {Fisher, Ronald A.},
  file = {/home/arthur/Dropbox/Zotero/Fisher_1936_The use of multiple measurements in taxonomic problems.pdf;/home/arthur/Zotero/storage/Q5V673TC/j.1469-1809.1936.tb02137.html}
}

@article{haynes_predicting_2005,
  langid = {english},
  title = {Predicting the Orientation of Invisible Stimuli from Activity in Human Primary Visual Cortex},
  volume = {8},
  issn = {1097-6256, 1546-1726},
  url = {http://www.nature.com/articles/nn1445},
  number = {5},
  journaltitle = {Nature Neuroscience},
  urldate = {2018-05-16},
  date = {2005-05},
  pages = {686-691},
  author = {Haynes, John-Dylan and Rees, Geraint},
  file = {/home/arthur/Dropbox/Zotero/Haynes_Rees_2005_Predicting the orientation of invisible stimuli from activity in human primary.pdf}
}

@article{fisher_statistical_1938,
  title = {The Statistical Utilization of Multiple Measurements},
  volume = {8},
  number = {4},
  journaltitle = {Annals of Human Genetics},
  date = {1938},
  pages = {376--386},
  author = {Fisher, Ronald A.},
  file = {/home/arthur/Dropbox/Zotero/Fisher_1938_The statistical utilization of multiple measurements.pdf;/home/arthur/Zotero/storage/5549AY5R/j.1469-1809.1938.tb02189.html}
}

@article{hearst_support_1998,
  title = {Support Vector Machines},
  volume = {13},
  number = {4},
  journaltitle = {IEEE Intelligent Systems and their applications},
  date = {1998},
  pages = {18--28},
  author = {Hearst, Marti A. and Dumais, Susan T. and Osuna, Edgar and Platt, John and Scholkopf, Bernhard},
  file = {/home/arthur/Dropbox/Zotero/Hearst et al_1998_Support vector machines.pdf;/home/arthur/Zotero/storage/KIUCCCAJ/708428.html}
}

@article{mumford_deconvolving_2012,
  title = {Deconvolving {{BOLD}} Activation in Event-Related Designs for Multivoxel Pattern Classification Analyses},
  volume = {59},
  issn = {1053-8119},
  url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3251697/},
  abstract = {Use of multivoxel pattern analysis (MVPA) to predict the cognitive state of a subject during task performance has become a popular focus of fMRI studies. The input to these analyses consists of activation patterns corresponding to different tasks or stimulus types. These activation patterns are fairly straightforward to calculate for blocked trials or slow event-related designs, but for rapid event-related designs the evoked BOLD signal for adjacent trials will overlap in time, complicating the identification of signal unique to specific trials. Rapid event-related designs are often preferred because they allow for more stimuli to be presented and subjects tend to be more focused on the task, and thus it would be beneficial to be able to use these types of designs in MVPA analyses. The present work compares 8 different models for estimating trial-by-trial activation patterns for a range of rapid event-related designs varying by interstimulus interval and signal-to-noise ratio. The most effective approach obtains each trial’s estimate through a general linear model including a regressor for that trial as well as another regressor for all other trials. Through the analysis of both simulated and real data we have found that this model shows some improvement over the standard approaches for obtaining activation patterns. The resulting trial-by-trial estimates are more representative of the true activation magnitudes, leading to a boost in classification accuracy in fast event-related designs with higher signal-to-noise. This provides the potential for fMRI studies that allow simultaneous optimization of both univariate and MVPA approaches.},
  number = {3},
  journaltitle = {Neuroimage},
  shortjournal = {Neuroimage},
  urldate = {2018-05-16},
  date = {2012-02-01},
  pages = {2636-2643},
  author = {Mumford, Jeanette A. and Turner, Benjamin O. and Ashby, F. Gregory and Poldrack, Russell A.},
  file = {/home/arthur/Dropbox/Zotero/Mumford et al_2012_Deconvolving BOLD activation in event-related designs for multivoxel pattern.pdf},
  eprinttype = {pmid},
  eprint = {21924359},
  pmcid = {PMC3251697}
}

@article{kanwisher_fusiform_1997,
  title = {The Fusiform Face Area: A Module in Human Extrastriate Cortex Specialized for Face Perception},
  volume = {17},
  shorttitle = {The Fusiform Face Area},
  number = {11},
  journaltitle = {Journal of neuroscience},
  date = {1997},
  pages = {4302--4311},
  author = {Kanwisher, Nancy and McDermott, Josh and Chun, Marvin M.},
  file = {/home/arthur/Zotero/storage/3Q7VV96W/4302.html;/home/arthur/Zotero/storage/3XHR7HSL/4302.html}
}

@thesis{pedregosa-izquierdo_feature_2015,
  title = {Feature Extraction and Supervised Learning on {{fMRI}}: From Practice to Theory},
  shorttitle = {Feature Extraction and Supervised Learning on {{fMRI}}},
  institution = {{Université Pierre et Marie Curie-Paris VI}},
  type = {PhD Thesis},
  date = {2015},
  author = {Pedregosa-Izquierdo, Fabian},
  file = {/home/arthur/Dropbox/Zotero/Pedregosa-Izquierdo_2015_Feature extraction and supervised learning on fMRI.pdf;/home/arthur/Zotero/storage/SSZJJRU6/tel-01100921.html}
}

@article{van_essen_human_2012,
  title = {The {{Human Connectome Project}}: A Data Acquisition Perspective},
  volume = {62},
  shorttitle = {The {{Human Connectome Project}}},
  number = {4},
  journaltitle = {Neuroimage},
  date = {2012},
  pages = {2222--2231},
  author = {Van Essen, David C. and Ugurbil, Kamil and Auerbach, E. and Barch, D. and Behrens, T. E. J. and Bucholz, R. and Chang, Acer and Chen, Liyong and Corbetta, Maurizio and Curtiss, Sandra W.},
  file = {/home/arthur/Zotero/storage/5XWRVJTM/PMC3606888.html;/home/arthur/Zotero/storage/LALJR2CI/S1053811912001954.html}
}

@article{mairal_sparse_2014,
  title = {Sparse Modeling for Image and Vision Processing},
  volume = {8},
  number = {2-3},
  journaltitle = {Foundations and Trends® in Computer Graphics and Vision},
  date = {2014},
  pages = {85--283},
  author = {Mairal, Julien and Bach, Francis and Ponce, Jean},
  file = {/home/arthur/Dropbox/Zotero/Mairal et al_2014_Sparse modeling for image and vision processing.pdf;/home/arthur/Zotero/storage/QSNK5QXH/CGV-058.html}
}

@article{bottou_optimization_2018,
  title = {Optimization {{Methods}} for {{Large}}-{{Scale Machine Learning}}},
  volume = {60},
  issn = {0036-1445},
  url = {https://epubs.siam.org/doi/10.1137/16M1080173},
  abstract = {This paper provides a review and commentary on the past, present, and future of numerical optimization algorithms in the context of machine learning applications.  Through case studies on text classification and the training of deep neural networks, we discuss how optimization problems arise in machine learning and what makes them challenging.  A major theme of our study is that large-scale machine learning represents a distinctive setting in which the stochastic gradient (SG) method has traditionally played a central role while conventional gradient-based nonlinear optimization techniques typically falter.  Based on this viewpoint, we present a comprehensive theory of a straightforward, yet versatile SG algorithm, discuss its practical behavior, and highlight  opportunities for designing algorithms with improved performance.  This leads to a discussion about the next generation of optimization methods for large-scale machine learning, including an investigation of two main streams of research on techniques that diminish noise in the stochastic directions and methods that make use of second-order derivative approximations.},
  number = {2},
  journaltitle = {SIAM Review},
  shortjournal = {SIAM Rev.},
  urldate = {2018-05-17},
  date = {2018-01-01},
  pages = {223-311},
  author = {Bottou, L. and Curtis, F. and Nocedal, J.},
  file = {/home/arthur/Zotero/storage/SPRMLASG/16M1080173.html}
}

@article{hastie_matrix_2015,
  title = {Matrix Completion and Low-Rank {{SVD}} via Fast Alternating Least Squares.},
  volume = {16},
  journaltitle = {Journal of Machine Learning Research},
  date = {2015},
  pages = {3367--3402},
  author = {Hastie, Trevor and Mazumder, Rahul and Lee, Jason D. and Zadeh, Reza},
  file = {/home/arthur/Dropbox/Zotero/Hastie et al_2015_Matrix completion and low-rank SVD via fast alternating least squares.pdf;/home/arthur/Dropbox/Zotero/Hastie et al_2015_Matrix completion and low-rank SVD via fast alternating least squares.pdf}
}

@article{loula_decoding_2017,
  langid = {english},
  title = {Decoding {{fMRI}} Activity in the Time Domain Improves Classification Performance},
  issn = {10538119},
  url = {http://linkinghub.elsevier.com/retrieve/pii/S1053811917306651},
  abstract = {Most current functional Magnetic Resonance Imaging (fMRI) decoding analyses rely on statistical summaries of the data resulting from a deconvolution approach: each stimulation event is associated with a brain response. This standard approach leads to simple learning procedures, yet it is ill-suited for decoding events with short inter-stimulus intervals. In order to overcome this issue, we propose a novel framework that separates the spatial and temporal components of the prediction by decoding the fMRI time-series continuously, i.e. scan-by-scan. The stimulation events can then be identiﬁed through a deconvolution of the reconstructed time series. We show that this model performs as well as or better than standard approaches across several datasets, most notably in regimes with small inter-stimuli intervals (3 to 5s), while also oﬀering predictions that are highly interpretable in the time domain. This opens the way toward analyzing datasets not normally thought of as suitable for decoding and makes it possible to run decoding on studies with reduced scan time.},
  journaltitle = {NeuroImage},
  urldate = {2018-05-17},
  date = {2017-08},
  author = {Loula, João and Varoquaux, Gaël and Thirion, Bertrand},
  file = {/home/arthur/Dropbox/Zotero/Loula et al_2017_Decoding fMRI activity in the time domain improves classification performance.pdf}
}

@article{desikan_automated_2006,
  langid = {english},
  title = {An Automated Labeling System for Subdividing the Human Cerebral Cortex on {{MRI}} Scans into Gyral Based Regions of Interest},
  volume = {31},
  issn = {1053-8119},
  doi = {10.1016/j.neuroimage.2006.01.021},
  abstract = {In this study, we have assessed the validity and reliability of an automated labeling system that we have developed for subdividing the human cerebral cortex on magnetic resonance images into gyral based regions of interest (ROIs). Using a dataset of 40 MRI scans we manually identified 34 cortical ROIs in each of the individual hemispheres. This information was then encoded in the form of an atlas that was utilized to automatically label ROIs. To examine the validity, as well as the intra- and inter-rater reliability of the automated system, we used both intraclass correlation coefficients (ICC), and a new method known as mean distance maps, to assess the degree of mismatch between the manual and the automated sets of ROIs. When compared with the manual ROIs, the automated ROIs were highly accurate, with an average ICC of 0.835 across all of the ROIs, and a mean distance error of less than 1 mm. Intra- and inter-rater comparisons yielded little to no difference between the sets of ROIs. These findings suggest that the automated method we have developed for subdividing the human cerebral cortex into standard gyral-based neuroanatomical regions is both anatomically valid and reliable. This method may be useful for both morphometric and functional studies of the cerebral cortex as well as for clinical investigations aimed at tracking the evolution of disease-induced changes over time, including clinical trials in which MRI-based measures are used to examine response to treatment.},
  number = {3},
  journaltitle = {NeuroImage},
  shortjournal = {Neuroimage},
  date = {2006-07-01},
  pages = {968-980},
  keywords = {Adult,Algorithms,Brain Mapping,Female,Humans,Image Processing; Computer-Assisted,Magnetic Resonance Imaging,Male,Reproducibility of Results,Middle Aged,Cerebral Cortex,Aged,Aged; 80 and over,Aging,Alzheimer Disease,Atrophy,Corpus Callosum,Dominance; Cerebral,Imaging; Three-Dimensional,Observer Variation,Software,Statistics as Topic},
  author = {Desikan, Rahul S. and Ségonne, Florent and Fischl, Bruce and Quinn, Brian T. and Dickerson, Bradford C. and Blacker, Deborah and Buckner, Randy L. and Dale, Anders M. and Maguire, R. Paul and Hyman, Bradley T. and Albert, Marilyn S. and Killiany, Ronald J.},
  eprinttype = {pmid},
  eprint = {16530430}
}

@article{yamashita_sparse_2008,
  title = {Sparse Estimation Automatically Selects Voxels Relevant for the Decoding of {{fMRI}} Activity Patterns},
  volume = {42},
  number = {4},
  journaltitle = {NeuroImage},
  date = {2008},
  pages = {1414--1429},
  author = {Yamashita, Okito and Sato, Masa-aki and Yoshioka, Taku and Tong, Frank and Kamitani, Yukiyasu},
  file = {/home/arthur/Zotero/storage/E8N7HP6L/PMC3158033.html;/home/arthur/Zotero/storage/SIRS4DFI/S1053811908006940.html}
}

@inproceedings{ng_generalized_2011,
  title = {Generalized Sparse Regularization with Application to {{fMRI}} Brain Decoding},
  booktitle = {Biennial {{International Conference}} on {{Information Processing}} in {{Medical Imaging}}},
  publisher = {{Springer}},
  date = {2011},
  pages = {612--623},
  author = {Ng, Bernard and Abugharbieh, Rafeef},
  file = {/home/arthur/Dropbox/Zotero/Ng_Abugharbieh_2011_Generalized sparse regularization with application to fMRI brain decoding.pdf;/home/arthur/Zotero/storage/CLDE8U25/978-3-642-22092-0_50.html}
}

@article{yuan_model_2006,
  langid = {english},
  title = {Model Selection and Estimation in Regression with Grouped Variables},
  volume = {68},
  issn = {1369-7412, 1467-9868},
  url = {http://doi.wiley.com/10.1111/j.1467-9868.2005.00532.x},
  abstract = {We consider the problem of selecting grouped variables (factors) for accurate prediction in regression. Such a problem arises naturally in many practical situations with the multi-factor ANOVA problem as the most important and well known example. Instead of selecting factors by stepwise backward elimination, we focus on estimation accuracy and consider extensions of the LASSO, the LARS, and the nonnegative garrote for factor selection. The LASSO, the LARS, and the nonnegative garrote are recently proposed regression methods that can be used to select individual variables. We study and propose eﬃcient algorithms for the extensions of these methods for factor selection, and show that these extensions give superior performance to the traditional stepwise backward elimination method in factor selection problems. We study the similarities and the diﬀerences among these methods. Simulations and real examples are used to illustrate the methods.},
  number = {1},
  journaltitle = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
  urldate = {2018-05-17},
  date = {2006-02},
  pages = {49-67},
  author = {Yuan, Ming and Lin, Yi},
  file = {/home/arthur/Dropbox/Zotero/Yuan_Lin_2006_Model selection and estimation in regression with grouped variables.pdf}
}

@article{michel_total_2011,
  title = {Total {{Variation Regularization}} for {{fMRI}}-{{Based Prediction}} of {{Behavior}}},
  volume = {30},
  issn = {0278-0062},
  doi = {10.1109/TMI.2011.2113378},
  abstract = {While medical imaging typically provides massive amounts of data, the extraction of relevant information for predictive diagnosis remains a difficult challenge. Functional magnetic resonance imaging (fMRI) data, that provide an indirect measure of task-related or spontaneous neuronal activity, are classically analyzed in a mass-univariate procedure yielding statistical parametric maps. This analysis framework disregards some important principles of brain organization: population coding, distributed and overlapping representations. Multivariate pattern analysis, i.e., the prediction of behavioral variables from brain activation patterns better captures this structure. To cope with the high dimensionality of the data, the learning method has to be regularized. However, the spatial structure of the image is not taken into account in standard regularization methods, so that the extracted features are often hard to interpret. More informative and interpretable results can be obtained with the ℓ1 norm of the image gradient, also known as its total variation (TV), as regularization. We apply for the first time this method to fMRI data, and show that TV regularization is well suited to the purpose of brain mapping while being a powerful tool for brain decoding. Moreover, this article presents the first use of TV regularization for classification.},
  number = {7},
  journaltitle = {IEEE Transactions on Medical Imaging},
  date = {2011-07},
  pages = {1328-1340},
  keywords = {Algorithms,Brain,Brain Mapping,Computer Simulation,Humans,Image Processing; Computer-Assisted,Magnetic Resonance Imaging,Optimization,Training,biomedical MRI,functional magnetic resonance imaging,image classification,medical image processing,Cognition,Classification,Regression Analysis,Multivariate Analysis,Accuracy,Behavior,brain,brain mapping,Brain modeling,brain organization,distributed representations,fMRI data,fMRI-based behavior prediction,functional magnetic resonance imaging (fMRI),Logistics,mass-univariate procedure,medical imaging,Minimization,multivariate pattern analysis,overlapping representations,population coding,predictive diagnosis,regression,regularization,relevant information,spatial structure,spontaneous neuronal activity,statistical parametric maps,total variation (TV),total variation regularization,TV,TV regularization},
  author = {Michel, V. and Gramfort, A. and Varoquaux, G. and Eger, E. and Thirion, B.},
  file = {/home/arthur/Zotero/storage/4G5RV7JD/5711672.html}
}

@article{kriegeskorte_information-based_2006,
  langid = {english},
  title = {Information-Based Functional Brain Mapping},
  volume = {103},
  issn = {0027-8424, 1091-6490},
  url = {http://www.pnas.org/content/103/10/3863},
  abstract = {The development of high-resolution neuroimaging and multielectrode electrophysiological recording provides neuroscientists with huge amounts of multivariate data. The complexity of the data creates a need for statistical summary, but the local averaging standardly applied to this end may obscure the effects of greatest neuroscientific interest. In neuroimaging, for example, brain mapping analysis has focused on the discovery of activation, i.e., of extended brain regions whose average activity changes across experimental conditions. Here we propose to ask a more general question of the data: Where in the brain does the activity pattern contain information about the experimental condition? To address this question, we propose scanning the imaged volume with a “searchlight,” whose contents are analyzed multivariately at each location in the brain.},
  number = {10},
  journaltitle = {Proceedings of the National Academy of Sciences},
  shortjournal = {PNAS},
  urldate = {2018-05-17},
  date = {2006-03-07},
  pages = {3863-3868},
  keywords = {functional magnetic resonance imaging,neuroimaging,statistical analysis},
  author = {Kriegeskorte, Nikolaus and Goebel, Rainer and Bandettini, Peter},
  file = {/home/arthur/Dropbox/Zotero/Kriegeskorte et al_2006_Information-based functional brain mapping.pdf;/home/arthur/Zotero/storage/4RAZNN3Z/3863.html},
  eprinttype = {pmid},
  eprint = {16537458}
}

@article{devlin_praise_2007,
  langid = {english},
  title = {In Praise of Tedious Anatomy},
  volume = {37},
  issn = {1053-8119},
  doi = {10.1016/j.neuroimage.2006.09.055},
  abstract = {Functional neuroimaging is fundamentally a tool for mapping function to structure, and its success consequently requires neuroanatomical precision and accuracy. Here we review the various means by which functional activation can be localised to neuroanatomy and suggest that the gold standard should be localisation to the individual's or group's own anatomy through the use of neuroanatomical knowledge and atlases of neuroanatomy. While automated means of localisation may be useful, they cannot provide the necessary accuracy, given variability between individuals. We also suggest that the field of functional neuroimaging needs to converge on a common set of methods for reporting functional localisation including a common "standard" space and criteria for what constitutes sufficient evidence to report activation in terms of Brodmann's areas.},
  number = {4},
  journaltitle = {NeuroImage},
  shortjournal = {Neuroimage},
  date = {2007-10-01},
  pages = {1033-1041; discussion 1050-1058},
  keywords = {Humans,Magnetic Resonance Imaging,Anatomy,Atlases as Topic,Nervous System,Nervous System Physiological Phenomena,Neural Pathways},
  author = {Devlin, Joseph T. and Poldrack, Russell A.},
  eprinttype = {pmid},
  eprint = {17870621},
  pmcid = {PMC1986635}
}

@article{etzel_introduction_2009,
  title = {An Introduction to Anatomical {{ROI}}-Based {{fMRI}} Classification Analysis},
  volume = {1282},
  issn = {0006-8993},
  url = {http://www.sciencedirect.com/science/article/pii/S000689930901110X},
  abstract = {Modern cognitive neuroscience often thinks at the interface between anatomy and function, hypothesizing that one structure is important for a task while another is not. A flexible and sensitive way to test such hypotheses is to evaluate the pattern of activity in the specific structures using multivariate classification techniques. These methods consider the activation patterns across groups of voxels, and so are consistent with current theories of how information is encoded in the brain: that the pattern of activity in brain areas is more important than the activity of single neurons or voxels. Classification techniques can identify many types of activation patterns, and patterns unique to each subject or shared across subjects. This paper is an introduction to applying classification methods to functional magnetic resonance imaging (fMRI) data, particularly for region of interest (ROI) based hypotheses. The first section describes the main steps required for such analyses while the second illustrates these steps using a simple example.},
  journaltitle = {Brain Research},
  shortjournal = {Brain Research},
  urldate = {2018-05-17},
  date = {2009-07-28},
  pages = {114-125},
  keywords = {fMRI,Classification analysis,Multivariate analysis,Multivoxel pattern analysis,Region of interest},
  author = {Etzel, Joset A. and Gazzola, Valeria and Keysers, Christian},
  file = {/home/arthur/Zotero/storage/GM4DUG32/S000689930901110X.html}
}

@article{saxe_divide_2006,
  langid = {english},
  title = {Divide and Conquer: A Defense of Functional Localizers},
  volume = {30},
  issn = {1053-8119},
  doi = {10.1016/j.neuroimage.2005.12.062},
  shorttitle = {Divide and Conquer},
  abstract = {Numerous functionally distinct regions of cortex (e.g., V1, MT, the fusiform face area) can be easily identified in any normal human subject in just a few minutes of fMRI scanning. However, the locations of these regions vary across subjects. Investigations of these regions have therefore often used a functional region of interest (fROI) approach in which the region is first identified functionally in each subject individually, before subsequent scans in the same subjects test specific hypotheses concerning that region. This fROI method, which resembled long-established practice in visual neurophysiology, has methodological, statistical, and theoretical advantages over standard alternatives (such as whole-brain analyses of group data): (i) because functional properties are more consistently and robustly associated with fROIs than with locations in stereotaxic space, functional hypotheses concerning fROIs are often the most straightforward to frame, motivate, and test, (ii) because hypotheses are tested in only a handful of fROIs (instead of in tens of thousands of voxels), advance specification of fROIs provides a massive increase in statistical power over whole-brain analyses, and (iii) some fROIs may serve as candidate distinct components of the mind/brain worth investigation as such. Of course fROIs can be productively used in conjunction with other complementary methods. Here, we explain the motivation for and advantages of the fROI approach, and we rebut the criticism of this method offered by Friston et al. (Friston, K., Rotshtein, P., Geng, J., Sterzer, P., Henson, R., in press. A critique of functional localizers. NeuroImage).},
  number = {4},
  journaltitle = {NeuroImage},
  shortjournal = {Neuroimage},
  date = {2006-05-01},
  pages = {1088-1096; discussion 1097-1099},
  keywords = {Brain Mapping,Humans,Image Processing; Computer-Assisted,Magnetic Resonance Imaging,Sensitivity and Specificity,Mathematical Computing,Cerebral Cortex,Dominance; Cerebral,Fourier Analysis,Mind-Body Relations; Metaphysical,Radiosurgery,Surgery; Computer-Assisted},
  author = {Saxe, Rebecca and Brett, Matthew and Kanwisher, Nancy},
  eprinttype = {pmid},
  eprint = {16635578}
}

@book{poldrack_handbook_2011,
  langid = {english},
  location = {{Cambridge}},
  title = {Handbook of {{Functional MRI Data Analysis}}},
  isbn = {978-0-511-89502-9},
  url = {http://ebooks.cambridge.org/ref/id/CBO9780511895029},
  publisher = {{Cambridge University Press}},
  urldate = {2018-05-17},
  date = {2011},
  author = {Poldrack, Russell A. and Nichols, Thomas and Mumford, Jeanette},
  file = {/home/arthur/Dropbox/Zotero/Poldrack et al_2011_Handbook of Functional MRI Data Analysis.pdf},
  doi = {10.1017/CBO9780511895029}
}

@article{de_martino_combining_2008,
  title = {Combining Multivariate Voxel Selection and Support Vector Machines for Mapping and Classification of {{fMRI}} Spatial Patterns},
  volume = {43},
  issn = {1053-8119},
  url = {http://www.sciencedirect.com/science/article/pii/S1053811908007854},
  abstract = {In functional brain mapping, pattern recognition methods allow detecting multivoxel patterns of brain activation which are informative with respect to a subject's perceptual or cognitive state. The sensitivity of these methods, however, is greatly reduced when the proportion of voxels that convey the discriminative information is small compared to the total number of measured voxels. To reduce this dimensionality problem, previous studies employed univariate voxel selection or region-of-interest-based strategies as a preceding step to the application of machine learning algorithms. Here we employ a strategy for classifying functional imaging data based on a multivariate feature selection algorithm, Recursive Feature Elimination (RFE) that uses the training algorithm (support vector machine) recursively to eliminate irrelevant voxels and estimate informative spatial patterns. Generalization performances on test data increases while features/voxels are pruned based on their discrimination ability. In this article we evaluate RFE in terms of sensitivity of discriminative maps (Receiver Operative Characteristic analysis) and generalization performances and compare it to previously used univariate voxel selection strategies based on activation and discrimination measures. Using simulated fMRI data, we show that the recursive approach is suitable for mapping discriminative patterns and that the combination of an initial univariate activation-based (F-test) reduction of voxels and multivariate recursive feature elimination produces the best results, especially when differences between conditions have a low contrast-to-noise ratio. Furthermore, we apply our method to high resolution (2 × 2 × 2mm3) data from an auditory fMRI experiment in which subjects were stimulated with sounds from four different categories. With these real data, our recursive algorithm proves able to detect and accurately classify multivoxel spatial patterns, highlighting the role of the superior temporal gyrus in encoding the information of sound categories. In line with the simulation results, our method outperforms univariate statistical analysis and statistical learning without feature selection.},
  number = {1},
  journaltitle = {NeuroImage},
  shortjournal = {NeuroImage},
  urldate = {2018-05-17},
  date = {2008-10-15},
  pages = {44-58},
  author = {De Martino, Federico and Valente, Giancarlo and Staeren, Noël and Ashburner, John and Goebel, Rainer and Formisano, Elia},
  file = {/home/arthur/Zotero/storage/7J6XIMBV/S1053811908007854.html}
}

@article{walder_neural_2018,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1802.03144},
  primaryClass = {cs},
  langid = {english},
  title = {Neural {{Dynamic Programming}} for {{Musical Self Similarity}}},
  url = {http://arxiv.org/abs/1802.03144},
  abstract = {We present a neural sequence model designed speciﬁcally for symbolic music. The model is based on a learned edit distance mechanism which generalises a classic recursion from computer science, leading to a neural dynamic program. Repeated motifs are detected by learning the transformations between them. We represent the arising computational dependencies using a novel data structure, the edit tree; this perspective suggests natural approximations which afford the scaling up of our otherwise cubic time algorithm. We demonstrate our model on real and synthetic data; in all cases it out-performs a strong stacked long short-term memory benchmark.},
  urldate = {2018-05-21},
  date = {2018-02-09},
  keywords = {Computer Science - Learning,Computer Science - Artificial Intelligence},
  author = {Walder, Christian J. and Kim, Dongwoo},
  file = {/home/arthur/Dropbox/Zotero/Walder_Kim_2018_Neural Dynamic Programming for Musical Self Similarity.pdf}
}

@article{mohri_semiring_2002,
  langid = {english},
  title = {Semiring {{Framework}} and {{Algorithms}} for {{Shortest}}-{{Distance Problems}}},
  volume = {7},
  abstract = {We deﬁne general algebraic frameworks for shortest-distance problems based on the structure of semirings. We give a generic algorithm for ﬁnding single-source shortest distances in a weighted directed graph when the weights satisfy the conditions of our general semiring framework. The same algorithm can be used to solve eﬃciently classical shortest paths problems or to ﬁnd the k-shortest distances in a directed graph. It can be used to solve single-source shortest-distance problems in weighted directed acyclic graphs over any semiring. We examine several semirings and describe some speciﬁc instances of our generic algorithms to illustrate their use and compare them with existing methods and algorithms. The proof of the soundness of all algorithms is given in detail, including their pseudocode and a full analysis of their running time complexity.},
  number = {3},
  date = {2002},
  pages = {321--350},
  author = {Mohri, Mehryar},
  file = {/home/arthur/Dropbox/Zotero/Mohri_2002_Semiring Framework and Algorithms for Shortest-Distance Problems.pdf}
}

@book{sutton_reinforcement_2018,
  langid = {english},
  location = {{Cambridge, Mass.}},
  title = {Reinforcement Learning: An Introduction},
  edition = {Nachdr.},
  isbn = {978-0-262-19398-6},
  shorttitle = {Reinforcement Learning},
  pagetotal = {322},
  series = {Adaptive computation and machine learning},
  publisher = {{MIT Press}},
  date = {2018},
  author = {Sutton, Richard S. and Barto, Andrew G.},
  file = {/home/arthur/Dropbox/Zotero/Sutton_Barto_2018_Reinforcement learning.pdf},
  note = {OCLC: 837901590}
}

@article{chizat_global_2018,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1805.09545},
  primaryClass = {cs, math, stat},
  title = {On the {{Global Convergence}} of {{Gradient Descent}} for {{Over}}-Parameterized {{Models}} Using {{Optimal Transport}}},
  url = {http://arxiv.org/abs/1805.09545},
  abstract = {Many tasks in machine learning and signal processing can be solved by minimizing a convex function of a measure. This includes sparse spikes deconvolution or training a neural network with a single hidden layer. For these problems, we study a simple minimization method: the unknown measure is discretized into a mixture of particles and a continuous-time gradient descent is performed on their weights and positions. This is an idealization of the usual way to train neural networks with a large hidden layer. We show that, when initialized correctly and in the many-particle limit, this gradient flow, although non-convex, converges to global minimizers. The proof involves Wasserstein gradient flows, a by-product of optimal transport theory. Numerical experiments show that this asymptotic behavior is already at play for a reasonable number of particles, even in high dimension.},
  urldate = {2018-05-28},
  date = {2018-05-24},
  keywords = {Mathematics - Optimization and Control,Statistics - Machine Learning,Computer Science - Neural and Evolutionary Computing,_tablet},
  author = {Chizat, Lenaic and Bach, Francis},
  file = {/home/arthur/Zotero/storage/ILSWL3TJ/Chizat_Bach_2018_On the Global Convergence of Gradient Descent for Over-parameterized Models.pdf;/home/arthur/Zotero/storage/T39AT9Y3/1805.html}
}

@article{maddison_concrete_2016,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1611.00712},
  primaryClass = {cs, stat},
  langid = {english},
  title = {The {{Concrete Distribution}}: {{A Continuous Relaxation}} of {{Discrete Random Variables}}},
  url = {http://arxiv.org/abs/1611.00712},
  shorttitle = {The {{Concrete Distribution}}},
  abstract = {The reparameterization trick enables optimizing large scale stochastic computation graphs via gradient descent. The essence of the trick is to refactor each stochastic node into a differentiable function of its parameters and a random variable with ﬁxed distribution. After refactoring, the gradients of the loss propagated by the chain rule through the graph are low variance unbiased estimators of the gradients of the expected loss. While many continuous random variables have such reparameterizations, discrete random variables lack useful reparameterizations due to the discontinuous nature of discrete states. In this work we introduce CONCRETE random variables—CONtinuous relaxations of disCRETE random variables. The Concrete distribution is a new family of distributions with closed form densities and a simple reparameterization. Whenever a discrete stochastic node of a computation graph can be refactored into a one-hot bit representation that is treated continuously, Concrete stochastic nodes can be used with automatic differentiation to produce low-variance biased gradients of objectives (including objectives that depend on the log-probability of latent stochastic nodes) on the corresponding discrete graph. We demonstrate the effectiveness of Concrete relaxations on density estimation and structured prediction tasks using neural networks.},
  urldate = {2018-05-30},
  date = {2016-11-02},
  keywords = {Computer Science - Learning,Statistics - Machine Learning},
  author = {Maddison, Chris J. and Mnih, Andriy and Teh, Yee Whye},
  file = {/home/arthur/Dropbox/Zotero/Maddison et al_2016_The Concrete Distribution.pdf}
}

@article{louizos_learning_2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1712.01312},
  primaryClass = {cs, stat},
  langid = {english},
  title = {Learning {{Sparse Neural Networks}} through \${{L}}\_0\$ {{Regularization}}},
  url = {http://arxiv.org/abs/1712.01312},
  abstract = {We propose a practical method for L0 norm regularization for neural networks: pruning the network during training by encouraging weights to become exactly zero. Such regularization is interesting since (1) it can greatly speed up training and inference, and (2) it can improve generalization. AIC and BIC, well-known model selection criteria, are special cases of L0 regularization. However, since the L0 norm of weights is non-differentiable, we cannot incorporate it directly as a regularization term in the objective function. We propose a solution through the inclusion of a collection of non-negative stochastic gates, which collectively determine which weights to set to zero. We show that, somewhat surprisingly, for certain distributions over the gates, the expected L0 norm of the resulting gated weights is differentiable with respect to the distribution parameters. We further propose the hard concrete distribution for the gates, which is obtained by “stretching” a binary concrete distribution and then transforming its samples with a hard-sigmoid. The parameters of the distribution over the gates can then be jointly optimized with the original network parameters. As a result our method allows for straightforward and efﬁcient learning of model structures with stochastic gradient descent and allows for conditional computation in a principled way. We perform various experiments to demonstrate the effectiveness of the resulting approach and regularizer.},
  urldate = {2018-05-30},
  date = {2017-12-04},
  keywords = {Computer Science - Learning,Statistics - Machine Learning},
  author = {Louizos, Christos and Welling, Max and Kingma, Diederik P.},
  file = {/home/arthur/Dropbox/Zotero/Louizos et al_2017_Learning Sparse Neural Networks through $L_0$ Regularization.pdf}
}

@inproceedings{bottou_global_1997,
  title = {Global Training of Document Processing Systems Using Graph Transformer Networks},
  doi = {10.1109/CVPR.1997.609370},
  abstract = {We propose a new machine learning paradigm called Graph Transformer Networks that extends the applicability of gradient-based learning algorithms to systems composed of modules that take graphs as inputs and produce graphs as output. Training is performed by computing gradients of a global objective function with respect to all the parameters in the system using a kind of back-propagation procedure. A complete check reading system based on these concepts is described. The system uses convolutional neural network character recognizers, combined with global training techniques to provide record accuracy on business and personal checks. It is presently deployed commercially and reads million of checks per month},
  eventtitle = {Proceedings of {{IEEE Computer Society Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  booktitle = {Proceedings of {{IEEE Computer Society Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  date = {1997-06},
  pages = {489-494},
  keywords = {Neural networks,backpropagation,backpropagation procedure,Business,Character recognition,check reading system,computer vision,document image processing,document processing systems,Error analysis,Feedforward systems,global objective function,global training,gradient-based learning algorithms,graph transformer networks,Image processing,Jacobian matrices,machine learning paradigm,Multi-layer neural network,neural network character recognizers,optical character recognition,Speech processing,Stochastic systems},
  author = {Bottou, L. and Bengio, Y. and Cun, Y. Le},
  file = {/home/arthur/Dropbox/Zotero/Bottou et al_1997_Global training of document processing systems using graph transformer networks.pdf;/home/arthur/Zotero/storage/TYNUF6UD/609370.html}
}

@article{cappe_online_2009,
  langid = {english},
  title = {Online {{EM Algorithm}} for {{Latent Data Models}}},
  volume = {71},
  abstract = {In this contribution, we propose a generic online (also sometimes called adaptive or recursive) version of the Expectation-Maximisation (EM) algorithm applicable to latent variable models of independent observations. Compared to the algorithm of Titterington (1984), this approach is more directly connected to the usual EM algorithm and does not rely on integration with respect to the complete data distribution. The resulting algorithm is usually simpler and is shown to achieve convergence to the stationary points of the Kullback-Leibler divergence between the marginal distribution of the observation and the model distribution at the optimal rate, i.e., that of the maximum likelihood estimator. In addition, the proposed approach is also suitable for conditional (or regression) models, as illustrated in the case of the mixture of linear regressions model.},
  number = {3},
  journaltitle = {Journal of the Royal Statistical Society: Series B},
  date = {2009},
  pages = {593-613},
  author = {Cappé, Olivier and Moulines, Eric},
  file = {/home/arthur/Dropbox/Zotero/Cappé_Moulines_2009_Online EM Algorithm for Latent Data Models.pdf}
}

@incollection{saad_-line_1999,
  langid = {english},
  location = {{Cambridge}},
  title = {On-Line {{Learning}} and {{Stochastic Approximations}}},
  isbn = {978-0-511-56992-0},
  url = {https://www.cambridge.org/core/product/identifier/CBO9780511569920A009/type/book_part},
  abstract = {The convergence of online learning algorithms is analyzed using the tools of the stochastic approximation theory, and proved under very weak conditions. A general framework for online learning algorithms is ﬁrst presented. This framework encompasses the most common online learning algorithms in use today, as illustrated by several examples. The stochastic approximation theory then provides general results describing the convergence of all these learning algorithms at once.},
  booktitle = {On-{{Line Learning}} in {{Neural Networks}}},
  publisher = {{Cambridge University Press}},
  urldate = {2018-06-20},
  date = {1999},
  pages = {9-42},
  author = {Bottou, Léon},
  editor = {Saad, David},
  file = {/home/arthur/Dropbox/Zotero/Bottou_1999_On-line Learning and Stochastic Approximations.pdf},
  doi = {10.1017/CBO9780511569920.003}
}

@book{ortega_iterative_1970,
  title = {Iterative {{Solution}} of {{Nonlinear Equations}} in {{Several Variables}}},
  url = {https://www.elsevier.com/books/iterative-solution-of-nonlinear-equations-in-several-variables/ortega/978-0-12-528550-6},
  publisher = {{Academic Press}},
  urldate = {2018-06-20},
  date = {1970},
  author = {Ortega, J. M. and Rheinboldt, W. C.}
}

@inproceedings{cauchy_methode_1847,
  title = {Méthode Générale Pour La Réesolution Des Systèmes d’équations Simultanées},
  booktitle = {Compte {{Rendu}} à l'{{Académie}} Des {{Sciences}} de {{Paris}}},
  date = {1847},
  author = {Cauchy, Louis Augustin}
}

@article{chen_neural_2018,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1806.07366},
  primaryClass = {cs, stat},
  title = {Neural {{Ordinary Differential Equations}}},
  url = {http://arxiv.org/abs/1806.07366},
  abstract = {We introduce a new family of deep neural network models. Instead of specifying a discrete sequence of hidden layers, we parameterize the derivative of the hidden state using a neural network. The output of the network is computed using a blackbox differential equation solver. These continuous-depth models have constant memory cost, adapt their evaluation strategy to each input, and can explicitly trade numerical precision for speed. We demonstrate these properties in continuous-depth residual networks and continuous-time latent variable models. We also construct continuous normalizing flows, a generative model that can train by maximum likelihood, without partitioning or ordering the data dimensions. For training, we show how to scalably backpropagate through any ODE solver, without access to its internal operations. This allows end-to-end training of ODEs within larger models.},
  urldate = {2018-06-20},
  date = {2018-06-19},
  keywords = {Computer Science - Learning,Statistics - Machine Learning,Computer Science - Artificial Intelligence},
  author = {Chen, Tian Qi and Rubanova, Yulia and Bettencourt, Jesse and Duvenaud, David},
  file = {/home/arthur/Dropbox/Zotero/Chen et al_2018_Neural Ordinary Differential Equations.pdf;/home/arthur/Zotero/storage/BQBEKNI6/1806.html}
}

@inproceedings{mensch_differentiable_2018,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1802.03676},
  title = {Differentiable {{Dynamic Programming}} for {{Structured Prediction}} and {{Attention}}},
  url = {http://arxiv.org/abs/1802.03676},
  abstract = {Dynamic programming (DP) solves a variety of structured combinatorial problems by iteratively breaking them down into smaller subproblems. In spite of their versatility, DP algorithms are usually non-differentiable, which hampers their use as a layer in neural networks trained by backpropagation. To address this issue, we propose to smooth the max operator in the dynamic programming recursion, using a strongly convex regularizer. This allows to relax both the optimal value and solution of the original combinatorial problem, and turns a broad class of DP algorithms into differentiable operators. Theoretically, we provide a new probabilistic perspective on backpropagating through these DP operators, and relate them to inference in graphical models. We derive two particular instantiations of our framework, a smoothed Viterbi algorithm for sequence prediction and a smoothed DTW algorithm for time-series alignment. We showcase these instantiations on two structured prediction tasks and on structured and sparse attention for neural machine translation.},
  booktitle = {Proceedings of the {{International Conference}} on {{Machine Learning}}},
  urldate = {2018-06-20},
  date = {2018},
  keywords = {Computer Science - Learning,Statistics - Machine Learning},
  author = {Mensch, Arthur and Blondel, Mathieu},
  file = {/home/arthur/Dropbox/Zotero/Mensch_Blondel_2018_Differentiable Dynamic Programming for Structured Prediction and Attention.pdf;/home/arthur/Zotero/storage/I3RGZ4EF/1802.html}
}

@article{simmons_false-positive_2011,
  langid = {english},
  title = {False-{{Positive Psychology}}: {{Undisclosed Flexibility}} in {{Data Collection}} and {{Analysis Allows Presenting Anything}} as {{Significant}}},
  volume = {22},
  issn = {0956-7976, 1467-9280},
  url = {http://journals.sagepub.com/doi/10.1177/0956797611417632},
  shorttitle = {False-{{Positive Psychology}}},
  abstract = {In this article, we accomplish two things. First, we show that despite empirical psychologists’ nominal endorsement of a low rate of false-positive findings (≤ .05), flexibility in data collection, analysis, and reporting dramatically increases actual false-positive rates. In many cases, a researcher is more likely to falsely find evidence that an effect exists than to correctly find evidence that it does not. We present computer simulations and a pair of actual experiments that demonstrate how unacceptably easy it is to accumulate (and report) statistically significant evidence for a false hypothesis. Second, we suggest a simple, low-cost, and straightforwardly effective disclosure-based solution to this problem. The solution involves six concrete requirements for authors and four guidelines for reviewers, all of which impose a minimal burden on the publication process.},
  number = {11},
  journaltitle = {Psychological Science},
  urldate = {2018-07-01},
  date = {2011-11},
  pages = {1359-1366},
  author = {Simmons, Joseph P. and Nelson, Leif D. and Simonsohn, Uri},
  file = {/home/arthur/Dropbox/Zotero/Simmons et al_2011_False-Positive Psychology.pdf}
}

@book{kandel_principles_1981,
  title = {Principles of Neural Science},
  publisher = {{Elsevier}},
  date = {1981},
  author = {Kandel, Eric R. and Schwartz, James H. and Jessell, Thomas M. and of Biochemistry, Department and Jessell, Molecular Biophysics Thomas and Siegelbaum, Steven and Hudspeth, A. J.},
  file = {/home/arthur/Dropbox/Zotero/Kandel et al_1981_Principles of neural science.pdf;/home/arthur/Dropbox/Zotero/Kandel et al_1981_Principles of neural science.pdf}
}

@article{petersen_positron_1989,
  title = {Positron Emission Tomographic Studies of the Processing of Singe Words},
  volume = {1},
  number = {2},
  journaltitle = {Journal of cognitive neuroscience},
  date = {1989},
  pages = {153--170},
  author = {Petersen, Steven E. and Fox, Peter T. and Posner, Michael I. and Mintun, Mark and Raichle, Marcus E.},
  file = {/home/arthur/Dropbox/Zotero/Petersen et al_1989_Positron emission tomographic studies of the processing of singe words.pdf;/home/arthur/Dropbox/Zotero/Petersen et al_1989_Positron emission tomographic studies of the processing of singe words.pdf;/home/arthur/Zotero/storage/484G97TR/jocn.1989.1.2.html}
}

@article{friston_trouble_1996,
  langid = {english},
  title = {The Trouble with Cognitive Subtraction},
  volume = {4},
  issn = {1053-8119},
  doi = {10.1006/nimg.1996.0033},
  abstract = {In this paper we present a critique of pure insertion. Pure insertion represents an implicit assumption behind many (but not all) studies that employ cognitive subtraction. The main contention is that pure insertion is not valid in relation to the neuronal instantiation of cognitive processes. Pure insertion asserts that there are no interactions among the cognitive components of a task. It is possible to evaluate and refute this assumption by testing explicitly for interactions using factorial experimental designs. It is proposed that factorial designs are more powerful than subtraction designs in characterizing cognitive neuroanatomy, precisely because they allow for interactions and eschew notions like pure insertion. In particular we suggest that the effect of a cognitive component (i.e., an effect that is independent of other components) is best captured by the main (activation) effect of that component and that the integration among components (i.e., the expression of one cognitive process in the context of another) can be assessed with the interaction terms. In this framework a complete characterization of cognitive neuroanatomy includes both regionally specific activations and regionally specific interactions. To illustrate our point we have used a factorial experimental design to show that inferotemporal activations, due to object recognition, are profoundly modulated by phonological retrieval of the object's name. This interaction implicates the inferotemporal regions in phonological retrieval, during object naming, despite the fact that phonological retrieval does not, by itself, activate this region.},
  number = {2},
  journaltitle = {NeuroImage},
  shortjournal = {Neuroimage},
  date = {1996-10},
  pages = {97-104},
  keywords = {Brain Mapping,Humans,Image Processing; Computer-Assisted,Reference Values,Semantics,Pattern Recognition; Visual,Cerebral Cortex,Temporal Lobe,Attention,Mental Recall,Phonetics,Tomography; Emission-Computed,Verbal Behavior,Verbal Learning},
  author = {Friston, K. J. and Price, C. J. and Fletcher, P. and Moore, C. and Frackowiak, R. S. and Dolan, R. J.},
  eprinttype = {pmid},
  eprint = {9345501}
}

@article{pereira_machine_2009,
  title = {Machine Learning Classifiers and {{fMRI}}: A Tutorial Overview},
  volume = {45},
  issn = {1053-8119},
  url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2892746/},
  shorttitle = {Machine Learning Classifiers and {{fMRI}}},
  issue = {1 Suppl},
  journaltitle = {NeuroImage},
  shortjournal = {Neuroimage},
  urldate = {2018-07-03},
  date = {2009-03},
  pages = {S199-S209},
  author = {Pereira, Francisco and Mitchell, Tom and Botvinick, Matthew},
  file = {/home/arthur/Dropbox/Zotero/Pereira et al_2009_Machine learning classifiers and fMRI.pdf},
  eprinttype = {pmid},
  eprint = {19070668},
  pmcid = {PMC2892746}
}

@article{maclin_popular_2011,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1106.0257},
  primaryClass = {cs},
  title = {Popular {{Ensemble Methods}}: {{An Empirical Study}}},
  url = {http://arxiv.org/abs/1106.0257},
  shorttitle = {Popular {{Ensemble Methods}}},
  abstract = {An ensemble consists of a set of individually trained classifiers (such as neural networks or decision trees) whose predictions are combined when classifying novel instances. Previous research has shown that an ensemble is often more accurate than any of the single classifiers in the ensemble. Bagging (Breiman, 1996c) and Boosting (Freund and Shapire, 1996; Shapire, 1990) are two relatively new but popular methods for producing ensembles. In this paper we evaluate these methods on 23 data sets using both neural networks and decision trees as our classification algorithm. Our results clearly indicate a number of conclusions. First, while Bagging is almost always more accurate than a single classifier, it is sometimes much less accurate than Boosting. On the other hand, Boosting can create ensembles that are less accurate than a single classifier -- especially when using neural networks. Analysis indicates that the performance of the Boosting methods is dependent on the characteristics of the data set being examined. In fact, further results show that Boosting ensembles may overfit noisy data sets, thus decreasing its performance. Finally, consistent with previous studies, our work suggests that most of the gain in an ensemble's performance comes in the first few classifiers combined; however, relatively large gains can be seen up to 25 classifiers when Boosting decision trees.},
  urldate = {2018-07-11},
  date = {2011-06-01},
  keywords = {Computer Science - Artificial Intelligence},
  author = {Maclin, R. and Opitz, D.},
  file = {/home/arthur/Dropbox/Zotero/Maclin_Opitz_2011_Popular Ensemble Methods.pdf;/home/arthur/Zotero/storage/LKK6GP72/1106.html}
}

@article{makni_fully_2008,
  title = {A Fully {{Bayesian}} Approach to the Parcel-Based Detection-Estimation of Brain Activity in {{fMRI}}},
  volume = {41},
  number = {3},
  journaltitle = {Neuroimage},
  date = {2008},
  pages = {941--969},
  author = {Makni, Salima and Idier, Jérôme and Vincent, Thomas and Thirion, Bertrand and Dehaene-Lambertz, Ghislaine and Ciuciu, Philippe},
  file = {/home/arthur/Dropbox/Zotero/Makni et al_2008_A fully Bayesian approach to the parcel-based detection-estimation of brain.pdf;/home/arthur/Zotero/storage/6F4BRDEI/S1053811908001298.html}
}

@inproceedings{evans_3d_1993,
  langid = {english},
  title = {{{3D}} Statistical Neuroanatomical Models from 305 {{MRI}} Volumes},
  isbn = {978-0-7803-1487-0},
  url = {http://ieeexplore.ieee.org/document/373602/},
  publisher = {{IEEE}},
  urldate = {2018-07-11},
  date = {1993},
  pages = {1813-1817},
  author = {Evans, A.C. and Collins, D.L. and Mills, S.R. and Brown, E.D. and Kelly, R.L. and Peters, T.M.},
  file = {/home/arthur/Dropbox/Zotero/Evans et al_1993_3D statistical neuroanatomical models from 305 MRI volumes.pdf}
}

@article{haxby_common_2011,
  langid = {english},
  title = {A {{Common}}, {{High}}-{{Dimensional Model}} of the {{Representational Space}} in {{Human Ventral Temporal Cortex}}},
  volume = {72},
  issn = {08966273},
  url = {http://linkinghub.elsevier.com/retrieve/pii/S0896627311007811},
  abstract = {We present a high-dimensional model of the representational space in human ventral temporal (VT) cortex in which dimensions are response-tuning functions that are common across individuals and patterns of response are modeled as weighted sums of basis patterns associated with these response tunings. We map response-pattern vectors, measured with fMRI, from individual subjects’ voxel spaces into this common model space using a new method, ‘‘hyperalignment.’’ Hyperalignment parameters based on responses during one experiment—movie viewing—identiﬁed 35 common response-tuning functions that captured ﬁne-grained distinctions among a wide range of stimuli in the movie and in two category perception experiments. Between-subject classiﬁcation (BSC, multivariate pattern classiﬁcation based on other subjects’ data) of response-pattern vectors in common model space greatly exceeded BSC of anatomically aligned responses and matched withinsubject classiﬁcation. Results indicate that population codes for complex visual stimuli in VT cortex are based on response-tuning functions that are common across individuals.},
  number = {2},
  journaltitle = {Neuron},
  urldate = {2018-07-11},
  date = {2011-10},
  pages = {404-416},
  author = {Haxby, James V. and Guntupalli, J. Swaroop and Connolly, Andrew C. and Halchenko, Yaroslav O. and Conroy, Bryan R. and Gobbini, M. Ida and Hanke, Michael and Ramadge, Peter J.},
  file = {/home/arthur/Dropbox/Zotero/Haxby et al_2011_A Common, High-Dimensional Model of the Representational Space in Human Ventral.pdf}
}

@article{raichle_brain_2006,
  langid = {english},
  title = {Brain Work and Brain Imaging},
  volume = {29},
  issn = {0147-006X},
  doi = {10.1146/annurev.neuro.29.051605.112819},
  abstract = {Functional brain imaging with positron emission tomography and magnetic resonance imaging has been used extensively to map regional changes in brain activity. The signal used by both techniques is based on changes in local circulation and metabolism (brain work). Our understanding of the cell biology of these changes has progressed greatly in the past decade. New insights have emerged on the role of astrocytes in signal transduction as has an appreciation of the unique contribution of aerobic glycolysis to brain energy metabolism. Likewise our understanding of the neurophysiologic processes responsible for imaging signals has progressed from an assumption that spiking activity (output) of neurons is most relevant to one focused on their input. Finally, neuroimaging, with its unique metabolic perspective, has alerted us to the ongoing and costly intrinsic activity within brain systems that most likely represents the largest fraction of the brain's functional activity.},
  journaltitle = {Annual Review of Neuroscience},
  shortjournal = {Annu. Rev. Neurosci.},
  date = {2006},
  pages = {449-476},
  keywords = {Brain,Brain Mapping,Humans,Diagnostic Imaging,Oxygen,Animals,Astrocytes,Energy Metabolism,Neurons,Neurophysiology,Radionuclide Imaging},
  author = {Raichle, Marcus E. and Mintun, Mark A.},
  eprinttype = {pmid},
  eprint = {16776593}
}

@article{grosenick_interpretable_2013,
  title = {Interpretable Whole-Brain Prediction Analysis with {{GraphNet}}},
  volume = {72},
  journaltitle = {NeuroImage},
  date = {2013},
  pages = {304--321},
  author = {Grosenick, Logan and Klingenberg, Brad and Katovich, Kiefer and Knutson, Brian and Taylor, Jonathan E.},
  file = {/home/arthur/Zotero/storage/GCLUNUUI/S1053811912012487.html;/home/arthur/Zotero/storage/LDHPMBP4/S1053811912012487.html}
}

@article{peyre_computational_2018,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1803.00567},
  primaryClass = {stat},
  title = {Computational {{Optimal Transport}}},
  url = {http://arxiv.org/abs/1803.00567},
  abstract = {Optimal Transport (OT) is a mathematical gem at the interface between probability, analysis and optimization. The goal of that theory is to define geometric tools that are useful to compare probability distributions. Earlier contributions originated from Monge's work in the 18th century, to be later rediscovered under a different formalism by Tolstoi in the 1920's, Kantorovich, Hitchcock and Koopmans in the 1940's. The problem was solved numerically by Dantzig in 1949 and others in the 1950's within the framework of linear programming, paving the way for major industrial applications in the second half of the 20th century. OT was later rediscovered under a different light by analysts in the 90's, following important work by Brenier and others, as well as in the computer vision/graphics fields under the name of earth mover's distances. Recent years have witnessed yet another revolution in the spread of OT, thanks to the emergence of approximate solvers that can scale to sizes and dimensions that are relevant to data sciences. Thanks to this newfound scalability, OT is being increasingly used to unlock various problems in imaging sciences (such as color or texture processing), computer vision and graphics (for shape manipulation) or machine learning (for regression,classification and density fitting). This short book reviews OT with a bias toward numerical methods and their applications in data sciences, and sheds lights on the theoretical properties of OT that make it particularly useful for some of these applications.},
  urldate = {2018-07-18},
  date = {2018-03-01},
  keywords = {Statistics - Machine Learning},
  author = {Peyré, Gabriel and Cuturi, Marco},
  file = {/home/arthur/Dropbox/Zotero/Peyré_Cuturi_2018_Computational Optimal Transport.pdf;/home/arthur/Zotero/storage/R8EKZ7KN/1803.html}
}

@article{bach_optimization_2011,
  langid = {english},
  title = {Optimization with {{Sparsity}}-{{Inducing Penalties}}},
  volume = {4},
  issn = {1935-8237, 1935-8245},
  url = {http://www.nowpublishers.com/article/Details/MAL-015},
  abstract = {Sparse estimation methods are aimed at using or obtaining parsimonious representations of data or models. They were ﬁrst dedicated to linear variable selection but numerous extensions have now emerged such as structured sparsity or kernel selection. It turns out that many of the related estimation problems can be cast as convex optimization problems by regularizing the empirical risk with appropriate nonsmooth norms. The goal of this monograph is to present from a general perspective optimization tools and techniques dedicated to such sparsity-inducing penalties. We cover proximal methods, block-coordinate descent, reweighted 2-penalized techniques, workingset and homotopy methods, as well as non-convex formulations and extensions, and provide an extensive set of experiments to compare various algorithms from a computational point of view.},
  number = {1},
  journaltitle = {Foundations and Trends® in Machine Learning},
  urldate = {2018-07-18},
  date = {2011},
  pages = {1-106},
  author = {Bach, Francis},
  file = {/home/arthur/Dropbox/Zotero/Bach_2011_Optimization with Sparsity-Inducing Penalties.pdf}
}

@article{silver_mastering_2017,
  title = {Mastering the Game of {{Go}} without Human Knowledge},
  volume = {550},
  issn = {0028-0836, 1476-4687},
  url = {http://www.nature.com/doifinder/10.1038/nature24270},
  number = {7676},
  journaltitle = {Nature},
  urldate = {2018-07-20},
  date = {2017-10-18},
  pages = {354-359},
  keywords = {_tablet},
  author = {Silver, David and Schrittwieser, Julian and Simonyan, Karen and Antonoglou, Ioannis and Huang, Aja and Guez, Arthur and Hubert, Thomas and Baker, Lucas and Lai, Matthew and Bolton, Adrian and Chen, Yutian and Lillicrap, Timothy and Hui, Fan and Sifre, Laurent and van den Driessche, George and Graepel, Thore and Hassabis, Demis},
  options = {useprefix=true},
  file = {/home/arthur/Zotero/storage/DGFI98DV/Silver et al_2017_Mastering the game of Go without human knowledge.pdf}
}

@inproceedings{haarnoja_soft_2018,
  langid = {english},
  title = {Soft {{Actor}}-{{Critic}}: {{Off}}-{{Policy Maximum Entropy Deep Reinforcement Learning}} with a {{Stochastic Actor}}},
  url = {http://proceedings.mlr.press/v80/haarnoja18b.html},
  shorttitle = {Soft {{Actor}}-{{Critic}}},
  abstract = {Model-free deep reinforcement learning (RL) algorithms have been demonstrated on a range of challenging decision making and control tasks. However, these methods typically suffer from two major cha...},
  eventtitle = {International {{Conference}} on {{Machine Learning}}},
  booktitle = {International {{Conference}} on {{Machine Learning}}},
  urldate = {2018-07-20},
  date = {2018-07-03},
  pages = {1856-1865},
  keywords = {_tablet},
  author = {Haarnoja, Tuomas and Zhou, Aurick and Abbeel, Pieter and Levine, Sergey},
  file = {/home/arthur/Zotero/storage/DEI6U9VF/Haarnoja et al_2018_Soft Actor-Critic.pdf;/home/arthur/Zotero/storage/2KJ9GSX6/haarnoja18b.html}
}

@article{deng_latent_2018,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1807.03756},
  primaryClass = {cs, stat},
  title = {Latent {{Alignment}} and {{Variational Attention}}},
  url = {http://arxiv.org/abs/1807.03756},
  abstract = {Neural attention has become central to many state-of-the-art models in natural language processing and related domains. Attention networks are an easy-to-train and effective method for softly simulating alignment; however, the approach does not marginalize over latent alignments in a probabilistic sense. This property makes it difficult to compare attention to other alignment approaches, to compose it with probabilistic models, and to perform posterior inference conditioned on observed data. A related latent approach, hard attention, fixes these issues, but is generally harder to train and less accurate. This work considers variational attention networks, alternatives to soft and hard attention for learning latent variable alignment models, with tighter approximation bounds based on amortized variational inference. We further propose methods for reducing the variance of gradients to make these approaches computationally feasible. Experiments show that for machine translation and visual question answering, inefficient exact latent variable models outperform standard neural attention, but these gains go away when using hard attention based training. On the other hand, variational attention retains most of the performance gain but with training speed comparable to neural attention.},
  urldate = {2018-07-23},
  date = {2018-07-10},
  keywords = {Statistics - Machine Learning,Computer Science - Computation and Language,Computer Science - Machine Learning},
  author = {Deng, Yuntian and Kim, Yoon and Chiu, Justin and Guo, Demi and Rush, Alexander M.},
  file = {/home/arthur/Dropbox/Zotero/Deng et al_2018_Latent Alignment and Variational Attention.pdf;/home/arthur/Zotero/storage/PISVXN6F/1807.html}
}

@article{muzellec_tsallis_2016,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1609.04495},
  primaryClass = {cs},
  title = {Tsallis {{Regularized Optimal Transport}} and {{Ecological Inference}}},
  url = {http://arxiv.org/abs/1609.04495},
  abstract = {Optimal transport is a powerful framework for computing distances between probability distributions. We unify the two main approaches to optimal transport, namely Monge-Kantorovitch and Sinkhorn-Cuturi, into what we define as Tsallis regularized optimal transport ($\backslash$trot). $\backslash$trot\textasciitilde{}interpolates a rich family of distortions from Wasserstein to Kullback-Leibler, encompassing as well Pearson, Neyman and Hellinger divergences, to name a few. We show that metric properties known for Sinkhorn-Cuturi generalize to $\backslash$trot, and provide efficient algorithms for finding the optimal transportation plan with formal convergence proofs. We also present the first application of optimal transport to the problem of ecological inference, that is, the reconstruction of joint distributions from their marginals, a problem of large interest in the social sciences. $\backslash$trot\textasciitilde{}provides a convenient framework for ecological inference by allowing to compute the joint distribution --- that is, the optimal transportation plan itself --- when side information is available, which is $\backslash$textit\{e.g.\} typically what census represents in political science. Experiments on data from the 2012 US presidential elections display the potential of $\backslash$trot\textasciitilde{}in delivering a faithful reconstruction of the joint distribution of ethnic groups and voter preferences.},
  urldate = {2018-07-23},
  date = {2016-09-14},
  keywords = {G.1.6,Computer Science - Machine Learning},
  author = {Muzellec, Boris and Nock, Richard and Patrini, Giorgio and Nielsen, Frank},
  file = {/home/arthur/Dropbox/Zotero/Muzellec et al_2016_Tsallis Regularized Optimal Transport and Ecological Inference.pdf;/home/arthur/Zotero/storage/9N7WLZ4T/1609.html}
}

@article{athalye_obfuscated_2018,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1802.00420},
  primaryClass = {cs},
  title = {Obfuscated {{Gradients Give}} a {{False Sense}} of {{Security}}: {{Circumventing Defenses}} to {{Adversarial Examples}}},
  url = {http://arxiv.org/abs/1802.00420},
  shorttitle = {Obfuscated {{Gradients Give}} a {{False Sense}} of {{Security}}},
  abstract = {We identify obfuscated gradients, a kind of gradient masking, as a phenomenon that leads to a false sense of security in defenses against adversarial examples. While defenses that cause obfuscated gradients appear to defeat iterative optimization-based attacks, we find defenses relying on this effect can be circumvented. We describe characteristic behaviors of defenses exhibiting the effect, and for each of the three types of obfuscated gradients we discover, we develop attack techniques to overcome it. In a case study, examining non-certified white-box-secure defenses at ICLR 2018, we find obfuscated gradients are a common occurrence, with 7 of 9 defenses relying on obfuscated gradients. Our new attacks successfully circumvent 6 completely, and 1 partially, in the original threat model each paper considers.},
  urldate = {2018-07-24},
  date = {2018-02-01},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Cryptography and Security},
  author = {Athalye, Anish and Carlini, Nicholas and Wagner, David},
  file = {/home/arthur/Dropbox/Zotero/Athalye et al_2018_Obfuscated Gradients Give a False Sense of Security.pdf;/home/arthur/Zotero/storage/D3D5V3E6/1802.html}
}

@article{gordon_precision_2017,
  langid = {english},
  title = {Precision {{Functional Mapping}} of {{Individual Human Brains}}},
  volume = {95},
  issn = {1097-4199},
  doi = {10.1016/j.neuron.2017.07.011},
  abstract = {Human functional MRI (fMRI) research primarily focuses on analyzing data averaged across groups, which limits the detail, specificity, and clinical utility~of fMRI resting-state functional connectivity (RSFC) and task-activation maps. To push our understanding of functional brain organization to the level of~individual humans, we assembled a novel MRI dataset containing 5~hr of RSFC data, 6~hr of~task fMRI, multiple structural MRIs, and neuropsychological tests from each of ten adults. Using~these data, we generated ten high-fidelity, individual-specific~functional connectomes. This individual-connectome approach revealed several new types~of spatial and organizational variability in brain networks, including unique network features and topologies that corresponded with structural and task-derived brain features. We are~releasing this highly sampled, individual-focused dataset as~a resource for neuroscientists, and we propose precision individual connectomics as a model for future work examining the organization of healthy and diseased individual human brains.},
  number = {4},
  journaltitle = {Neuron},
  shortjournal = {Neuron},
  date = {2017-08-16},
  pages = {791-807.e7},
  keywords = {Adult,Brain,Brain Mapping,Female,Humans,Image Processing; Computer-Assisted,Magnetic Resonance Imaging,Male,Models; Neurological,Reproducibility of Results,functional connectivity,fMRI,Neural Pathways,Analysis of Variance,Oxygen,Young Adult,Connectome,brain networks,individual variability,Individuality,myelin mapping,Myelin Sheath,Rest},
  author = {Gordon, Evan M. and Laumann, Timothy O. and Gilmore, Adrian W. and Newbold, Dillan J. and Greene, Deanna J. and Berg, Jeffrey J. and Ortega, Mario and Hoyt-Drazen, Catherine and Gratton, Caterina and Sun, Haoxin and Hampton, Jacqueline M. and Coalson, Rebecca S. and Nguyen, Annie L. and McDermott, Kathleen B. and Shimony, Joshua S. and Snyder, Abraham Z. and Schlaggar, Bradley L. and Petersen, Steven E. and Nelson, Steven M. and Dosenbach, Nico U. F.},
  eprinttype = {pmid},
  eprint = {28757305},
  pmcid = {PMC5576360}
}

@article{gilmer_neural_2017,
  title = {Neural Message Passing for Quantum Chemistry},
  journaltitle = {arXiv preprint arXiv:1704.01212},
  date = {2017},
  author = {Gilmer, Justin and Schoenholz, Samuel S. and Riley, Patrick F. and Vinyals, Oriol and Dahl, George E.},
  file = {/home/arthur/Dropbox/Zotero/Gilmer et al_2017_Neural message passing for quantum chemistry.pdf;/home/arthur/Zotero/storage/BP6GQVBS/1704.html}
}

@article{machado_laplacian_2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1703.00956},
  primaryClass = {cs},
  langid = {english},
  title = {A {{Laplacian Framework}} for {{Option Discovery}} in {{Reinforcement Learning}}},
  url = {http://arxiv.org/abs/1703.00956},
  abstract = {Representation learning and option discovery are two of the biggest challenges in reinforcement learning (RL). Proto-value functions (PVFs) are a well-known approach for representation learning in MDPs. In this paper we address the option discovery problem by showing how PVFs implicitly deﬁne options. We do it by introducing eigenpurposes, intrinsic reward functions derived from the learned representations. The options discovered from eigenpurposes traverse the principal directions of the state space. They are useful for multiple tasks because they are discovered without taking the environment’s rewards into consideration. Moreover, different options act at different time scales, making them helpful for exploration. We demonstrate features of eigenpurposes in traditional tabular domains as well as in Atari 2600 games.},
  urldate = {2018-07-25},
  date = {2017-03-02},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  author = {Machado, Marlos C. and Bellemare, Marc G. and Bowling, Michael},
  file = {/home/arthur/Dropbox/Zotero/Machado et al_2017_A Laplacian Framework for Option Discovery in Reinforcement Learning.pdf}
}

@article{bronstein_geometric_nodate,
  langid = {english},
  title = {Geometric Deep Learning: Going beyond {{Euclidean}} Data},
  pages = {23},
  keywords = {_tablet},
  author = {Bronstein, Michael M and Bruna, Joan and LeCun, Yann and Szlam, Arthur and Vandergheynst, Pierre},
  file = {/home/arthur/Zotero/storage/6RKCJ2LV/Bronstein et al_Geometric deep learning.pdf}
}

@article{venturi_neural_2018,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1802.06384},
  primaryClass = {cs, math, stat},
  title = {Neural {{Networks}} with {{Finite Intrinsic Dimension}} Have No {{Spurious Valleys}}},
  url = {http://arxiv.org/abs/1802.06384},
  abstract = {Neural networks provide a rich class of high-dimensional, non-convex optimization problems. Despite their non-convexity, gradient-descent methods often successfully optimize these models. This has motivated a recent spur in research attempting to characterize properties of their loss surface that may be responsible for such success. In particular, several authors have noted that over-parametrization appears to act as a remedy against non-convexity. In this paper, we address this phenomenon by studying key topological properties of the loss, such as the presence or absence of "spurious valleys", defined as connected components of sub-level sets that do not include a global minimum. Focusing on a class of two-layer neural networks defined by smooth (but generally non-linear) activation functions, our main contribution is to prove that as soon as the hidden layer size matches the intrinsic dimension of the reproducing space, defined as the linear functional space generated by the activations, no spurious valleys exist, thus allowing the existence of descent directions. Our setup includes smooth activations such as polynomials, both in the empirical and population risk, and generic activations in the empirical risk case.},
  urldate = {2018-07-25},
  date = {2018-02-18},
  keywords = {Mathematics - Optimization and Control,Statistics - Machine Learning,Computer Science - Machine Learning},
  author = {Venturi, Luca and Bandeira, Afonso S. and Bruna, Joan},
  file = {/home/arthur/Dropbox/Zotero/Venturi et al_2018_Neural Networks with Finite Intrinsic Dimension have no Spurious Valleys.pdf;/home/arthur/Zotero/storage/3WHD2HXT/1802.html}
}

@inproceedings{gorgolewski_openneuro_2017,
  title = {{{OpenNeuro}} – a Free Online Platform for Sharing and Analysis of Neuroimaging Data},
  volume = {6},
  doi = {10.7490/f1000research.1114354.1},
  booktitle = {Annual {{Meeting}} of the {{Organization}} for {{Human Brain Mapping}}},
  date = {2017-07-04},
  author = {Gorgolewski, Krzysztof J. and Esteban, Oscar and Schaefer, Gunnar and Wandell, Brian A. and Poldrack, Russell A.},
  file = {/home/arthur/Zotero/storage/9JDIEUCM/6-1055.html}
}

@article{olah_building_2018,
  title = {The {{Building Blocks}} of {{Interpretability}}},
  volume = {3},
  number = {3},
  journaltitle = {Distill},
  date = {2018},
  pages = {e10},
  author = {Olah, Chris and Satyanarayan, Arvind and Johnson, Ian and Carter, Shan and Schubert, Ludwig and Ye, Katherine and Mordvintsev, Alexander},
  file = {/home/arthur/Zotero/storage/2MCD38RC/building-blocks.html;/home/arthur/Zotero/storage/RTPML74S/building-blocks.html}
}

@article{poldrack_scanning_2017,
  langid = {english},
  title = {Scanning the Horizon: Towards Transparent and Reproducible Neuroimaging Research},
  volume = {18},
  issn = {1471-0048},
  url = {https://www.nature.com/articles/nrn.2016.167},
  shorttitle = {Scanning the Horizon},
  abstract = {Functional neuroimaging techniques have transformed our ability to probe the neurobiological basis of behaviour and are increasingly being applied by the wider neuroscience community. However, concerns have recently been raised that the conclusions that are drawn from some human neuroimaging studies are either spurious or not generalizable. Problems such as low statistical power, flexibility in data analysis, software errors and a lack of direct replication apply to many fields, but perhaps particularly to functional MRI. Here, we discuss these problems, outline current and suggested best practices, and describe how we think the field should evolve to produce the most meaningful and reliable answers to neuroscientific questions.},
  number = {2},
  journaltitle = {Nature Reviews Neuroscience},
  urldate = {2018-07-26},
  date = {2017-02},
  pages = {115-126},
  author = {Poldrack, Russell A. and Baker, Chris I. and Durnez, Joke and Gorgolewski, Krzysztof J. and Matthews, Paul M. and Munafò, Marcus R. and Nichols, Thomas E. and Poline, Jean-Baptiste and Vul, Edward and Yarkoni, Tal},
  file = {/home/arthur/Dropbox/Zotero/Poldrack et al_2017_Scanning the horizon.pdf;/home/arthur/Zotero/storage/9EDFEFCV/nrn.2016.html}
}

@article{laird_brainmap_2011,
  title = {The {{BrainMap}} Strategy for Standardization, Sharing, and Meta-Analysis of Neuroimaging Data},
  volume = {4},
  issn = {1756-0500},
  url = {https://doi.org/10.1186/1756-0500-4-349},
  abstract = {Neuroimaging researchers have developed rigorous community data and metadata standards that encourage meta-analysis as a method for establishing robust and meaningful convergence of knowledge of human brain structure and function. Capitalizing on these standards, the BrainMap project offers databases, software applications, and other associated tools for supporting and promoting quantitative coordinate-based meta-analysis of the structural and functional neuroimaging literature.},
  number = {1},
  journaltitle = {BMC Research Notes},
  shortjournal = {BMC Research Notes},
  urldate = {2018-07-26},
  date = {2011-09-09},
  pages = {349},
  author = {Laird, Angela R. and Eickhoff, Simon B. and Fox, P. Mickle and Uecker, Angela M. and Ray, Kimberly L. and Saenz, Juan J. and McKay, D. Reese and Bzdok, Danilo and Laird, Robert W. and Robinson, Jennifer L. and Turner, Jessica A. and Turkeltaub, Peter E. and Lancaster, Jack L. and Fox, Peter T.},
  file = {/home/arthur/Dropbox/Zotero/Laird et al_2011_The BrainMap strategy for standardization, sharing, and meta-analysis of.pdf;/home/arthur/Zotero/storage/CK29MUA5/1756-0500-4-349.html}
}

@article{yarkoni_large-scale_2011,
  title = {Large-Scale Automated Synthesis of Human Functional Neuroimaging Data},
  volume = {8},
  number = {8},
  journaltitle = {Nature methods},
  date = {2011},
  pages = {665},
  author = {Yarkoni, Tal and Poldrack, Russell A. and Nichols, Thomas E. and Van Essen, David C. and Wager, Tor D.},
  file = {/home/arthur/Dropbox/Zotero/Yarkoni et al_2011_Large-scale automated synthesis of human functional neuroimaging data.pdf;/home/arthur/Zotero/storage/QWVTARGV/nmeth.html;/home/arthur/Zotero/storage/REEBLZE6/PMC3146590.html}
}

@article{abdullah_note_2018,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1802.03976},
  primaryClass = {cs},
  title = {A Note on Reinforcement Learning with {{Wasserstein}} Distance Regularisation, with Applications to Multipolicy Learning},
  url = {http://arxiv.org/abs/1802.03976},
  abstract = {In this note we describe an application of Wasserstein distance to Reinforcement Learning. The Wasserstein distance in question is between the distribution of mappings of trajectories of a policy into some metric space, and some other fixed distribution (which may, for example, come from another policy). Different policies induce different distributions, so given an underlying metric, the Wasserstein distance quantifies how different policies are. This can be used to learn multiple polices which are different in terms of such Wasserstein distances by using a Wasserstein regulariser. Changing the sign of the regularisation parameter, one can learn a policy for which its trajectory mapping distribution is attracted to a given fixed distribution.},
  urldate = {2018-07-30},
  date = {2018-02-12},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  author = {Abdullah, Mohammed Amin and Pacchiano, Aldo and Draief, Moez},
  file = {/home/arthur/Dropbox/Zotero/Abdullah et al_2018_A note on reinforcement learning with Wasserstein distance regularisation, with.pdf;/home/arthur/Zotero/storage/I7RMF5TK/1802.html}
}

@article{lee_maximum_2018,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1805.08336},
  primaryClass = {cs, stat},
  langid = {english},
  title = {Maximum {{Causal Tsallis Entropy Imitation Learning}}},
  url = {http://arxiv.org/abs/1805.08336},
  abstract = {In this paper, we propose a novel maximum causal Tsallis entropy (MCTE) framework for imitation learning which can efﬁciently learn a sparse multi-modal policy distribution from demonstrations. We provide the full mathematical analysis of the proposed framework. First, the optimal solution of an MCTE problem is shown to be a sparsemax distribution, whose supporting set can be adjusted. The proposed method has advantages over a softmax distribution in that it can exclude unnecessary actions by assigning zero probability. Second, we prove that an MCTE problem is equivalent to robust Bayes estimation in the sense of the Brier score. Third, we propose a maximum causal Tsallis entropy imitation learning (MCTEIL) algorithm with a sparse mixture density network (sparse MDN) by modeling mixture weights using a sparsemax distribution. In particular, we show that the causal Tsallis entropy of an MDN encourages exploration and efﬁcient mixture utilization while Boltzmann Gibbs entropy is less effective. We validate the proposed method in two simulation studies and MCTEIL outperforms existing imitation learning methods in terms of average returns and learning multi-modal policies.},
  urldate = {2018-07-30},
  date = {2018-05-21},
  keywords = {Statistics - Machine Learning,Computer Science - Machine Learning},
  author = {Lee, Kyungjae and Choi, Sungjoon and Oh, Songhwai},
  file = {/home/arthur/Dropbox/Zotero/Lee et al_2018_Maximum Causal Tsallis Entropy Imitation Learning.pdf}
}

@inproceedings{chow_path_2018,
  langid = {english},
  title = {Path {{Consistency Learning}} in {{Tsallis Entropy Regularized MDPs}}},
  url = {http://proceedings.mlr.press/v80/chow18a.html},
  abstract = {We study the sparse entropy-regularized reinforcement learning (ERL) problem in which the entropy term is a special form of the Tsallis entropy. The optimal policy of this formulation is sparse, i....},
  eventtitle = {International {{Conference}} on {{Machine Learning}}},
  booktitle = {International {{Conference}} on {{Machine Learning}}},
  urldate = {2018-07-30},
  date = {2018-07-03},
  pages = {978-987},
  author = {Chow, Yinlam and Nachum, Ofir and Ghavamzadeh, Mohammad},
  file = {/home/arthur/Dropbox/Zotero/Chow et al_2018_Path Consistency Learning in Tsallis Entropy Regularized MDPs.pdf;/home/arthur/Zotero/storage/M8859IZJ/chow18a.html}
}

@article{resnick_backplay_2018,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1807.06919},
  primaryClass = {cs, stat},
  title = {Backplay: "{{Man}} Muss Immer Umkehren"},
  url = {http://arxiv.org/abs/1807.06919},
  shorttitle = {Backplay},
  abstract = {A long-standing problem in model free reinforcement learning (RL) is that it requires a large number of trials to learn a good policy, especially in environments with sparse rewards. We explore a method to increase the sample efficiency of RL when we have access to demonstrations. Our approach, which we call Backplay, uses a single demonstration to construct a curriculum for a given task. Rather than starting each training episode in the environment's fixed initial state, we start the agent near the end of the demonstration and move the starting point backwards during the course of training until we reach the initial state. We perform experiments in a competitive four player game (Pommerman) and a path-finding maze game. We find that this weak form of guidance provides significant gains in sample complexity with a stark advantage in sparse reward environments. In some cases, standard RL did not yield any improvement while Backplay reached success rates greater than 50\% and generalized to unseen initial conditions in the same amount of training time. Additionally, we see that agents trained via Backplay can learn policies superior to those of the original demonstration.},
  urldate = {2018-07-30},
  date = {2018-07-18},
  keywords = {Statistics - Machine Learning,Computer Science - Artificial Intelligence,Computer Science - Machine Learning,_tablet},
  author = {Resnick, Cinjon and Raileanu, Roberta and Kapoor, Sanyam and Peysakhovich, Alex and Cho, Kyunghyun and Bruna, Joan},
  file = {/home/arthur/Zotero/storage/IWVC5SKP/Resnick et al_2018_Backplay.pdf;/home/arthur/Zotero/storage/SUR4JFL8/1807.html}
}

@article{silver_mastering_2017-1,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1712.01815},
  primaryClass = {cs},
  title = {Mastering {{Chess}} and {{Shogi}} by {{Self}}-{{Play}} with a {{General Reinforcement Learning Algorithm}}},
  url = {http://arxiv.org/abs/1712.01815},
  abstract = {The game of chess is the most widely-studied domain in the history of artificial intelligence. The strongest programs are based on a combination of sophisticated search techniques, domain-specific adaptations, and handcrafted evaluation functions that have been refined by human experts over several decades. In contrast, the AlphaGo Zero program recently achieved superhuman performance in the game of Go, by tabula rasa reinforcement learning from games of self-play. In this paper, we generalise this approach into a single AlphaZero algorithm that can achieve, tabula rasa, superhuman performance in many challenging domains. Starting from random play, and given no domain knowledge except the game rules, AlphaZero achieved within 24 hours a superhuman level of play in the games of chess and shogi (Japanese chess) as well as Go, and convincingly defeated a world-champion program in each case.},
  urldate = {2018-07-30},
  date = {2017-12-05},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,_tablet},
  author = {Silver, David and Hubert, Thomas and Schrittwieser, Julian and Antonoglou, Ioannis and Lai, Matthew and Guez, Arthur and Lanctot, Marc and Sifre, Laurent and Kumaran, Dharshan and Graepel, Thore and Lillicrap, Timothy and Simonyan, Karen and Hassabis, Demis},
  file = {/home/arthur/Zotero/storage/Y36PF6H6/Silver et al_2017_Mastering Chess and Shogi by Self-Play with a General Reinforcement Learning.pdf;/home/arthur/Zotero/storage/29U23LRU/1712.html}
}

@inproceedings{haarnoja_off-policy_2018,
  langid = {english},
  title = {Off-{{Policy Maximum Entropy Deep Reinforcement Learning}} with a {{Stochastic Actor}}},
  abstract = {Model-free deep reinforcement learning (RL) algorithms have been demonstrated on a range of challenging decision making and control tasks. However, these methods typically suffer from two major challenges: very high sample complexity and brittle convergence properties, which necessitate meticulous hyperparameter tuning. Both of these challenges severely limit the applicability of such methods to complex, real-world domains. In this paper, we propose soft actor-critic, an off-policy actor-critic deep RL algorithm based on the maximum entropy reinforcement learning framework. In this framework, the actor aims to maximize expected reward while also maximizing entropy—that is, to succeed at the task while acting as randomly as possible. Prior deep RL methods based on this framework have been formulated as Q-learning methods. By combining off-policy updates with a stable stochastic actorcritic formulation, our method achieves state-ofthe-art performance on a range of continuous control benchmark tasks, outperforming prior onpolicy and off-policy methods. Furthermore, we demonstrate that, in contrast to other off-policy algorithms, our approach is very stable, achieving very similar performance across different random seeds.},
  booktitle = {Proceedings of  the {{International Conference}} on {{Machine Learning}}},
  date = {2018},
  pages = {10},
  keywords = {_tablet},
  author = {Haarnoja, Tuomas and Zhou, Aurick and Abbeel, Pieter and Levine, Sergey},
  file = {/home/arthur/Zotero/storage/H6CUCV6N/Haarnoja et al_2018_Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor.pdf}
}

@article{niculae_sparsemap_2018,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1802.04223},
  primaryClass = {cs, stat},
  title = {{{SparseMAP}}: {{Differentiable Sparse Structured Inference}}},
  url = {http://arxiv.org/abs/1802.04223},
  shorttitle = {{{SparseMAP}}},
  abstract = {Structured prediction requires searching over a combinatorial number of structures. To tackle it, we introduce SparseMAP: a new method for sparse structured inference, and its natural loss function. SparseMAP automatically selects only a few global structures: it is situated between MAP inference, which picks a single structure, and marginal inference, which assigns probability mass to all structures, including implausible ones. Importantly, SparseMAP can be computed using only calls to a MAP oracle, making it applicable to problems with intractable marginal inference, e.g., linear assignment. Sparsity makes gradient backpropagation efficient regardless of the structure, enabling us to augment deep neural networks with generic and sparse structured hidden layers. Experiments in dependency parsing and natural language inference reveal competitive accuracy, improved interpretability, and the ability to capture natural language ambiguities, which is attractive for pipeline systems.},
  urldate = {2018-07-30},
  date = {2018-02-12},
  keywords = {Statistics - Machine Learning,Computer Science - Computation and Language,I.2.6,68T50,Computer Science - Machine Learning},
  author = {Niculae, Vlad and Martins, André F. T. and Blondel, Mathieu and Cardie, Claire},
  file = {/home/arthur/Dropbox/Zotero/Niculae et al_2018_SparseMAP.pdf;/home/arthur/Zotero/storage/9W2CMY6R/1802.html}
}

@unpublished{kolter_markov_2016,
  langid = {english},
  title = {Markov {{Decision Processes}}},
  date = {2016},
  author = {Kolter, J Zico},
  file = {/home/arthur/Dropbox/Zotero/Kolter_2016_Markov Decision Processes.pdf}
}

@article{seguy_large-scale_2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1711.02283},
  primaryClass = {stat},
  title = {Large-{{Scale Optimal Transport}} and {{Mapping Estimation}}},
  url = {http://arxiv.org/abs/1711.02283},
  abstract = {This paper presents a novel two-step approach for the fundamental problem of learning an optimal map from one distribution to another. First, we learn an optimal transport (OT) plan, which can be thought as a one-to-many map between the two distributions. To that end, we propose a stochastic dual approach of regularized OT, and show empirically that it scales better than a recent related approach when the amount of samples is very large. Second, we estimate a $\backslash$textit\{Monge map\} as a deep neural network learned by approximating the barycentric projection of the previously-obtained OT plan. This parameterization allows generalization of the mapping outside the support of the input measure. We prove two theoretical stability results of regularized OT which show that our estimations converge to the OT plan and Monge map between the underlying continuous measures. We showcase our proposed approach on two applications: domain adaptation and generative modeling.},
  urldate = {2018-08-02},
  date = {2017-11-06},
  keywords = {Statistics - Machine Learning},
  author = {Seguy, Vivien and Damodaran, Bharath Bhushan and Flamary, Rémi and Courty, Nicolas and Rolet, Antoine and Blondel, Mathieu},
  file = {/home/arthur/Dropbox/Zotero/Seguy et al_2017_Large-Scale Optimal Transport and Mapping Estimation.pdf;/home/arthur/Zotero/storage/VUP7RKQT/1711.html}
}

@article{anthony_thinking_2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1705.08439},
  primaryClass = {cs},
  title = {Thinking {{Fast}} and {{Slow}} with {{Deep Learning}} and {{Tree Search}}},
  url = {http://arxiv.org/abs/1705.08439},
  abstract = {Sequential decision making problems, such as structured prediction, robotic control, and game playing, require a combination of planning policies and generalisation of those plans. In this paper, we present Expert Iteration (ExIt), a novel reinforcement learning algorithm which decomposes the problem into separate planning and generalisation tasks. Planning new policies is performed by tree search, while a deep neural network generalises those plans. Subsequently, tree search is improved by using the neural network policy to guide search, increasing the strength of new plans. In contrast, standard deep Reinforcement Learning algorithms rely on a neural network not only to generalise plans, but to discover them too. We show that ExIt outperforms REINFORCE for training a neural network to play the board game Hex, and our final tree search agent, trained tabula rasa, defeats MoHex 1.0, the most recent Olympiad Champion player to be publicly released.},
  urldate = {2018-08-06},
  date = {2017-05-23},
  keywords = {Computer Science - Artificial Intelligence},
  author = {Anthony, Thomas and Tian, Zheng and Barber, David},
  file = {/home/arthur/Dropbox/Zotero/Anthony et al_2017_Thinking Fast and Slow with Deep Learning and Tree Search.pdf;/home/arthur/Zotero/storage/4GP53YKU/1705.html}
}

@article{schulman_trust_2015,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1502.05477},
  primaryClass = {cs},
  title = {Trust {{Region Policy Optimization}}},
  url = {http://arxiv.org/abs/1502.05477},
  abstract = {We describe an iterative procedure for optimizing policies, with guaranteed monotonic improvement. By making several approximations to the theoretically-justified procedure, we develop a practical algorithm, called Trust Region Policy Optimization (TRPO). This algorithm is similar to natural policy gradient methods and is effective for optimizing large nonlinear policies such as neural networks. Our experiments demonstrate its robust performance on a wide variety of tasks: learning simulated robotic swimming, hopping, and walking gaits; and playing Atari games using images of the screen as input. Despite its approximations that deviate from the theory, TRPO tends to give monotonic improvement, with little tuning of hyperparameters.},
  urldate = {2018-08-06},
  date = {2015-02-19},
  keywords = {Computer Science - Machine Learning},
  author = {Schulman, John and Levine, Sergey and Moritz, Philipp and Jordan, Michael I. and Abbeel, Pieter},
  file = {/home/arthur/Dropbox/Zotero/Schulman et al_2015_Trust Region Policy Optimization.pdf;/home/arthur/Zotero/storage/2ERB5MIY/1502.html}
}

@article{arora_simple_2015,
  langid = {english},
  title = {Simple, {{Eﬃcient}}, and {{Neural Algorithms}} for {{Sparse Coding}}},
  abstract = {Sparse coding is a basic task in many ﬁelds including signal processing, neuroscience and machine learning where the goal is to learn a basis that enables a sparse representation of a given set of data, if one exists. Its standard formulation is as a non-convex optimization problem which is solved in practice by heuristics based on alternating minimization. Recent work has resulted in several algorithms for sparse coding with provable guarantees, but somewhat surprisingly these are outperformed by the simple alternating minimization heuristics. Here we give a general framework for understanding alternating minimization which we leverage to analyze existing heuristics and to design new ones also with provable guarantees. Some of these algorithms seem implementable on simple neural architectures, which was the original motivation of Olshausen and Field (1997a) in introducing sparse coding. We also give the ﬁrst eﬃcient algorithm for sparse coding that works almost up to the information theoretic limit for sparse recovery on incoherent dictionaries. All previous algorithms that approached or surpassed this limit run in time exponential in some natural parameter. Finally, our algorithms improve upon the sample complexity of existing approaches. We believe that our analysis framework will have applications in other settings where simple iterative algorithms are used.},
  date = {2015},
  pages = {37},
  author = {Arora, Sanjeev and Ge, Rong and Ma, Tengyu and Moitra, Ankur},
  file = {/home/arthur/Dropbox/Zotero/Arora et al_2015_Simple, Eﬃcient, and Neural Algorithms for Sparse Coding.pdf}
}

@incollection{guo_deep_2014,
  title = {Deep {{Learning}} for {{Real}}-{{Time Atari Game Play Using Offline Monte}}-{{Carlo Tree Search Planning}}},
  url = {http://papers.nips.cc/paper/5421-deep-learning-for-real-time-atari-game-play-using-offline-monte-carlo-tree-search-planning.pdf},
  booktitle = {Advances in {{Neural Information Processing Systems}} 27},
  publisher = {{Curran Associates, Inc.}},
  urldate = {2018-08-07},
  date = {2014},
  pages = {3338--3346},
  author = {Guo, Xiaoxiao and Singh, Satinder and Lee, Honglak and Lewis, Richard L and Wang, Xiaoshi},
  editor = {Ghahramani, Z. and Welling, M. and Cortes, C. and Lawrence, N. D. and Weinberger, K. Q.},
  file = {/home/arthur/Dropbox/Zotero/Guo et al_2014_Deep Learning for Real-Time Atari Game Play Using Offline Monte-Carlo Tree.pdf;/home/arthur/Zotero/storage/24SUNNZB/5421-deep-learning-for-real-time-atari-game-play-using-offline-monte-carlo-tree-search-planning.html}
}

@article{heinrich_deep_2016,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1603.01121},
  primaryClass = {cs},
  langid = {english},
  title = {Deep {{Reinforcement Learning}} from {{Self}}-{{Play}} in {{Imperfect}}-{{Information Games}}},
  url = {http://arxiv.org/abs/1603.01121},
  abstract = {Many real-world applications can be described as large-scale games of imperfect information. To deal with these challenging domains, prior work has focused on computing Nash equilibria in a handcrafted abstraction of the domain. In this paper we introduce the ﬁrst scalable end-to-end approach to learning approximate Nash equilibria without prior domain knowledge. Our method combines ﬁctitious self-play with deep reinforcement learning. When applied to Leduc poker, Neural Fictitious Self-Play (NFSP) approached a Nash equilibrium, whereas common reinforcement learning methods diverged. In Limit Texas Hold’em, a poker game of real-world scale, NFSP learnt a strategy that approached the performance of state-of-the-art, superhuman algorithms based on signiﬁcant domain expertise.},
  urldate = {2018-08-08},
  date = {2016-03-03},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Computer Science and Game Theory},
  author = {Heinrich, Johannes and Silver, David},
  file = {/home/arthur/Dropbox/Zotero/Heinrich_Silver_2016_Deep Reinforcement Learning from Self-Play in Imperfect-Information Games.pdf}
}

@article{lanctot_unified_2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1711.00832},
  primaryClass = {cs},
  langid = {english},
  title = {A {{Unified Game}}-{{Theoretic Approach}} to {{Multiagent Reinforcement Learning}}},
  url = {http://arxiv.org/abs/1711.00832},
  abstract = {To achieve general intelligence, agents must learn how to interact with others in a shared environment: this is the challenge of multiagent reinforcement learning (MARL). The simplest form is independent reinforcement learning (InRL), where each agent treats its experience as part of its (non-stationary) environment. In this paper, we ﬁrst observe that policies learned using InRL can overﬁt to the other agents’ policies during training, failing to sufﬁciently generalize during execution. We introduce a new metric, joint-policy correlation, to quantify this effect. We describe an algorithm for general MARL, based on approximate best responses to mixtures of policies generated using deep reinforcement learning, and empirical game-theoretic analysis to compute meta-strategies for policy selection. The algorithm generalizes previous ones such as InRL, iterated best response, double oracle, and ﬁctitious play. Then, we present a scalable implementation which reduces the memory requirement using decoupled meta-solvers. Finally, we demonstrate the generality of the resulting policies in two partially observable settings: gridworld coordination games and poker.},
  urldate = {2018-08-08},
  date = {2017-11-02},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Computer Science and Game Theory,Computer Science - Multiagent Systems},
  author = {Lanctot, Marc and Zambaldi, Vinicius and Gruslys, Audrunas and Lazaridou, Angeliki and Tuyls, Karl and Perolat, Julien and Silver, David and Graepel, Thore},
  file = {/home/arthur/Dropbox/Zotero/Lanctot et al_2017_A Unified Game-Theoretic Approach to Multiagent Reinforcement Learning.pdf}
}

@inproceedings{saffidine_alpha-beta_2012,
  title = {Alpha-{{Beta Pruning}} for {{Games}} with {{Simultaneous Moves}}.},
  booktitle = {{{AAAI}}},
  date = {2012},
  author = {Saffidine, Abdallah and Finnsson, Hilmar and Buro, Michael},
  file = {/home/arthur/Zotero/storage/AWK42HQK/5238.html}
}

@article{ontanon_combinatorial_2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1710.04805},
  primaryClass = {cs},
  langid = {english},
  title = {Combinatorial {{Multi}}-Armed {{Bandits}} for {{Real}}-{{Time Strategy Games}}},
  url = {http://arxiv.org/abs/1710.04805},
  abstract = {Games with large branching factors pose a signiﬁcant challenge for game tree search algorithms. In this paper, we address this problem with a sampling strategy for Monte Carlo Tree Search (MCTS) algorithms called na¨ıve sampling, based on a variant of the Multiarmed Bandit problem called Combinatorial Multi-armed Bandits (CMAB). We analyze the theoretical properties of several variants of na¨ıve sampling, and empirically compare it against the other existing strategies in the literature for CMABs. We then evaluate these strategies in the context of real-time strategy (RTS) games, a genre of computer games characterized by their very large branching factors. Our results show that as the branching factor grows, na¨ıve sampling outperforms the other sampling strategies.},
  urldate = {2018-08-08},
  date = {2017-10-13},
  keywords = {Computer Science - Artificial Intelligence},
  author = {Ontañón, Santiago},
  file = {/home/arthur/Dropbox/Zotero/Ontañón_2017_Combinatorial Multi-armed Bandits for Real-Time Strategy Games.pdf}
}

@article{kapoor_multi-agent_2018,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1807.09427},
  primaryClass = {cs, stat},
  langid = {english},
  title = {Multi-{{Agent Reinforcement Learning}}: {{A Report}} on {{Challenges}} and {{Approaches}}},
  url = {http://arxiv.org/abs/1807.09427},
  shorttitle = {Multi-{{Agent Reinforcement Learning}}},
  abstract = {Reinforcement Learning (RL) is a learning paradigm concerned with learning to control a system so as to maximize an objective over the long term. This approach to learning has received immense interest in recent times and success manifests itself in the form of human-level performance on games like Go. While RL is emerging as a practical component in reallife systems, most successes have been in Single Agent domains. This report will instead speciﬁcally focus on challenges that are unique to Multi-Agent Systems interacting in mixed cooperative and competitive environments. The report concludes with advances in the paradigm of training Multi-Agent Systems called Decentralized Actor, Centralized Critic, based on an extension of MDPs called Decentralized Partially Observable MDPs, which has seen a renewed interest lately.},
  urldate = {2018-08-28},
  date = {2018-07-24},
  keywords = {Statistics - Machine Learning,Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  author = {Kapoor, Sanyam},
  file = {/home/arthur/Dropbox/Zotero/Kapoor_2018_Multi-Agent Reinforcement Learning.pdf}
}

@article{resnick_backplay_2018-1,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1807.06919},
  primaryClass = {cs, stat},
  langid = {english},
  title = {Backplay: "{{Man}} Muss Immer Umkehren"},
  url = {http://arxiv.org/abs/1807.06919},
  shorttitle = {Backplay},
  abstract = {A long-standing problem in model free reinforcement learning (RL) is that it requires a large number of trials to learn a good policy, especially in environments with sparse rewards. We explore a method to increase the sample efﬁciency of RL when we have access to demonstrations. Our approach, which we call Backplay, uses a single demonstration to construct a curriculum for a given task. Rather than starting each training episode in the environment’s ﬁxed initial state, we start the agent near the end of the demonstration and move the starting point backwards during the course of training until we reach the initial state. We perform experiments in a competitive four player game (Pommerman) and a path-ﬁnding maze game. We ﬁnd that this weak form of guidance provides signiﬁcant gains in sample complexity with a stark advantage in sparse reward environments. In some cases, standard RL did not yield any improvement while Backplay reached success rates greater than 50\% and generalized to unseen initial conditions in the same amount of training time. Additionally, we see that agents trained via Backplay can learn policies superior to those of the original demonstration.},
  urldate = {2018-08-28},
  date = {2018-07-18},
  keywords = {Statistics - Machine Learning,Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  author = {Resnick, Cinjon and Raileanu, Roberta and Kapoor, Sanyam and Peysakhovich, Alex and Cho, Kyunghyun and Bruna, Joan},
  file = {/home/arthur/Dropbox/Zotero/Resnick et al_2018_Backplay.pdf}
}

@incollection{segal_scalability_2011,
  langid = {english},
  location = {{Berlin, Heidelberg}},
  title = {On the {{Scalability}} of {{Parallel UCT}}},
  volume = {6515},
  isbn = {978-3-642-17927-3 978-3-642-17928-0},
  url = {http://link.springer.com/10.1007/978-3-642-17928-0_4},
  abstract = {The parallelization of MCTS across multiple-machines has proven surprisingly diﬃcult. The limitations of existing algorithms were evident in the 2009 Computer Olympiad where Zen using a single fourcore machine defeated both Fuego with ten eight-core machines, and Mogo with twenty thirty-two core machines. This paper investigates the limits of parallel MCTS in order to understand why distributed parallelism has proven so diﬃcult and to pave the way towards future distributed algorithms with better scaling. We ﬁrst analyze the singlethreaded scaling of Fuego and ﬁnd that there is an upper bound on the play-quality improvements which can come from additional search. We then analyze the scaling of an idealized N-core shared memory machine to determine the maximum amount of parallelism supported by MCTS. We show that parallel speedup depends critically on how much time is given to each player. We use this relationship to predict parallel scaling for time scales beyond what can be empirically evaluated due to the immense computation required. Our results show that MCTS can scale nearly perfectly to at least 64 threads when combined with virtual loss, but without virtual loss scaling is limited to just eight threads. We also ﬁnd that for competition time controls scaling to thousands of threads is impossible not necessarily due to MCTS not scaling, but because high levels of parallelism can start to bump up against the upper performance bound of Fuego itself.},
  booktitle = {Computers and {{Games}}},
  publisher = {{Springer Berlin Heidelberg}},
  urldate = {2018-08-28},
  date = {2011},
  pages = {36-47},
  author = {Segal, B., Richard},
  file = {/home/arthur/Dropbox/Zotero/Segal_2011_On the Scalability of Parallel UCT.pdf},
  doi = {10.1007/978-3-642-17928-0_4}
}

@unpublished{bartlett_m-estimator_2013,
  langid = {english},
  title = {M-Estimator Consistency},
  date = {2013},
  author = {Bartlett, Peter},
  file = {/home/arthur/Dropbox/Zotero/Bartlett_2013_M-estimator consistency.pdf}
}

@article{azizzadenesheli_sample-efficient_2018,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1806.05780},
  primaryClass = {cs, stat},
  langid = {english},
  title = {Sample-{{Efficient Deep RL}} with {{Generative Adversarial Tree Search}}},
  url = {http://arxiv.org/abs/1806.05780},
  abstract = {We propose Generative Adversarial Tree Search (GATS), a sample-efﬁcient Deep Reinforcement Learning (DRL) algorithm. While Monte Carlo Tree Search (MCTS) is known to be effective for search and planning in RL, it is often sampleinefﬁcient and therefore expensive to apply in practice. In this work, we develop a Generative Adversarial Network (GAN) architecture to model an environment’s dynamics and a predictor model for the reward function. We exploit collected data from interaction with the environment to learn these models, which we then use for model-based planning. During planning, we deploy a ﬁnite depth MCTS, using the learned model for tree search and a learned Q-value for the leaves, to ﬁnd the best action. We theoretically show that GATS improves the bias-variance tradeoff in value-based DRL. Moreover, we show that the generative model learns the model dynamics using orders of magnitude fewer samples than the Q-learner. In non-stationary settings where the environment model changes, we ﬁnd the generative model adapts signiﬁcantly faster than the Q-learner to the new environment.},
  urldate = {2018-09-03},
  date = {2018-06-14},
  keywords = {Statistics - Machine Learning,Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  author = {Azizzadenesheli, Kamyar and Yang, Brandon and Liu, Weitang and Brunskill, Emma and Lipton, Zachary C. and Anandkumar, Animashree},
  file = {/home/arthur/Dropbox/Zotero/Azizzadenesheli et al_2018_Sample-Efficient Deep RL with Generative Adversarial Tree Search.pdf}
}

@article{mei_mean_2018,
  langid = {english},
  title = {A Mean Field View of the Landscape of Two-Layer Neural Networks},
  issn = {0027-8424, 1091-6490},
  url = {http://www.pnas.org/content/early/2018/07/26/1806579115},
  abstract = {Multilayer neural networks are among the most powerful models in machine learning, yet the fundamental reasons for this success defy mathematical understanding. Learning a neural network requires optimizing a nonconvex high-dimensional objective (risk function), a problem that is usually attacked using stochastic gradient descent (SGD). Does SGD converge to a global optimum of the risk or only to a local optimum? In the former case, does this happen because local minima are absent or because SGD somehow avoids them? In the latter, why do local minima reached by SGD have good generalization properties? In this paper, we consider a simple case, namely two-layer neural networks, and prove that—in a suitable scaling limit—SGD dynamics is captured by a certain nonlinear partial differential equation (PDE) that we call distributional dynamics (DD). We then consider several specific examples and show how DD can be used to prove convergence of SGD to networks with nearly ideal generalization error. This description allows for “averaging out” some of the complexities of the landscape of neural networks and can be used to prove a general convergence result for noisy SGD.},
  journaltitle = {Proceedings of the National Academy of Sciences},
  shortjournal = {PNAS},
  urldate = {2018-09-03},
  date = {2018-07-27},
  pages = {201806579},
  keywords = {gradient flow,neural networks,partial differential equations,stochastic gradient descent,Wasserstein space},
  author = {Mei, Song and Montanari, Andrea and Nguyen, Phan-Minh},
  file = {/home/arthur/Dropbox/Zotero/Mei et al_2018_A mean field view of the landscape of two-layer neural networks.pdf;/home/arthur/Dropbox/Zotero/Mei et al_2018_A mean field view of the landscape of two-layer neural networks.pdf;/home/arthur/Zotero/storage/A5ILE24U/1806579115.html},
  eprinttype = {pmid},
  eprint = {30054315}
}

@article{dalalyan_further_2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1704.04752},
  primaryClass = {math, stat},
  langid = {english},
  title = {Further and Stronger Analogy between Sampling and Optimization: {{Langevin Monte Carlo}} and Gradient Descent},
  url = {http://arxiv.org/abs/1704.04752},
  shorttitle = {Further and Stronger Analogy between Sampling and Optimization},
  abstract = {In this paper1, we revisit the recently established theoretical guarantees for the convergence of the Langevin Monte Carlo algorithm of sampling from a smooth and (strongly) log-concave density. We improve the existing results when the convergence is measured in the Wasserstein distance and provide further insights on the very tight relations between, on the one hand, the Langevin Monte Carlo for sampling and, on the other hand, the gradient descent for optimization. Finally, we also establish guarantees for the convergence of a version of the Langevin Monte Carlo algorithm that is based on noisy evaluations of the gradient.},
  urldate = {2018-09-03},
  date = {2017-04-16},
  keywords = {Mathematics - Statistics Theory},
  author = {Dalalyan, Arnak S.},
  file = {/home/arthur/Dropbox/Zotero/Dalalyan_2017_Further and stronger analogy between sampling and optimization.pdf}
}

@article{genevay_learning_2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1706.00292},
  primaryClass = {stat},
  title = {Learning {{Generative Models}} with {{Sinkhorn Divergences}}},
  url = {http://arxiv.org/abs/1706.00292},
  abstract = {The ability to compare two degenerate probability distributions (i.e. two probability distributions supported on two distinct low-dimensional manifolds living in a much higher-dimensional space) is a crucial problem arising in the estimation of generative models for high-dimensional observations such as those arising in computer vision or natural language. It is known that optimal transport metrics can represent a cure for this problem, since they were specifically designed as an alternative to information divergences to handle such problematic scenarios. Unfortunately, training generative machines using OT raises formidable computational and statistical challenges, because of (i) the computational burden of evaluating OT losses, (ii) the instability and lack of smoothness of these losses, (iii) the difficulty to estimate robustly these losses and their gradients in high dimension. This paper presents the first tractable computational method to train large scale generative models using an optimal transport loss, and tackles these three issues by relying on two key ideas: (a) entropic smoothing, which turns the original OT loss into one that can be computed using Sinkhorn fixed point iterations; (b) algorithmic (automatic) differentiation of these iterations. These two approximations result in a robust and differentiable approximation of the OT loss with streamlined GPU execution. Entropic smoothing generates a family of losses interpolating between Wasserstein (OT) and Maximum Mean Discrepancy (MMD), thus allowing to find a sweet spot leveraging the geometry of OT and the favorable high-dimensional sample complexity of MMD which comes with unbiased gradient estimates. The resulting computational architecture complements nicely standard deep network generative models by a stack of extra layers implementing the loss function.},
  urldate = {2018-09-04},
  date = {2017-06-01},
  keywords = {Statistics - Machine Learning},
  author = {Genevay, Aude and Peyré, Gabriel and Cuturi, Marco},
  file = {/home/arthur/Dropbox/Zotero/Genevay et al_2017_Learning Generative Models with Sinkhorn Divergences.pdf;/home/arthur/Zotero/storage/SCM9V2XG/1706.html}
}

@article{karakida_universal_2018,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1806.01316},
  primaryClass = {cond-mat, stat},
  title = {Universal {{Statistics}} of {{Fisher Information}} in {{Deep Neural Networks}}: {{Mean Field Approach}}},
  url = {http://arxiv.org/abs/1806.01316},
  shorttitle = {Universal {{Statistics}} of {{Fisher Information}} in {{Deep Neural Networks}}},
  abstract = {This study analyzes the Fisher information matrix (FIM) by applying mean-field theory to deep neural networks with random weights. We theoretically find novel statistics of the FIM, which are universal among a wide class of deep networks with any number of layers and various activation functions. Although most of the FIM's eigenvalues are close to zero, the maximum eigenvalue takes on a huge value and the eigenvalue distribution has an extremely long tail. These statistics suggest that the shape of a loss landscape is locally flat in most dimensions, but strongly distorted in the other dimensions. Moreover, our theory of the FIM leads to quantitative evaluation of learning in deep networks. First, the maximum eigenvalue enables us to estimate an appropriate size of a learning rate for steepest gradient methods to converge. Second, the flatness induced by the small eigenvalues is connected to generalization ability through a norm-based capacity measure.},
  urldate = {2018-09-04},
  date = {2018-06-04},
  keywords = {Statistics - Machine Learning,Computer Science - Machine Learning,Condensed Matter - Disordered Systems and Neural Networks},
  author = {Karakida, Ryo and Akaho, Shotaro and Amari, Shun-ichi},
  file = {/home/arthur/Dropbox/Zotero/Karakida et al_2018_Universal Statistics of Fisher Information in Deep Neural Networks.pdf;/home/arthur/Zotero/storage/RMHHXUL8/1806.html}
}

@article{nielsen_elementary_2018,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1808.08271},
  primaryClass = {cs, math, stat},
  title = {An Elementary Introduction to Information Geometry},
  url = {http://arxiv.org/abs/1808.08271},
  abstract = {We describe the fundamental differential-geometric structures of information manifolds, state the fundamental theorem of information geometry, and illustrate some uses of these information manifolds in information sciences. The exposition is self-contained by concisely introducing the necessary concepts of differential geometry with proofs omitted for brevity.},
  urldate = {2018-09-04},
  date = {2018-08-16},
  keywords = {Statistics - Machine Learning,Computer Science - Information Theory,Computer Science - Machine Learning},
  author = {Nielsen, Frank},
  file = {/home/arthur/Dropbox/Zotero/Nielsen_2018_An elementary introduction to information geometry.pdf;/home/arthur/Zotero/storage/7XRHFM4P/1808.html}
}

@article{nocedal_updating_1980,
  langid = {american},
  title = {Updating Quasi-{{Newton}} Matrices with Limited Storage},
  volume = {35},
  issn = {0025-5718, 1088-6842},
  url = {https://www.ams.org/home/page/},
  abstract = {We study how to use the BFGS quasi-Newton matrices to precondition minimization methods for problems where the storage is critical. We give an update formula which generates matrices using information from the last m iterations, where m is any number supplied by the user. The quasi-Newton matrix is updated at every iteration by dropping the oldest information and replacing it by the newest information. It is shown that the matrices generated have some desirable properties.},
  number = {151},
  journaltitle = {Mathematics of Computation},
  shortjournal = {Math. Comp.},
  urldate = {2018-09-05},
  date = {1980},
  pages = {773-782},
  author = {Nocedal, Jorge},
  file = {/home/arthur/Dropbox/Zotero/Nocedal_1980_Updating quasi-Newton matrices with limited storage.pdf;/home/arthur/Zotero/storage/XVMAWKDJ/home.html}
}

@article{varoquaux_cross-validation_2018,
  title = {Cross-Validation Failure: {{Small}} Sample Sizes Lead to Large Error Bars},
  volume = {180},
  issn = {1053-8119},
  url = {http://www.sciencedirect.com/science/article/pii/S1053811917305311},
  shorttitle = {Cross-Validation Failure},
  abstract = {Predictive models ground many state-of-the-art developments in statistical brain image analysis: decoding, MVPA, searchlight, or extraction of biomarkers. The principled approach to establish their validity and usefulness is cross-validation, testing prediction on unseen data. Here, I would like to raise awareness on error bars of cross-validation, which are often underestimated. Simple experiments show that sample sizes of many neuroimaging studies inherently lead to large error bars, eg±10\% for 100 samples. The standard error across folds strongly underestimates them. These large error bars compromise the reliability of conclusions drawn with predictive models, such as biomarkers or methods developments where, unlike with cognitive neuroimaging MVPA approaches, more samples cannot be acquired by repeating the experiment across many subjects. Solutions to increase sample size must be investigated, tackling possible increases in heterogeneity of the data.},
  journaltitle = {NeuroImage},
  shortjournal = {NeuroImage},
  series = {New advances in encoding and decoding of brain signals},
  urldate = {2018-09-05},
  date = {2018-10-15},
  pages = {68-77},
  keywords = {MVPA,fMRI,Decoding,Biomarkers,Cross-validation,Model selection,Statistics},
  author = {Varoquaux, Gaël},
  file = {/home/arthur/Dropbox/Zotero/Varoquaux_2018_Cross-validation failure.pdf;/home/arthur/Zotero/storage/9MXRPQCL/S1053811917305311.html}
}

@article{tibshirani_dykstras_2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1705.04768},
  primaryClass = {math, stat},
  title = {Dykstra's {{Algorithm}}, {{ADMM}}, and {{Coordinate Descent}}: {{Connections}}, {{Insights}}, and {{Extensions}}},
  url = {http://arxiv.org/abs/1705.04768},
  shorttitle = {Dykstra's {{Algorithm}}, {{ADMM}}, and {{Coordinate Descent}}},
  abstract = {We study connections between Dykstra's algorithm for projecting onto an intersection of convex sets, the augmented Lagrangian method of multipliers or ADMM, and block coordinate descent. We prove that coordinate descent for a regularized regression problem, in which the (separable) penalty functions are seminorms, is exactly equivalent to Dykstra's algorithm applied to the dual problem. ADMM on the dual problem is also seen to be equivalent, in the special case of two sets, with one being a linear subspace. These connections, aside from being interesting in their own right, suggest new ways of analyzing and extending coordinate descent. For example, from existing convergence theory on Dykstra's algorithm over polyhedra, we discern that coordinate descent for the lasso problem converges at an (asymptotically) linear rate. We also develop two parallel versions of coordinate descent, based on the Dykstra and ADMM connections.},
  urldate = {2018-09-07},
  date = {2017-05-12},
  keywords = {Mathematics - Optimization and Control,Statistics - Computation},
  author = {Tibshirani, Ryan J.},
  file = {/home/arthur/Dropbox/Zotero/Tibshirani_2017_Dykstra's Algorithm, ADMM, and Coordinate Descent.pdf;/home/arthur/Zotero/storage/KHZBP3GX/1705.html}
}

@article{kim_sparse_2007-1,
  langid = {english},
  title = {Sparse Non-Negative Matrix Factorizations via Alternating Non-Negativity-Constrained Least Squares for Microarray Data Analysis},
  volume = {23},
  issn = {1367-4811},
  doi = {10.1093/bioinformatics/btm134},
  abstract = {MOTIVATION: Many practical pattern recognition problems require non-negativity constraints. For example, pixels in digital images and chemical concentrations in bioinformatics are non-negative. Sparse non-negative matrix factorizations (NMFs) are useful when the degree of sparseness in the non-negative basis matrix or the non-negative coefficient matrix in an NMF needs to be controlled in approximating high-dimensional data in a lower dimensional space.
RESULTS: In this article, we introduce a novel formulation of sparse NMF and show how the new formulation leads to a convergent sparse NMF algorithm via alternating non-negativity-constrained least squares. We apply our sparse NMF algorithm to cancer-class discovery and gene expression data analysis and offer biological analysis of the results obtained. Our experimental results illustrate that the proposed sparse NMF algorithm often achieves better clustering performance with shorter computing time compared to other existing NMF algorithms.
AVAILABILITY: The software is available as supplementary material.},
  number = {12},
  journaltitle = {Bioinformatics (Oxford, England)},
  shortjournal = {Bioinformatics},
  date = {2007-06-15},
  pages = {1495-1502},
  keywords = {Algorithms,Humans,Pattern Recognition; Automated,Entropy,Cluster Analysis,Computational Biology,Data Interpretation; Statistical,Databases; Genetic,Factor Analysis; Statistical,Gene Expression,Least-Squares Analysis,Microarray Analysis,Neoplasms},
  author = {Kim, Hyunsoo and Park, Haesun},
  eprinttype = {pmid},
  eprint = {17483501}
}

@article{poldrack_phenome-wide_2016,
  title = {A Phenome-Wide Examination of Neural and Cognitive Function},
  volume = {3},
  url = {http://dx.doi.org/10.1038/sdata.2016.110},
  journaltitle = {Scientific Data},
  shortjournal = {Scientific Data},
  date = {2016-12-06},
  pages = {160110},
  author = {Poldrack, R.A. and Congdon, E. and Triplett, W. and Gorgolewski, K.J. and Karlsgodt, K.H. and Mumford, J.A. and Sabb, F.W. and Freimer, N.B. and London, E.D. and Cannon, T.D. and Bilder, R.M.}
}

@article{mensch_learning_nodate,
  langid = {english},
  title = {Learning Representations from Functional {{MRI}} Data},
  pages = {183},
  author = {Mensch, Arthur},
  file = {/home/arthur/Zotero/storage/3U325DUX/Mensch - Learning representations from functional MRI data.pdf}
}

@incollection{silver_monte-carlo_2010,
  title = {Monte-{{Carlo Planning}} in {{Large POMDPs}}},
  url = {http://papers.nips.cc/paper/4031-monte-carlo-planning-in-large-pomdps.pdf},
  booktitle = {Advances in {{Neural Information Processing Systems}} 23},
  publisher = {{Curran Associates, Inc.}},
  urldate = {2018-10-25},
  date = {2010},
  pages = {2164--2172},
  author = {Silver, David and Veness, Joel},
  editor = {Lafferty, J. D. and Williams, C. K. I. and Shawe-Taylor, J. and Zemel, R. S. and Culotta, A.},
  file = {/home/arthur/Dropbox/Zotero/Silver_Veness_2010_Monte-Carlo Planning in Large POMDPs.pdf;/home/arthur/Zotero/storage/KINBRX24/4031-monte-carlo-planning-in-large-pomdps.html}
}

@article{blondel_learning_2018-1,
  langid = {english},
  title = {Learning {{Classifiers}} with {{Fenchel}}-{{Young Losses}}: {{Generalized Entropies}}, {{Margins}}, and {{Algorithms}}},
  url = {https://arxiv.org/abs/1805.09717},
  shorttitle = {Learning {{Classifiers}} with {{Fenchel}}-{{Young Losses}}},
  urldate = {2018-10-29},
  date = {2018-05-24},
  author = {Blondel, Mathieu and Martins, André F. T. and Niculae, Vlad},
  file = {/home/arthur/Dropbox/Zotero/Blondel et al_2018_Learning Classifiers with Fenchel-Young Losses2.pdf;/home/arthur/Zotero/storage/CSUFPLGE/1805.html}
}

@article{niculae_towards_2018,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1809.00653},
  primaryClass = {cs, stat},
  title = {Towards {{Dynamic Computation Graphs}} via {{Sparse Latent Structure}}},
  url = {http://arxiv.org/abs/1809.00653},
  abstract = {Deep NLP models benefit from underlying structures in the data---e.g., parse trees---typically extracted using off-the-shelf parsers. Recent attempts to jointly learn the latent structure encounter a tradeoff: either make factorization assumptions that limit expressiveness, or sacrifice end-to-end differentiability. Using the recently proposed SparseMAP inference, which retrieves a sparse distribution over latent structures, we propose a novel approach for end-to-end learning of latent structure predictors jointly with a downstream predictor. To the best of our knowledge, our method is the first to enable unrestricted dynamic computation graph construction from the global latent structure, while maintaining differentiability.},
  urldate = {2018-10-29},
  date = {2018-09-03},
  keywords = {Statistics - Machine Learning,Computer Science - Computation and Language,I.2.6,68T50,I.2.7,Computer Science - Machine Learning},
  author = {Niculae, Vlad and Martins, André F. T. and Cardie, Claire},
  file = {/home/arthur/Dropbox/Zotero/Niculae et al_2018_Towards Dynamic Computation Graphs via Sparse Latent Structure.pdf;/home/arthur/Zotero/storage/9IWUNGCP/1809.html}
}

@article{peyre_entropic_2015,
  langid = {english},
  title = {Entropic {{Wasserstein Gradient Flows}}},
  url = {https://arxiv.org/abs/1502.06216},
  urldate = {2018-10-29},
  date = {2015-02-22},
  author = {Peyré, Gabriel},
  file = {/home/arthur/Dropbox/Zotero/Peyré_2015_Entropic Wasserstein Gradient Flows.pdf;/home/arthur/Zotero/storage/Q9K8KYW4/1502.html}
}

@article{santambrogio__2016,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1609.03890},
  primaryClass = {math},
  title = {\{ \vphantom\}{{Euclidean}}, {{Metric}}, and {{Wasserstein}} \vphantom\{\} {{Gradient Flows}}: An Overview},
  url = {http://arxiv.org/abs/1609.03890},
  shorttitle = {\{ \vphantom\}{{Euclidean}}, {{Metric}}, and {{Wasserstein}} \vphantom\{\} {{Gradient Flows}}},
  abstract = {This is an expository paper on the theory of gradient flows, and in particular of those PDEs which can be interpreted as gradient flows for the Wasserstein metric on the space of probability measures (a distance induced by optimal transport). The starting point is the Euclidean theory, and then its generalization to metric spaces, according to the work of Ambrosio, Gigli and Savar\{$\backslash$'e\}. Then comes an independent exposition of the Wasserstein theory, with a short introduction to the optimal transport tools that are needed and to the notion of geodesic convexity, followed by a precise desciption of the Jordan-Kinderleher-Otto scheme, with proof of convergence in the easiest case: the linear Fokker-Planck equation. A discussion of other gradient flows PDEs and of numerical methods based on these ideas is also provided. The paper ends with a new, theoretical, development, due to Ambrosio, Gigli, Savar\{$\backslash$'e\}, Kuwada and Ohta: the study of the heat flow in metric measure spaces.},
  urldate = {2018-10-29},
  date = {2016-09-13},
  keywords = {Mathematics - Dynamical Systems,Mathematics - Analysis of PDEs},
  author = {Santambrogio, Filippo},
  file = {/home/arthur/Dropbox/Zotero/Santambrogio_2016_ Euclidean, Metric, and Wasserstein Gradient Flows.pdf;/home/arthur/Zotero/storage/2YRZLTCS/1609.html}
}

@article{santambrogio_optimal_2015,
  langid = {english},
  title = {Optimal {{Transport}} for {{Applied Mathematicians}} – {{Calculus}} of {{Variations}}, {{PDEs}} and {{Modeling}}},
  date = {2015},
  pages = {356},
  author = {Santambrogio, Filippo},
  file = {/home/arthur/Zotero/storage/FJCAG9X5/Santambrogio - Optimal Transport for Applied Mathematicians – Cal.pdf}
}

@article{genevay_sample_2018,
  langid = {english},
  title = {Sample {{Complexity}} of {{Sinkhorn}} Divergences},
  url = {https://arxiv.org/abs/1810.02733},
  urldate = {2018-10-30},
  date = {2018-10-05},
  author = {Genevay, Aude and Chizat, Lénaic and Bach, Francis and Cuturi, Marco and Peyré, Gabriel},
  file = {/home/arthur/Dropbox/Zotero/Genevay et al_2018_Sample Complexity of Sinkhorn divergences.pdf;/home/arthur/Zotero/storage/9RNYSY4H/1810.html}
}

@article{bornemann_finite-element_2004,
  langid = {english},
  title = {Finite-{{Element Discretization}} of {{Static Hamilton}}-{{Jacobi Equations Based}} on a {{Local Variational Principle}}},
  url = {https://arxiv.org/abs/math/0403517},
  urldate = {2018-10-30},
  date = {2004-03-30},
  author = {Bornemann, Folkmar and Rasch, Christian},
  file = {/home/arthur/Dropbox/Zotero/Bornemann_Rasch_2004_Finite-Element Discretization of Static Hamilton-Jacobi Equations Based on a.pdf;/home/arthur/Zotero/storage/YXLAHVY9/0403517.html}
}

@article{mirebeau_automatic_2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1704.03782},
  primaryClass = {math},
  langid = {english},
  title = {Automatic Differentiation of Non-Holonomic Fast Marching for Computing Most Threatening Trajectories under Sensors Surveillance},
  url = {http://arxiv.org/abs/1704.03782},
  abstract = {We consider a two player game, where a ﬁrst player has to install a surveillance system within an admissible region. The second player needs to enter the the monitored area, visit a target region, and then leave the area, while minimizing his overall probability of detection. Both players know the target region, and the second player knows the surveillance installation details.},
  urldate = {2018-10-30},
  date = {2017-04-12},
  keywords = {Mathematics - Optimization and Control},
  author = {Mirebeau, Jean-Marie and Dreo, Johann},
  file = {/home/arthur/Zotero/storage/FEBRC2RJ/Mirebeau and Dreo - 2017 - Automatic differentiation of non-holonomic fast ma.pdf}
}

@article{kingma_semi-supervised_2014,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1406.5298},
  primaryClass = {cs, stat},
  langid = {english},
  title = {Semi-{{Supervised Learning}} with {{Deep Generative Models}}},
  url = {http://arxiv.org/abs/1406.5298},
  abstract = {The ever-increasing size of modern data sets combined with the difﬁculty of obtaining label information has made semi-supervised learning one of the problems of signiﬁcant practical importance in modern data analysis. We revisit the approach to semi-supervised learning with generative models and develop new models that allow for effective generalisation from small labelled data sets to large unlabelled ones. Generative approaches have thus far been either inﬂexible, inefﬁcient or non-scalable. We show that deep generative models and approximate Bayesian inference exploiting recent advances in variational methods can be used to provide signiﬁcant improvements, making generative approaches highly competitive for semi-supervised learning.},
  urldate = {2018-10-31},
  date = {2014-06-20},
  keywords = {Statistics - Machine Learning,Computer Science - Machine Learning},
  author = {Kingma, Diederik P. and Rezende, Danilo J. and Mohamed, Shakir and Welling, Max},
  file = {/home/arthur/Zotero/storage/2H57QXWL/Kingma et al. - 2014 - Semi-Supervised Learning with Deep Generative Mode.pdf}
}

@article{peyre_geodesic_2010,
  title = {Geodesic {{Methods}} in {{Computer Vision}} and {{Graphics}}},
  volume = {5},
  url = {https://hal.archives-ouvertes.fr/hal-00528999},
  abstract = {This paper reviews both the theory and practice of the numerical computation of geodesic distances on Riemannian manifolds. The notion of Riemannian manifold allows one to define a local metric (a symmetric positive tensor field) that encodes the information about the problem one wishes to solve. This takes into account a local isotropic cost (whether some point should be avoided or not) and a local anisotropy (which direction should be preferred). Using this local tensor field, the geodesic distance is used to solve many problems of practical interest such as segmentation using geodesic balls and Voronoi regions, sampling points at regular geodesic distance or meshing a domain with geodesic Delaunay triangles. The shortest paths for this Riemannian distance, the so-called geodesics, are also important because they follow salient curvilinear structures in the domain. We show several applications of the numerical computation of geodesic distances and shortest paths to problems in surface and shape processing, in particular segmentation, sampling, meshing and comparison of shapes.},
  number = {3-4},
  journaltitle = {Foundations and Trends in Computer Graphics and Vision},
  urldate = {2018-10-31},
  date = {2010},
  pages = {197-397},
  keywords = {image segmentation,medical imaging,active contour,computer graphics,Computer vision,Fast Marching,geodesic distance,image,level set,mesh,remeshing,Riemannian manifold,sampling,shortest path},
  author = {Peyré, Gabriel and Péchaud, Mickaël and Keriven, Renaud and Cohen, Laurent D.},
  file = {/home/arthur/Dropbox/Zotero/Peyré et al_2010_Geodesic Methods in Computer Vision and Graphics.pdf}
}

@article{tsitsiklis_efficient_1995,
  title = {Efficient Algorithms for Globally Optimal Trajectories},
  volume = {40},
  number = {9},
  journaltitle = {IEEE Transactions on Automatic Control},
  date = {1995},
  pages = {1528--1538},
  author = {Tsitsiklis, John N.},
  file = {/home/arthur/Dropbox/Zotero/Tsitsiklis_1995_Efficient algorithms for globally optimal trajectories.pdf;/home/arthur/Zotero/storage/LPUD2APL/412624.html}
}

@article{lee_revisiting_2017,
  langid = {english},
  title = {Revisiting the Redistancing Problem Using the {{Hopf}}–{{Lax}} Formula},
  volume = {330},
  issn = {00219991},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0021999116305903},
  abstract = {This article presents a fast new numerical method for redistancing objective functions based on the Hopf–Lax formula [1]. The algorithm suggested here is a special case of the previous work in [2] and an extension that applies the Hopf–Lax formula for computing the signed distance to the front. We propose the split Bregman approach to solve the minimization problem as a solution of the eikonal equation obtained from Hopf–Lax formula. Our redistancing procedure is expected to be generalized and widely applied to many ﬁelds such as computational ﬂuid dynamics, the minimal surface problem, and elsewhere.},
  journaltitle = {Journal of Computational Physics},
  urldate = {2018-11-05},
  date = {2017-02},
  pages = {268-281},
  author = {Lee, Byungjoon and Darbon, Jérôme and Osher, Stanley and Kang, Myungjoo},
  file = {/home/arthur/Zotero/storage/2Z2ZKE8A/Lee et al. - 2017 - Revisiting the redistancing problem using the Hopf.pdf}
}

@article{lee_revisiting_2017-1,
  langid = {english},
  title = {Revisiting the Redistancing Problem Using the {{Hopf}}–{{Lax}} Formula},
  volume = {330},
  issn = {00219991},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0021999116305903},
  abstract = {This article presents a fast new numerical method for redistancing objective functions based on the Hopf–Lax formula [1]. The algorithm suggested here is a special case of the previous work in [2] and an extension that applies the Hopf–Lax formula for computing the signed distance to the front. We propose the split Bregman approach to solve the minimization problem as a solution of the eikonal equation obtained from Hopf–Lax formula. Our redistancing procedure is expected to be generalized and widely applied to many ﬁelds such as computational ﬂuid dynamics, the minimal surface problem, and elsewhere.},
  journaltitle = {Journal of Computational Physics},
  urldate = {2018-11-05},
  date = {2017-02},
  pages = {268-281},
  author = {Lee, Byungjoon and Darbon, Jérôme and Osher, Stanley and Kang, Myungjoo},
  file = {/home/arthur/Zotero/storage/C22TZZ5Q/Lee et al. - 2017 - Revisiting the redistancing problem using the Hopf.pdf}
}

@article{chaudhari_deep_2017,
  langid = {english},
  title = {Deep {{Relaxation}}: Partial Differential Equations for Optimizing Deep Neural Networks},
  url = {https://arxiv.org/abs/1704.04932},
  shorttitle = {Deep {{Relaxation}}},
  urldate = {2018-11-05},
  date = {2017-04-17},
  author = {Chaudhari, Pratik and Oberman, Adam and Osher, Stanley and Soatto, Stefano and Carlier, Guillaume},
  file = {/home/arthur/Dropbox/Zotero/Chaudhari et al_2017_Deep Relaxation.pdf;/home/arthur/Zotero/storage/6LQRCLPC/1704.html}
}

@article{sirignano_dgm_2018,
  title = {{{DGM}}: {{A}} Deep Learning Algorithm for Solving Partial Differential Equations},
  volume = {375},
  issn = {0021-9991},
  url = {http://www.sciencedirect.com/science/article/pii/S0021999118305527},
  shorttitle = {{{DGM}}},
  abstract = {High-dimensional PDEs have been a longstanding computational challenge. We propose to solve high-dimensional PDEs by approximating the solution with a deep neural network which is trained to satisfy the differential operator, initial condition, and boundary conditions. Our algorithm is meshfree, which is key since meshes become infeasible in higher dimensions. Instead of forming a mesh, the neural network is trained on batches of randomly sampled time and space points. The algorithm is tested on a class of high-dimensional free boundary PDEs, which we are able to accurately solve in up to 200 dimensions. The algorithm is also tested on a high-dimensional Hamilton–Jacobi–Bellman PDE and Burgers' equation. The deep learning algorithm approximates the general solution to the Burgers' equation for a continuum of different boundary conditions and physical conditions (which can be viewed as a high-dimensional space). We call the algorithm a “Deep Galerkin Method (DGM)” since it is similar in spirit to Galerkin methods, with the solution approximated by a neural network instead of a linear combination of basis functions. In addition, we prove a theorem regarding the approximation power of neural networks for a class of quasilinear parabolic PDEs.},
  journaltitle = {Journal of Computational Physics},
  shortjournal = {Journal of Computational Physics},
  urldate = {2018-11-05},
  date = {2018-12-15},
  pages = {1339-1364},
  keywords = {Machine learning,Deep learning,High-dimensional partial differential equations,Partial differential equations},
  author = {Sirignano, Justin and Spiliopoulos, Konstantinos},
  file = {/home/arthur/Dropbox/Zotero/Sirignano_Spiliopoulos_2018_DGM.pdf;/home/arthur/Zotero/storage/YGR7KUD6/S0021999118305527.html}
}

@article{chaudhari_entropy-sgd_2016,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1611.01838},
  primaryClass = {cs, stat},
  langid = {english},
  title = {Entropy-{{SGD}}: {{Biasing Gradient Descent Into Wide Valleys}}},
  url = {http://arxiv.org/abs/1611.01838},
  shorttitle = {Entropy-{{SGD}}},
  abstract = {This paper proposes a new optimization algorithm called Entropy-SGD for training deep neural networks that is motivated by the local geometry of the energy landscape. Local extrema with low generalization error have a large proportion of almost-zero eigenvalues in the Hessian with very few positive or negative eigenvalues. We leverage upon this observation to construct a local-entropy-based objective function that favors well-generalizable solutions lying in large ﬂat regions of the energy landscape, while avoiding poorly-generalizable solutions located in the sharp valleys. Conceptually, our algorithm resembles two nested loops of SGD where we use Langevin dynamics in the inner loop to compute the gradient of the local entropy before each update of the weights. We show that the new objective has a smoother energy landscape and show improved generalization over SGD using uniform stability, under certain assumptions. Our experiments on convolutional and recurrent networks demonstrate that Entropy-SGD compares favorably to state-of-the-art techniques in terms of generalization error and training time.},
  urldate = {2018-11-05},
  date = {2016-11-06},
  keywords = {Statistics - Machine Learning,Computer Science - Machine Learning},
  author = {Chaudhari, Pratik and Choromanska, Anna and Soatto, Stefano and LeCun, Yann and Baldassi, Carlo and Borgs, Christian and Chayes, Jennifer and Sagun, Levent and Zecchina, Riccardo},
  file = {/home/arthur/Zotero/storage/HN6U7CWG/Chaudhari et al. - 2016 - Entropy-SGD Biasing Gradient Descent Into Wide Va.pdf}
}

@article{osokin_structured_2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1703.02403},
  primaryClass = {cs, stat},
  langid = {english},
  title = {On {{Structured Prediction Theory}} with {{Calibrated Convex Surrogate Losses}}},
  url = {http://arxiv.org/abs/1703.02403},
  abstract = {We provide novel theoretical insights on structured prediction in the context of efﬁcient convex surrogate loss minimization with consistency guarantees. For any task loss, we construct a convex surrogate that can be optimized via stochastic gradient descent and we prove tight bounds on the so-called “calibration function” relating the excess surrogate risk to the actual risk. In contrast to prior related work, we carefully monitor the effect of the exponential number of classes in the learning guarantees as well as on the optimization complexity. As an interesting consequence, we formalize the intuition that some task losses make learning harder than others, and that the classical 0-1 loss is ill-suited for structured prediction.},
  urldate = {2018-11-06},
  date = {2017-03-07},
  keywords = {Statistics - Machine Learning,Computer Science - Machine Learning},
  author = {Osokin, Anton and Bach, Francis and Lacoste-Julien, Simon},
  file = {/home/arthur/Zotero/storage/U729KRDL/Osokin et al. - 2017 - On Structured Prediction Theory with Calibrated Co.pdf}
}

@article{gao_consistency_2013,
  langid = {english},
  title = {On the Consistency of Multi-Label Learning},
  volume = {199-200},
  issn = {00043702},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0004370213000313},
  abstract = {Multi-label learning has attracted much attention during the past few years. Many multilabel learning approaches have been developed, mostly working with surrogate loss functions since multi-label loss functions are usually diﬃcult to optimize directly owing to non-convexity and discontinuity. Though these approaches are eﬀective, to the best of our knowledge, there is no theoretical result on the convergence of risk of the learned functions to the Bayes risk. In this paper, focusing on two well-known multi-label loss functions, i.e., ranking loss and hamming loss, we prove a necessary and suﬃcient condition for the consistency of multi-label learning based on surrogate loss functions. Our results disclose that, surprisingly, none convex surrogate loss is consistent with the ranking loss. Inspired by the ﬁnding, we introduce the partial ranking loss, with which some surrogate functions are consistent. For hamming loss, we show that some recent multi-label learning approaches are inconsistent even for deterministic multi-label classiﬁcation, and give a surrogate loss function which is consistent for the deterministic case. Finally, we discuss on the consistency of learning approaches which address multi-label learning by decomposing into a set of binary classiﬁcation problems.},
  journaltitle = {Artificial Intelligence},
  urldate = {2018-11-06},
  date = {2013-06},
  pages = {22-44},
  author = {Gao, Wei and Zhou, Zhi-Hua},
  file = {/home/arthur/Zotero/storage/FMH8938N/Gao and Zhou - 2013 - On the consistency of multi-label learning.pdf}
}

@incollection{tewari_consistency_2005,
  langid = {english},
  location = {{Berlin, Heidelberg}},
  title = {On the {{Consistency}} of {{Multiclass Classification Methods}}},
  volume = {3559},
  isbn = {978-3-540-26556-6 978-3-540-31892-7},
  url = {http://link.springer.com/10.1007/11503415_10},
  abstract = {Binary classiﬁcation is a well studied special case of the classiﬁcation problem. Statistical properties of binary classiﬁers, such as consistency, have been investigated in a variety of settings. Binary classiﬁcation methods can be generalized in many ways to handle multiple classes. It turns out that one can lose consistency in generalizing a binary classiﬁcation method to deal with multiple classes. We study a rich family of multiclass methods and provide a necessary and sufﬁcient condition for their consistency. We illustrate our approach by applying it to some multiclass methods proposed in the literature.},
  booktitle = {Learning {{Theory}}},
  publisher = {{Springer Berlin Heidelberg}},
  urldate = {2018-11-06},
  date = {2005},
  pages = {143-157},
  author = {Tewari, Ambuj and Bartlett, Peter L.},
  file = {/home/arthur/Zotero/storage/TENJNYNS/Tewari and Bartlett - 2005 - On the Consistency of Multiclass Classification Me.pdf},
  doi = {10.1007/11503415_10}
}

@article{koyejo_consistent_2015,
  langid = {english},
  title = {Consistent {{Multilabel Classification}}},
  abstract = {Multilabel classiﬁcation is rapidly developing as an important aspect of modern predictive modeling, motivating study of its theoretical aspects. To this end, we propose a framework for constructing and analyzing multilabel classiﬁcation metrics which reveals novel results on a parametric form for population optimal classiﬁers, and additional insight into the role of label correlations. In particular, we show that for multilabel metrics constructed as instance-, micro- and macroaverages, the population optimal classiﬁer can be decomposed into binary classiﬁers based on the marginal instance-conditional distribution of each label, with a weak association between labels via the threshold. Thus, our analysis extends the state of the art from a few known multilabel classiﬁcation metrics such as Hamming loss, to a general framework applicable to many of the classiﬁcation metrics in common use. Based on the population-optimal classiﬁer, we propose a computationally efﬁcient and general-purpose plug-in classiﬁcation algorithm, and prove its consistency with respect to the metric of interest. Empirical results on synthetic and benchmark datasets are supportive of our theoretical ﬁndings.},
  date = {2015},
  pages = {9},
  author = {Koyejo, Sanmi and Natarajan, Nagarajan and Ravikumar, Pradeep K and Dhillon, Inderjit S},
  file = {/home/arthur/Zotero/storage/BF6E3LT7/Koyejo et al. - Consistent Multilabel Classification.pdf}
}

@article{lapin_analysis_2018,
  title = {Analysis and {{Optimization}} of {{Loss Functions}} for {{Multiclass}}, {{Top}}-k, and {{Multilabel Classification}}},
  volume = {40},
  issn = {0162-8828, 2160-9292},
  url = {https://ieeexplore.ieee.org/document/8036272/},
  number = {7},
  journaltitle = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  urldate = {2018-11-06},
  date = {2018-07-01},
  pages = {1533-1554},
  author = {Lapin, Maksim and Hein, Matthias and Schiele, Bernt},
  file = {/home/arthur/Dropbox/Zotero/Lapin et al_2018_Analysis and Optimization of Loss Functions for Multiclass, Top-k, and.pdf;/home/arthur/Zotero/storage/KGVMU4AM/lapin2017.pdf}
}

@article{pedregosa_consistency_2017,
  langid = {english},
  title = {On the {{Consistency}} of {{Ordinal Regression Methods}}},
  abstract = {Many of the ordinal regression models that have been proposed in the literature can be seen as methods that minimize a convex surrogate of the zero-one, absolute, or squared loss functions. A key property that allows to study the statistical implications of such approximations is that of Fisher consistency. Fisher consistency is a desirable property for surrogate loss functions and implies that in the population setting, i.e., if the probability distribution that generates the data were available, then optimization of the surrogate would yield the best possible model. In this paper we will characterize the Fisher consistency of a rich family of surrogate loss functions used in the context of ordinal regression, including support vector ordinal regression, ORBoosting and least absolute deviation. We will see that, for a family of surrogate loss functions that subsumes support vector ordinal regression and ORBoosting, consistency can be fully characterized by the derivative of a real-valued function at zero, as happens for convex margin-based surrogates in binary classiﬁcation. We also derive excess risk bounds for a surrogate of the absolute error that generalize existing risk bounds for binary classiﬁcation. Finally, our analysis suggests a novel surrogate of the squared error loss. We compare this novel surrogate with competing approaches on 9 diﬀerent datasets. Our method shows to be highly competitive in practice, outperforming the least squares loss on 7 out of 9 datasets.},
  date = {2017},
  pages = {35},
  author = {Pedregosa, Fabian and Bach, Francis and Gramfort, Alexandre},
  file = {/home/arthur/Zotero/storage/7PMVRNZR/Pedregosa et al. - On the Consistency of Ordinal Regression Methods.pdf}
}

@article{laha_controllable_2018,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1810.11975},
  primaryClass = {cs, stat},
  langid = {english},
  title = {On {{Controllable Sparse Alternatives}} to {{Softmax}}},
  url = {http://arxiv.org/abs/1810.11975},
  abstract = {Converting an n-dimensional vector to a probability distribution over n objects is a commonly used component in many machine learning tasks like multiclass classiﬁcation, multilabel classiﬁcation, attention mechanisms etc. For this, several probability mapping functions have been proposed and employed in literature such as softmax, sum-normalization, spherical softmax, and sparsemax, but there is very little understanding in terms how they relate with each other. Further, none of the above formulations offer an explicit control over the degree of sparsity. To address this, we develop a uniﬁed framework that encompasses all these formulations as special cases. This framework ensures simple closed-form solutions and existence of sub-gradients suitable for learning via backpropagation. Within this framework, we propose two novel sparse formulations, sparsegen-lin and sparsehourglass, that seek to provide a control over the degree of desired sparsity. We further develop novel convex loss functions that help induce the behavior of aforementioned formulations in the multilabel classiﬁcation setting, showing improved performance. We also demonstrate empirically that the proposed formulations, when used to compute attention weights, achieve better or comparable performance on standard seq2seq tasks like neural machine translation and abstractive summarization.},
  urldate = {2018-11-06},
  date = {2018-10-29},
  keywords = {Statistics - Machine Learning,Computer Science - Computation and Language,Computer Science - Machine Learning},
  author = {Laha, Anirban and Chemmengath, Saneem A. and Agrawal, Priyanka and Khapra, Mitesh M. and Sankaranarayanan, Karthik and Ramaswamy, Harish G.},
  file = {/home/arthur/Zotero/storage/DSQJQX6X/Laha et al. - 2018 - On Controllable Sparse Alternatives to Softmax.pdf}
}

@article{dembczynski_label_2012,
  langid = {english},
  title = {On Label Dependence and Loss Minimization in Multi-Label Classification},
  volume = {88},
  issn = {1573-0565},
  url = {https://doi.org/10.1007/s10994-012-5285-8},
  abstract = {Most of the multi-label classification (MLC) methods proposed in recent years intended to exploit, in one way or the other, dependencies between the class labels. Comparing to simple binary relevance learning as a baseline, any gain in performance is normally explained by the fact that this method is ignoring such dependencies. Without questioning the correctness of such studies, one has to admit that a blanket explanation of that kind is hiding many subtle details, and indeed, the underlying mechanisms and true reasons for the improvements reported in experimental studies are rarely laid bare. Rather than proposing yet another MLC algorithm, the aim of this paper is to elaborate more closely on the idea of exploiting label dependence, thereby contributing to a better understanding of MLC. Adopting a statistical perspective, we claim that two types of label dependence should be distinguished, namely conditional and marginal dependence. Subsequently, we present three scenarios in which the exploitation of one of these types of dependence may boost the predictive performance of a classifier. In this regard, a close connection with loss minimization is established, showing that the benefit of exploiting label dependence does also depend on the type of loss to be minimized. Concrete theoretical results are presented for two representative loss functions, namely the Hamming loss and the subset 0/1 loss. In addition, we give an overview of state-of-the-art decomposition algorithms for MLC and we try to reveal the reasons for their effectiveness. Our conclusions are supported by carefully designed experiments on synthetic and benchmark data.},
  number = {1},
  journaltitle = {Machine Learning},
  shortjournal = {Mach Learn},
  urldate = {2018-11-06},
  date = {2012-07-01},
  pages = {5-45},
  keywords = {Label dependence,Loss functions,Multi-label classification},
  author = {Dembczyński, Krzysztof and Waegeman, Willem and Cheng, Weiwei and Hüllermeier, Eyke},
  file = {/home/arthur/Dropbox/Zotero/Dembczyński et al_2012_On label dependence and loss minimization in multi-label classification.pdf}
}

@article{liu_darts_2018,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1806.09055},
  primaryClass = {cs, stat},
  title = {{{DARTS}}: {{Differentiable Architecture Search}}},
  url = {http://arxiv.org/abs/1806.09055},
  shorttitle = {{{DARTS}}},
  abstract = {This paper addresses the scalability challenge of architecture search by formulating the task in a differentiable manner. Unlike conventional approaches of applying evolution or reinforcement learning over a discrete and non-differentiable search space, our method is based on the continuous relaxation of the architecture representation, allowing efficient search of the architecture using gradient descent. Extensive experiments on CIFAR-10, ImageNet, Penn Treebank and WikiText-2 show that our algorithm excels in discovering high-performance convolutional architectures for image classification and recurrent architectures for language modeling, while being orders of magnitude faster than state-of-the-art non-differentiable techniques.},
  urldate = {2018-11-06},
  date = {2018-06-23},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Statistics - Machine Learning,Computer Science - Computation and Language,Computer Science - Machine Learning},
  author = {Liu, Hanxiao and Simonyan, Karen and Yang, Yiming},
  file = {/home/arthur/Dropbox/Zotero/Liu et al_2018_DARTS.pdf;/home/arthur/Zotero/storage/SVI5ZNPT/1806.html}
}

@article{rozsa_adversarial_2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1708.01697},
  primaryClass = {cs},
  title = {Adversarial {{Robustness}}: {{Softmax}} versus {{Openmax}}},
  url = {http://arxiv.org/abs/1708.01697},
  shorttitle = {Adversarial {{Robustness}}},
  abstract = {Deep neural networks (DNNs) provide state-of-the-art results on various tasks and are widely used in real world applications. However, it was discovered that machine learning models, including the best performing DNNs, suffer from a fundamental problem: they can unexpectedly and confidently misclassify examples formed by slightly perturbing otherwise correctly recognized inputs. Various approaches have been developed for efficiently generating these so-called adversarial examples, but those mostly rely on ascending the gradient of loss. In this paper, we introduce the novel logits optimized targeting system (LOTS) to directly manipulate deep features captured at the penultimate layer. Using LOTS, we analyze and compare the adversarial robustness of DNNs using the traditional Softmax layer with Openmax, which was designed to provide open set recognition by defining classes derived from deep representations, and is claimed to be more robust to adversarial perturbations. We demonstrate that Openmax provides less vulnerable systems than Softmax to traditional attacks, however, we show that it can be equally susceptible to more sophisticated adversarial generation techniques that directly work on deep representations.},
  urldate = {2018-11-06},
  date = {2017-08-04},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  author = {Rozsa, Andras and Günther, Manuel and Boult, Terrance E.},
  file = {/home/arthur/Dropbox/Zotero/Rozsa et al_2017_Adversarial Robustness.pdf;/home/arthur/Zotero/storage/N255LYAU/1708.html}
}

@article{elsayed_large_2018,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1803.05598},
  primaryClass = {cs, stat},
  langid = {english},
  title = {Large {{Margin Deep Networks}} for {{Classification}}},
  url = {http://arxiv.org/abs/1803.05598},
  abstract = {We present a formulation of deep learning that aims at producing a large margin classiﬁer. The notion of margin, minimum distance to a decision boundary, has served as the foundation of several theoretically profound and empirically successful results for both classiﬁcation and regression tasks. However, most large margin algorithms are applicable only to shallow models with a preset feature representation; and conventional margin methods for neural networks only enforce margin at the output layer. Such methods are therefore not well suited for deep networks. In this work, we propose a novel loss function to impose a margin on any chosen set of layers of a deep network (including input and hidden layers). Our formulation allows choosing any lp norm (p ≥ 1) on the metric measuring the margin. We demonstrate that the decision boundary obtained by our loss has nice properties compared to standard classiﬁcation loss functions. Speciﬁcally, we show improved empirical results on the MNIST, CIFAR-10 and ImageNet datasets on multiple tasks: generalization from small training sets, corrupted labels, and robustness against adversarial perturbations. The resulting loss is general and complementary to existing data augmentation (such as random/adversarial input transform) and regularization techniques (such as weight decay, dropout, and batch norm).},
  urldate = {2018-11-06},
  date = {2018-03-15},
  keywords = {Statistics - Machine Learning,Computer Science - Machine Learning},
  author = {Elsayed, Gamaleldin F. and Krishnan, Dilip and Mobahi, Hossein and Regan, Kevin and Bengio, Samy},
  file = {/home/arthur/Zotero/storage/CYNXMTYD/Elsayed et al. - 2018 - Large Margin Deep Networks for Classification.pdf}
}

@article{bousquet_stability_2002,
  langid = {english},
  title = {Stability and {{Generalization}}},
  abstract = {We deﬁne notions of stability for learning algorithms and show how to use these notions to derive generalization error bounds based on the empirical error and the leave-one-out error. The methods we use can be applied in the regression framework as well as in the classiﬁcation one when the classiﬁer is obtained by thresholding a real-valued function. We study the stability properties of large classes of learning algorithms such as regularization based algorithms. In particular we focus on Hilbert space regularization and Kullback-Leibler regularization. We demonstrate how to apply the results to SVM for regression and classiﬁcation.},
  date = {2002},
  pages = {28},
  author = {Bousquet, Olivier and Elisseeﬀ, Andre},
  file = {/home/arthur/Zotero/storage/PNBHUVUA/Bousquet and Elisseeﬀ - Stability and Generalization.pdf}
}

@article{elsayed_large_2018-1,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1803.05598},
  primaryClass = {cs, stat},
  langid = {english},
  title = {Large {{Margin Deep Networks}} for {{Classification}}},
  url = {http://arxiv.org/abs/1803.05598},
  abstract = {We present a formulation of deep learning that aims at producing a large margin classiﬁer. The notion of margin, minimum distance to a decision boundary, has served as the foundation of several theoretically profound and empirically successful results for both classiﬁcation and regression tasks. However, most large margin algorithms are applicable only to shallow models with a preset feature representation; and conventional margin methods for neural networks only enforce margin at the output layer. Such methods are therefore not well suited for deep networks. In this work, we propose a novel loss function to impose a margin on any chosen set of layers of a deep network (including input and hidden layers). Our formulation allows choosing any lp norm (p ≥ 1) on the metric measuring the margin. We demonstrate that the decision boundary obtained by our loss has nice properties compared to standard classiﬁcation loss functions. Speciﬁcally, we show improved empirical results on the MNIST, CIFAR-10 and ImageNet datasets on multiple tasks: generalization from small training sets, corrupted labels, and robustness against adversarial perturbations. The resulting loss is general and complementary to existing data augmentation (such as random/adversarial input transform) and regularization techniques (such as weight decay, dropout, and batch norm).},
  urldate = {2018-11-06},
  date = {2018-03-15},
  keywords = {Statistics - Machine Learning,Computer Science - Machine Learning},
  author = {Elsayed, Gamaleldin F. and Krishnan, Dilip and Mobahi, Hossein and Regan, Kevin and Bengio, Samy},
  file = {/home/arthur/Zotero/storage/VJJYMH4J/Elsayed et al. - 2018 - Large Margin Deep Networks for Classification.pdf}
}

@article{sun_depth_2015,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1506.05232},
  primaryClass = {cs},
  title = {On the {{Depth}} of {{Deep Neural Networks}}: {{A Theoretical View}}},
  url = {http://arxiv.org/abs/1506.05232},
  shorttitle = {On the {{Depth}} of {{Deep Neural Networks}}},
  abstract = {People believe that depth plays an important role in success of deep neural networks (DNN). However, this belief lacks solid theoretical justifications as far as we know. We investigate role of depth from perspective of margin bound. In margin bound, expected error is upper bounded by empirical margin error plus Rademacher Average (RA) based capacity term. First, we derive an upper bound for RA of DNN, and show that it increases with increasing depth. This indicates negative impact of depth on test performance. Second, we show that deeper networks tend to have larger representation power (measured by Betti numbers based complexity) than shallower networks in multi-class setting, and thus can lead to smaller empirical margin error. This implies positive impact of depth. The combination of these two results shows that for DNN with restricted number of hidden units, increasing depth is not always good since there is a tradeoff between positive and negative impacts. These results inspire us to seek alternative ways to achieve positive impact of depth, e.g., imposing margin-based penalty terms to cross entropy loss so as to reduce empirical margin error without increasing depth. Our experiments show that in this way, we achieve significantly better test performance.},
  urldate = {2018-11-06},
  date = {2015-06-17},
  keywords = {Computer Science - Machine Learning},
  author = {Sun, Shizhao and Chen, Wei and Wang, Liwei and Liu, Xiaoguang and Liu, Tie-Yan},
  file = {/home/arthur/Dropbox/Zotero/Sun et al_2015_On the Depth of Deep Neural Networks.pdf;/home/arthur/Zotero/storage/9FCIEWD3/1506.html}
}

@article{lapin_top-k_2015,
  langid = {english},
  title = {Top-k {{Multiclass SVM}}},
  url = {https://arxiv.org/abs/1511.06683},
  urldate = {2018-11-07},
  date = {2015-11-20},
  author = {Lapin, Maksim and Hein, Matthias and Schiele, Bernt},
  file = {/home/arthur/Dropbox/Zotero/Lapin et al_2015_Top-k Multiclass SVM.pdf;/home/arthur/Zotero/storage/LVPQNBLM/1511.html}
}

@article{duchi_multiclass_2016,
  langid = {english},
  title = {Multiclass {{Classification}}, {{Information}}, {{Divergence}}, and {{Surrogate Risk}}},
  url = {https://arxiv.org/abs/1603.00126},
  urldate = {2018-11-07},
  date = {2016-03-01},
  author = {Duchi, John C. and Khosravi, Khashayar and Ruan, Feng},
  file = {/home/arthur/Dropbox/Zotero/Duchi et al_2016_Multiclass Classification, Information, Divergence, and Surrogate Risk.pdf;/home/arthur/Zotero/storage/GZNUMXYY/1603.html}
}

@article{boucheron_theory_2005,
  langid = {english},
  title = {Theory of {{Classification}}: A {{Survey}} of {{Some Recent Advances}}},
  volume = {9},
  issn = {1292-8100, 1262-3318},
  url = {http://www.esaim-ps.org/10.1051/ps:2005018},
  shorttitle = {Theory of {{Classification}}},
  abstract = {The last few years have witnessed important new developments in the theory and practice of pattern classiﬁcation. We intend to survey some of the main new ideas that have led to these recent results.},
  journaltitle = {ESAIM: Probability and Statistics},
  urldate = {2018-11-07},
  date = {2005-11},
  pages = {323-375},
  author = {Boucheron, Stéphane and Bousquet, Olivier and Lugosi, Gábor},
  file = {/home/arthur/Dropbox/Zotero/Boucheron et al_2005_Theory of Classification.pdf}
}

@article{ciliberto_consistent_2016,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1605.07588},
  primaryClass = {cs, stat},
  title = {A {{Consistent Regularization Approach}} for {{Structured Prediction}}},
  url = {http://arxiv.org/abs/1605.07588},
  abstract = {We propose and analyze a regularization approach for structured prediction problems. We characterize a large class of loss functions that allows to naturally embed structured outputs in a linear space. We exploit this fact to design learning algorithms using a surrogate loss approach and regularization techniques. We prove universal consistency and finite sample bounds characterizing the generalization properties of the proposed methods. Experimental results are provided to demonstrate the practical usefulness of the proposed approach.},
  urldate = {2018-11-07},
  date = {2016-05-24},
  keywords = {Statistics - Machine Learning,Computer Science - Machine Learning},
  author = {Ciliberto, Carlo and Rudi, Alessandro and Rosasco, Lorenzo},
  file = {/home/arthur/Dropbox/Zotero/Ciliberto et al_2016_A Consistent Regularization Approach for Structured Prediction.pdf;/home/arthur/Zotero/storage/QQ8E78QT/1605.html}
}

@article{xu_robustness_2008,
  langid = {english},
  title = {Robustness and {{Regularization}} of {{Support Vector Machines}}},
  url = {https://arxiv.org/abs/0803.3490},
  urldate = {2018-11-07},
  date = {2008-03-25},
  author = {Xu, Huan and Caramanis, Constantine and Mannor, Shie},
  file = {/home/arthur/Dropbox/Zotero/Xu et al_2008_Robustness and Regularization of Support Vector Machines.pdf;/home/arthur/Zotero/storage/L66SK59I/0803.html}
}

@article{bietti_regularization_2018,
  langid = {english},
  title = {On {{Regularization}} and {{Robustness}} of {{Deep Neural Networks}}},
  url = {https://arxiv.org/abs/1810.00363},
  urldate = {2018-11-07},
  date = {2018-09-30},
  author = {Bietti, Alberto and Mialon, Grégoire and Mairal, Julien},
  file = {/home/arthur/Dropbox/Zotero/Bietti et al_2018_On Regularization and Robustness of Deep Neural Networks.pdf;/home/arthur/Zotero/storage/2LP84A8Q/1810.html}
}

@article{nowak-vila_sharp_2018,
  langid = {english},
  title = {Sharp {{Analysis}} of {{Learning}} with {{Discrete Losses}}},
  url = {https://arxiv.org/abs/1810.06839},
  urldate = {2018-11-07},
  date = {2018-10-16},
  author = {Nowak-Vila, Alex and Bach, Francis and Rudi, Alessandro},
  file = {/home/arthur/Dropbox/Zotero/Nowak-Vila et al_2018_Sharp Analysis of Learning with Discrete Losses.pdf;/home/arthur/Zotero/storage/FG8LJZHV/1810.html}
}

@article{bartlett_convexity_2006,
  title = {Convexity, Classification, and Risk Bounds},
  volume = {101},
  number = {473},
  journaltitle = {Journal of the American Statistical Association},
  date = {2006},
  pages = {138--156},
  author = {Bartlett, Peter L. and Jordan, Michael I. and McAuliffe, Jon D.},
  file = {/home/arthur/Dropbox/Zotero/Bartlett et al_2006_Convexity, classification, and risk bounds.pdf;/home/arthur/Zotero/storage/R3PNBAQM/016214505000000907.html}
}

@inproceedings{quadrianto_estimating_2008,
  langid = {english},
  location = {{Helsinki, Finland}},
  title = {Estimating Labels from Label Proportions},
  isbn = {978-1-60558-205-4},
  url = {http://portal.acm.org/citation.cfm?doid=1390156.1390254},
  abstract = {Consider the following problem: given sets of unlabeled observations, each set with known label proportions, predict the labels of another set of observations, possibly with known label proportions. This problem occurs in areas like e-commerce, politics, spam ﬁltering and improper content detection. We present consistent estimators which can reconstruct the correct labels with high probability in a uniform convergence sense. Experiments show that our method works well in practice.},
  eventtitle = {The 25th International Conference},
  booktitle = {Proceedings of the 25th International Conference on {{Machine}} Learning - {{ICML}} '08},
  publisher = {{ACM Press}},
  urldate = {2018-11-07},
  date = {2008},
  pages = {776-783},
  author = {Quadrianto, Novi and Smola, Alex J. and Caetano, Tiberio S. and Le, Quoc V.},
  file = {/home/arthur/Zotero/storage/WVN9D7AH/Quadrianto et al. - 2008 - Estimating labels from label proportions.pdf}
}

@article{struminsky_quantifying_2018,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1810.11544},
  primaryClass = {cs, stat},
  langid = {english},
  title = {Quantifying {{Learning Guarantees}} for {{Convex}} but {{Inconsistent Surrogates}}},
  url = {http://arxiv.org/abs/1810.11544},
  abstract = {We study consistency properties of machine learning methods based on minimizing convex surrogates. We extend the recent framework of Osokin et al. [14] for the quantitative analysis of consistency properties to the case of inconsistent surrogates. Our key technical contribution consists in a new lower bound on the calibration function for the quadratic surrogate, which is non-trivial (not always zero) for inconsistent cases. The new bound allows to quantify the level of inconsistency of the setting and shows how learning with inconsistent surrogates can have guarantees on sample complexity and optimization difﬁculty. We apply our theory to two concrete cases: multi-class classiﬁcation with the tree-structured loss and ranking with the mean average precision loss. The results show the approximation-computation trade-offs caused by inconsistent surrogates and their potential beneﬁts.},
  urldate = {2018-11-07},
  date = {2018-10-26},
  keywords = {Statistics - Machine Learning,Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  author = {Struminsky, Kirill and Lacoste-Julien, Simon and Osokin, Anton},
  file = {/home/arthur/Zotero/storage/Y6RX26SF/Struminsky et al. - 2018 - Quantifying Learning Guarantees for Convex but Inc.pdf}
}

@article{zhang_statistical_2004,
  title = {Statistical Analysis of Some Multi-Category Large Margin Classification Methods},
  volume = {5},
  issue = {Oct},
  journaltitle = {Journal of Machine Learning Research},
  date = {2004},
  pages = {1225--1251},
  author = {Zhang, Tong},
  file = {/home/arthur/Dropbox/Zotero/Zhang_2004_Statistical analysis of some multi-category large margin classification methods.pdf;/home/arthur/Zotero/storage/2ME3Q289/zhang04b.html}
}

@article{lee_maximum_2018-1,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1805.08336},
  primaryClass = {cs, stat},
  langid = {english},
  title = {Maximum {{Causal Tsallis Entropy Imitation Learning}}},
  url = {http://arxiv.org/abs/1805.08336},
  abstract = {In this paper, we propose a novel maximum causal Tsallis entropy (MCTE) framework for imitation learning which can efﬁciently learn a sparse multi-modal policy distribution from demonstrations. We provide the full mathematical analysis of the proposed framework. First, the optimal solution of an MCTE problem is shown to be a sparsemax distribution, whose supporting set can be adjusted. The proposed method has advantages over a softmax distribution in that it can exclude unnecessary actions by assigning zero probability. Second, we prove that an MCTE problem is equivalent to robust Bayes estimation in the sense of the Brier score. Third, we propose a maximum causal Tsallis entropy imitation learning (MCTEIL) algorithm with a sparse mixture density network (sparse MDN) by modeling mixture weights using a sparsemax distribution. In particular, we show that the causal Tsallis entropy of an MDN encourages exploration and efﬁcient mixture utilization while Boltzmann Gibbs entropy is less effective. We validate the proposed method in two simulation studies and MCTEIL outperforms existing imitation learning methods in terms of average returns and learning multi-modal policies.},
  urldate = {2018-11-07},
  date = {2018-05-21},
  keywords = {Statistics - Machine Learning,Computer Science - Machine Learning},
  author = {Lee, Kyungjae and Choi, Sungjoon and Oh, Songhwai},
  file = {/home/arthur/Zotero/storage/JNDFWCPB/Lee et al. - 2018 - Maximum Causal Tsallis Entropy Imitation Learning.pdf}
}

@article{nguyen_surrogate_2009,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {math/0510521},
  langid = {english},
  title = {On Surrogate Loss Functions and \$f\$-Divergences},
  volume = {37},
  issn = {0090-5364},
  url = {http://arxiv.org/abs/math/0510521},
  abstract = {The goal of binary classification is to estimate a discriminant function \$$\backslash$gamma\$ from observations of covariate vectors and corresponding binary labels. We consider an elaboration of this problem in which the covariates are not available directly but are transformed by a dimensionality-reducing quantizer \$Q\$. We present conditions on loss functions such that empirical risk minimization yields Bayes consistency when both the discriminant function and the quantizer are estimated. These conditions are stated in terms of a general correspondence between loss functions and a class of functionals known as Ali-Silvey or \$f\$-divergence functionals. Whereas this correspondence was established by Blackwell [Proc. 2nd Berkeley Symp. Probab. Statist. 1 (1951) 93--102. Univ. California Press, Berkeley] for the 0--1 loss, we extend the correspondence to the broader class of surrogate loss functions that play a key role in the general theory of Bayes consistency for binary classification. Our result makes it possible to pick out the (strict) subset of surrogate loss functions that yield Bayes consistency for joint estimation of the discriminant function and the quantizer.},
  number = {2},
  journaltitle = {The Annals of Statistics},
  urldate = {2018-11-08},
  date = {2009-04},
  pages = {876-904},
  keywords = {Mathematics - Statistics Theory,Computer Science - Information Theory,62G10; 68Q32; 62K05 (Primary)},
  author = {Nguyen, XuanLong and Wainwright, Martin J. and Jordan, Michael I.},
  file = {/home/arthur/Zotero/storage/SGMWNC5C/Nguyen et al. - 2009 - On surrogate loss functions and $f$-divergences.pdf}
}

@article{volkovs_loss-sensitive_2011,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1107.1805},
  primaryClass = {cs, stat},
  langid = {english},
  title = {Loss-Sensitive {{Training}} of {{Probabilistic Conditional Random Fields}}},
  url = {http://arxiv.org/abs/1107.1805},
  abstract = {We consider the problem of training probabilistic conditional random ﬁelds (CRFs) in the context of a task where performance is measured using a speciﬁc loss function. While maximum likelihood is the most common approach to training CRFs, it ignores the inherent structure of the task’s loss function. We describe alternatives to maximum likelihood which take that loss into account. These include a novel adaptation of a loss upper bound from the structured SVMs literature to the CRF context, as well as a new loss-inspired KL divergence objective which relies on the probabilistic nature of CRFs. These loss-sensitive objectives are compared to maximum likelihood using ranking as a benchmark task. This comparison conﬁrms the importance of incorporating loss information in the probabilistic training of CRFs, with the loss-inspired KL outperforming all other objectives.},
  urldate = {2018-11-09},
  date = {2011-07-09},
  keywords = {Statistics - Machine Learning,Computer Science - Artificial Intelligence},
  author = {Volkovs, Maksims N. and Larochelle, Hugo and Zemel, Richard S.},
  file = {/home/arthur/Zotero/storage/W8596B44/Volkovs et al. - 2011 - Loss-sensitive Training of Probabilistic Condition.pdf}
}

@article{fathony_distributionally_2018,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1811.02728},
  primaryClass = {cs, stat},
  title = {Distributionally {{Robust Graphical Models}}},
  url = {http://arxiv.org/abs/1811.02728},
  abstract = {In many structured prediction problems, complex relationships between variables are compactly defined using graphical structures. The most prevalent graphical prediction methods---probabilistic graphical models and large margin methods---have their own distinct strengths but also possess significant drawbacks. Conditional random fields (CRFs) are Fisher consistent, but they do not permit integration of customized loss metrics into their learning process. Large-margin models, such as structured support vector machines (SSVMs), have the flexibility to incorporate customized loss metrics, but lack Fisher consistency guarantees. We present adversarial graphical models (AGM), a distributionally robust approach for constructing a predictor that performs robustly for a class of data distributions defined using a graphical structure. Our approach enjoys both the flexibility of incorporating customized loss metrics into its design as well as the statistical guarantee of Fisher consistency. We present exact learning and prediction algorithms for AGM with time complexity similar to existing graphical models and show the practical benefits of our approach with experiments.},
  urldate = {2018-11-09},
  date = {2018-11-06},
  keywords = {Statistics - Machine Learning,Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  author = {Fathony, Rizal and Rezaei, Ashkan and Bashiri, Mohammad Ali and Zhang, Xinhua and Ziebart, Brian D.},
  file = {/home/arthur/Dropbox/Zotero/Fathony et al_2018_Distributionally Robust Graphical Models.pdf;/home/arthur/Zotero/storage/FB32JN7D/1811.html}
}

@article{xu_how_2018,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1810.00826},
  primaryClass = {cs, stat},
  langid = {english},
  title = {How {{Powerful}} Are {{Graph Neural Networks}}?},
  url = {http://arxiv.org/abs/1810.00826},
  abstract = {Graph Neural Networks (GNNs) for representation learning of graphs broadly follow a neighborhood aggregation framework, where the representation vector of a node is computed by recursively aggregating and transforming feature vectors of its neighboring nodes. Many GNN variants have been proposed and have achieved state-of-the-art results on both node and graph classiﬁcation tasks. However, despite GNNs revolutionizing graph representation learning, there is limited understanding of their representational properties and limitations. Here, we present a theoretical framework for analyzing the expressive power of GNNs in capturing different graph structures. Our results characterize the discriminative power of popular GNN variants, such as Graph Convolutional Networks and GraphSAGE, and show that they cannot learn to distinguish certain simple graph structures. We then develop a simple architecture that is provably the most expressive among the class of GNNs and is as powerful as the Weisfeiler-Lehman graph isomorphism test. We empirically validate our theoretical ﬁndings on a number of graph classiﬁcation benchmarks, and demonstrate that our model achieves state-of-the-art performance.},
  urldate = {2018-11-09},
  date = {2018-10-01},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Statistics - Machine Learning,Computer Science - Machine Learning},
  author = {Xu, Keyulu and Hu, Weihua and Leskovec, Jure and Jegelka, Stefanie},
  file = {/home/arthur/Zotero/storage/K3YDXZ93/Xu et al. - 2018 - How Powerful are Graph Neural Networks.pdf}
}

@article{peyre_gromov-wasserstein_2016,
  langid = {english},
  title = {Gromov-{{Wasserstein Averaging}} of {{Kernel}} and {{Distance Matrices}}},
  abstract = {This paper presents a new technique for computing the barycenter of a set of distance or kernel matrices. These matrices, which deﬁne the interrelationships between points sampled from individual domains, are not required to have the same size or to be in row-by-row correspondence. We compare these matrices using the softassign criterion, which measures the minimum distortion induced by a probabilistic map from the rows of one similarity matrix to the rows of another; this criterion amounts to a regularized version of the Gromov-Wasserstein (GW) distance between metric-measure spaces. The barycenter is then deﬁned as a Fre´chet mean of the input matrices with respect to this criterion, minimizing a weighted sum of softassign values. We provide a fast iterative algorithm for the resulting nonconvex optimization problem, built upon state-ofthe-art tools for regularized optimal transportation. We demonstrate its application to the computation of shape barycenters and to the prediction of energy levels from molecular conﬁgurations in quantum chemistry.},
  date = {2016},
  pages = {9},
  author = {Peyré, Gabriel and Cuturi, Marco and Solomon, Justin},
  file = {/home/arthur/Zotero/storage/4NDLDZ4D/Peyré et al. - Gromov-Wasserstein Averaging of Kernel and Distanc.pdf}
}

@article{amid_two-temperature_2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1705.07210},
  primaryClass = {cs, stat},
  title = {Two-Temperature Logistic Regression Based on the {{Tsallis}} Divergence},
  url = {http://arxiv.org/abs/1705.07210},
  abstract = {We develop a variant of multiclass logistic regression that achieves three properties: i) We minimize a non-convex surrogate loss which makes the method robust to outliers, ii) our method allows transitioning between non-convex and convex losses by the choice of the parameters, iii) the surrogate loss is Bayes consistent, even in the non-convex case. The algorithm has one weight vector per class and the surrogate loss is a function of the linear activations (one per class). The surrogate loss of an example with linear activation vector \$$\backslash$mathbf\{a\}\$ and class \$c\$ has the form \$-$\backslash$log\_\{t\_1\} $\backslash$exp\_\{t\_2\} (a\_c - G\_\{t\_2\}($\backslash$mathbf\{a\}))\$ where the two temperatures \$t\_1\$ and \$t\_2\$ "temper" the \$$\backslash$log\$ and \$$\backslash$exp\$, respectively, and \$G\_\{t\_2\}\$ is a generalization of the log-partition function. We motivate this loss using the Tsallis divergence. As the temperature of the logarithm becomes smaller than the temperature of the exponential, the surrogate loss becomes "more quasi-convex". Various tunings of the temperatures recover previous methods and tuning the degree of non-convexity is crucial in the experiments. The choice \$t\_1$<$1\$ and \$t\_2$>$1\$ performs best experimentally. We explain this by showing that \$t\_1 $<$ 1\$ caps the surrogate loss and \$t\_2 $>$1\$ makes the predictive distribution have a heavy tail.},
  urldate = {2018-11-09},
  date = {2017-05-19},
  keywords = {Statistics - Machine Learning,Computer Science - Machine Learning},
  author = {Amid, Ehsan and Warmuth, Manfred K.},
  file = {/home/arthur/Dropbox/Zotero/Amid_Warmuth_2017_Two-temperature logistic regression based on the Tsallis divergence.pdf;/home/arthur/Zotero/storage/PBTQ5KHL/1705.html}
}

@article{sridharan_minimizing_2012,
  langid = {english},
  title = {Minimizing {{The Misclassification Error Rate}}  {{Using}} a {{Surrogate Convex Loss}}},
  abstract = {We carefully study how well minimizing convex surrogate loss functions corresponds to minimizing the misclassiﬁcation error rate for the problem of binary classiﬁcation with linear predictors. We consider the agnostic setting, and investigate guarantees on the misclassiﬁcation error of the loss-minimizer in terms of the margin error rate of the best predictor. We show that, aiming for such a guarantee, the hinge loss is essentially optimal among all convex losses.},
  date = {2012},
  pages = {8},
  author = {Sridharan, Karthik},
  file = {/home/arthur/Zotero/storage/3L9MZFUQ/Sridharan - Minimizing The Misclassification Error Rate  Using.pdf}
}

@article{amari_geometry_2012,
  langid = {english},
  title = {Geometry of Deformed Exponential Families: {{Invariant}}, Dually-Flat and Conformal Geometries},
  volume = {391},
  issn = {03784371},
  url = {http://linkinghub.elsevier.com/retrieve/pii/S037843711200310X},
  shorttitle = {Geometry of Deformed Exponential Families},
  number = {18},
  journaltitle = {Physica A: Statistical Mechanics and its Applications},
  urldate = {2018-11-09},
  date = {2012-09},
  pages = {4308-4319},
  author = {Amari, Shun-ichi and Ohara, Atsumi and Matsuzoe, Hiroshi},
  file = {/home/arthur/Dropbox/Zotero/Amari et al_2012_Geometry of deformed exponential families.pdf}
}

@article{bach_non-strongly-convex_2013,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1306.2119},
  primaryClass = {cs, math, stat},
  langid = {english},
  title = {Non-Strongly-Convex Smooth Stochastic Approximation with Convergence Rate {{O}}(1/N)},
  url = {http://arxiv.org/abs/1306.2119},
  abstract = {We consider the stochastic approximation problem where a convex function has to be minimized, given only the knowledge of unbiased estimates of its gradients at certain points, a framework which includes machine learning methods based on the minimization of the empirical risk. We focus on problems without strong convexity, for which all previously known algorithms achieve a convergence rate for function values of O(1/√n). We consider and analyze two algorithms that achieve a rate of O(1/n) for classical supervised learning problems. For least-squares regression, we show that averaged stochastic gradient descent with constant step-size achieves the desired rate. For logistic regression, this is achieved by a simple novel stochastic gradient algorithm that (a) constructs successive local quadratic approximations of the loss functions, while (b) preserving the same running time complexity as stochastic gradient descent. For these algorithms, we provide a non-asymptotic analysis of the generalization error (in expectation, and also in high probability for least-squares), and run extensive experiments on standard machine learning benchmarks showing that they often outperform existing approaches.},
  urldate = {2018-11-09},
  date = {2013-06-10},
  keywords = {Mathematics - Optimization and Control,Statistics - Machine Learning,Computer Science - Machine Learning},
  author = {Bach, Francis and Moulines, Eric},
  file = {/home/arthur/Zotero/storage/ACWAAQ97/Bach and Moulines - 2013 - Non-strongly-convex smooth stochastic approximatio.pdf}
}

@article{jain_accelerating_2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1704.08227},
  primaryClass = {cs, math, stat},
  langid = {english},
  title = {Accelerating {{Stochastic Gradient Descent For Least Squares Regression}}},
  url = {http://arxiv.org/abs/1704.08227},
  abstract = {There is widespread sentiment that fast gradient methods (e.g. Nesterov’s acceleration, conjugate gradient, heavy ball) are not effective for stochastic optimization due to their instability and error accumulation. Numerous works have attempted to quantify these instabilities in the face of either statistical or non-statistical errors (Paige, 1971; Proakis, 1974; Polyak, 1987; Greenbaum, 1989; Devolder et al., 2014). This work considers these issues for the case of stochastic approximation for the least squares regression problem, and our main result refutes this conventional wisdom by showing that acceleration can be made robust to statistical errors. In particular, this work introduces an accelerated stochastic gradient method that provably achieves the minimax optimal statistical risk faster than stochastic gradient descent. Critical to the analysis is a sharp characterization of accelerated stochastic gradient descent as a stochastic process. We hope this characterization gives insights towards the broader question of designing simple and effective accelerated stochastic methods for general convex and non-convex optimization problems.},
  urldate = {2018-11-09},
  date = {2017-04-26},
  keywords = {Mathematics - Optimization and Control,Mathematics - Statistics Theory,Statistics - Machine Learning,Computer Science - Machine Learning},
  author = {Jain, Prateek and Kakade, Sham M. and Kidambi, Rahul and Netrapalli, Praneeth and Sidford, Aaron},
  file = {/home/arthur/Zotero/storage/Q86QZBI2/Jain et al. - 2017 - Accelerating Stochastic Gradient Descent For Least.pdf}
}

@article{ostrovskii_finite-sample_2018,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1810.06838},
  primaryClass = {math, stat},
  title = {Finite-Sample {{Analysis}} of {{M}}-Estimators Using {{Self}}-Concordance},
  url = {http://arxiv.org/abs/1810.06838},
  abstract = {We demonstrate how self-concordance of the loss can be exploited to obtain asymptotically optimal rates for M-estimators in finite-sample regimes. We consider two classes of losses: (i) canonically self-concordant losses in the sense of Nesterov and Nemirovski (1994), i.e., with the third derivative bounded with the \$3/2\$ power of the second; (ii) pseudo self-concordant losses, for which the power is removed, as introduced by Bach (2010). These classes contain some losses arising in generalized linear models, including logistic regression; in addition, the second class includes some common pseudo-Huber losses. Our results consist in establishing the critical sample size sufficient to reach the asymptotically optimal excess risk for both classes of losses. Denoting \$d\$ the parameter dimension, and \$d\_\{$\backslash$text\{eff\}\}\$ the effective dimension which takes into account possible model misspecification, we find the critical sample size to be \$O(d\_\{$\backslash$text\{eff\}\} $\backslash$cdot d)\$ for canonically self-concordant losses, and \$O($\backslash$rho $\backslash$cdot d\_\{$\backslash$text\{eff\}\} $\backslash$cdot d)\$ for pseudo self-concordant losses, where \$$\backslash$rho\$ is the problem-dependent local curvature parameter. In contrast to the existing results, we only impose local assumptions on the data distribution, assuming that the calibrated design, i.e., the design scaled with the square root of the second derivative of the loss, is subgaussian at the best predictor \$$\backslash$theta\_*\$. Moreover, we obtain the improved bounds on the critical sample size, scaling near-linearly in \$$\backslash$max(d\_\{$\backslash$text\{eff\}\},d)\$, under the extra assumption that the calibrated design is subgaussian in the Dikin ellipsoid of \$$\backslash$theta\_*\$. Motivated by these findings, we construct canonically self-concordant analogues of the Huber and logistic losses with improved statistical properties. Finally, we extend some of these results to \$$\backslash$ell\_1\$-regularized M-estimators in high dimensions.},
  urldate = {2018-11-09},
  date = {2018-10-16},
  keywords = {Mathematics - Optimization and Control,Mathematics - Statistics Theory,Statistics - Machine Learning},
  author = {Ostrovskii, Dmitrii and Bach, Francis},
  file = {/home/arthur/Dropbox/Zotero/Ostrovskii_Bach_2018_Finite-sample Analysis of M-estimators using Self-concordance.pdf;/home/arthur/Zotero/storage/8R8QCFLU/1810.html}
}

@article{collins_logistic_2002,
  langid = {english},
  title = {Logistic {{Regression}}, {{AdaBoost}} and {{Bregman Distances}}},
  abstract = {We give a uniﬁed account of boosting and logistic regression in which each learning problem is cast in terms of optimization of Bregman distances. The striking similarity of the two problems in this framework allows us to design and analyze algorithms for both simultaneously, and to easily adapt algorithms designed for one problem to the other. For both problems, we give new algorithms and explain their potential advantages over existing methods. These algorithms are iterative and can be divided into two types based on whether the parameters are updated sequentially (one at a time) or in parallel (all at once). We also describe a parameterized family of algorithms that includes both a sequential- and a parallel-update algorithm as special cases, thus showing how the sequential and parallel approaches can themselves be uniﬁed. For all of the algorithms, we give convergence proofs using a general formalization of the auxiliary-function proof technique. As one of our sequential-update algorithms is equivalent to AdaBoost, this provides the ﬁrst general proof of convergence for AdaBoost. We show that all of our algorithms generalize easily to the multiclass case, and we contrast the new algorithms with the iterative scaling algorithm. We conclude with a few experimental results with synthetic data that highlight the behavior of the old and newly proposed algorithms in different settings.},
  journaltitle = {LOGISTIC REGRESSION},
  date = {2002},
  pages = {33},
  author = {Collins, Michael},
  file = {/home/arthur/Zotero/storage/IECN5XQT/Collins - Logistic Regression, AdaBoost and Bregman Distance.pdf}
}

@article{natarajan_cost-sensitive_2018,
  langid = {english},
  title = {Cost-{{Sensitive Learning}} with {{Noisy Labels}}},
  date = {2018},
  pages = {33},
  author = {Natarajan, Nagarajan and Dhillon, Inderjit S and Ravikumar, Pradeep and Tewari, Ambuj},
  file = {/home/arthur/Zotero/storage/CBBFPDIG/Natarajan et al. - Cost-Sensitive Learning with Noisy Labels.pdf}
}

@inproceedings{patrini_making_2017,
  langid = {english},
  location = {{Honolulu, HI}},
  title = {Making {{Deep Neural Networks Robust}} to {{Label Noise}}: {{A Loss Correction Approach}}},
  isbn = {978-1-5386-0457-1},
  url = {http://ieeexplore.ieee.org/document/8099723/},
  shorttitle = {Making {{Deep Neural Networks Robust}} to {{Label Noise}}},
  abstract = {We present a theoretically grounded approach to train deep neural networks, including recurrent networks, subject to class-dependent label noise. We propose two procedures for loss correction that are agnostic to both application domain and network architecture. They simply amount to at most a matrix inversion and multiplication, provided that we know the probability of each class being corrupted into another. We further show how one can estimate these probabilities, adapting a recent technique for noise estimation to the multi-class setting, and thus providing an end-to-end framework. Extensive experiments on MNIST, IMDB, CIFAR10, CIFAR-100 and a large scale dataset of clothing images employing a diversity of architectures — stacking dense, convolutional, pooling, dropout, batch normalization, word embedding, LSTM and residual layers — demonstrate the noise robustness of our proposals. Incidentally, we also prove that, when ReLU is the only non-linearity, the loss curvature is immune to class-dependent label noise.},
  eventtitle = {2017 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  booktitle = {2017 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  publisher = {{IEEE}},
  urldate = {2018-11-12},
  date = {2017-07},
  pages = {2233-2241},
  author = {Patrini, Giorgio and Rozza, Alessandro and Menon, Aditya Krishna and Nock, Richard and Qu, Lizhen},
  file = {/home/arthur/Zotero/storage/A4ATLB42/Patrini et al. - 2017 - Making Deep Neural Networks Robust to Label Noise.pdf}
}

@article{patrini_sinkhorn_2018,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1810.01118},
  primaryClass = {cs, stat},
  title = {Sinkhorn {{AutoEncoders}}},
  url = {http://arxiv.org/abs/1810.01118},
  abstract = {Optimal Transport offers an alternative to maximum likelihood for learning generative autoencoding models. We show how this principle dictates the minimization of the Wasserstein distance between the encoder aggregated posterior and the prior, plus a reconstruction error. We prove that in the non-parametric limit the autoencoder generates the data distribution if and only if the two distributions match exactly, and that the optimum can be obtained by deterministic autoencoders. We then introduce the Sinkhorn AutoEncoder (SAE), which casts the problem into Optimal Transport on the latent space. The resulting Wasserstein distance is minimized by backpropagating through the Sinkhorn algorithm. SAE models the aggregated posterior as an implicit distribution and therefore does not need a reparameterization trick for gradients estimation. Moreover, it requires virtually no adaptation to different prior distributions. We demonstrate its flexibility by considering models with hyperspherical and Dirichlet priors, as well as a simple case of probabilistic programming. SAE matches or outperforms other autoencoding models in visual quality and FID scores.},
  urldate = {2018-11-12},
  date = {2018-10-02},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Statistics - Machine Learning,Computer Science - Machine Learning},
  author = {Patrini, Giorgio and Carioni, Marcello and Forré, Patrick and Bhargav, Samarth and Welling, Max and van den Berg, Rianne and Genewein, Tim and Nielsen, Frank},
  file = {/home/arthur/Dropbox/Zotero/Patrini et al_2018_Sinkhorn AutoEncoders.pdf;/home/arthur/Zotero/storage/YDK7G8T2/1810.html}
}

@article{ramaswamy_convex_2016,
  title = {Convex {{Calibration Dimension}} for {{Multiclass Loss Matrices}}},
  volume = {17},
  url = {http://jmlr.org/papers/v17/14-316.html},
  number = {14},
  journaltitle = {Journal of Machine Learning Research},
  urldate = {2018-11-12},
  date = {2016},
  pages = {1-45},
  author = {Ramaswamy, Harish G. and Agarwal, Shivani},
  file = {/home/arthur/Dropbox/Zotero/Ramaswamy_Agarwal_2016_Convex Calibration Dimension for Multiclass Loss Matrices.pdf;/home/arthur/Zotero/storage/KDB64SNY/14-316.html}
}

@article{zhang_statistical_2004-1,
  langid = {english},
  title = {Statistical {{Analysis}} of {{Some Multi}}-{{Category Large Margin Classiﬁcation Methods}}},
  abstract = {The purpose of this paper is to investigate statistical properties of risk minimization based multicategory classiﬁcation methods. These methods can be considered as natural extensions of binary large margin classiﬁcation. We establish conditions that guarantee the consistency of classiﬁers obtained in the risk minimization framework with respect to the classiﬁcation error. Examples are provided for four speciﬁc forms of the general formulation, which extend a number of known methods. Using these examples, we show that some risk minimization formulations can also be used to obtain conditional probability estimates for the underlying problem. Such conditional probability information can be useful for statistical inferencing tasks beyond classiﬁcation.},
  date = {2004},
  pages = {27},
  author = {Zhang, Tong},
  file = {/home/arthur/Zotero/storage/DUW7MXDH/Zhang - Statistical Analysis of Some Multi-Category Large .pdf}
}

@article{zhang_review_2014,
  langid = {english},
  title = {A {{Review}} on {{Multi}}-{{Label Learning Algorithms}}},
  volume = {26},
  issn = {1041-4347},
  url = {http://ieeexplore.ieee.org/document/6471714/},
  abstract = {Multi-label learning studies the problem where each example is represented by a single instance while associated with a set of labels simultaneously. During the past decade, signiﬁcant amount of progresses have been made towards this emerging machine learning paradigm. This paper aims to provide a timely review on this area with emphasis on state-of-the-art multi-label learning algorithms. Firstly, fundamentals on multi-label learning including formal deﬁnition and evaluation metrics are given. Secondly and primarily, eight representative multi-label learning algorithms are scrutinized under common notations with relevant analyses and discussions. Thirdly, several related learning settings are brieﬂy summarized. As a conclusion, online resources and open research problems on multi-label learning are outlined for reference purposes.},
  number = {8},
  journaltitle = {IEEE Transactions on Knowledge and Data Engineering},
  urldate = {2018-11-12},
  date = {2014-08},
  pages = {1819-1837},
  author = {Zhang, Min-Ling and Zhou, Zhi-Hua},
  file = {/home/arthur/Zotero/storage/CR57Y2NR/Zhang and Zhou - 2014 - A Review on Multi-Label Learning Algorithms.pdf}
}

@article{rockafellar_risk_2018,
  langid = {english},
  title = {Risk and Utility in the Duality Framework of Convex Analysis},
  abstract = {Measures of risk have grown in importance in expressing preferences between diﬀerent manifestations of uncertain cost or loss in ﬁnance and engineering, but utility functions and expected utility have had a more traditional role. This article surveys how risk and utility are in fact more closely related than may have been appreciated by practitioners. The tools of convex analysis, including conjugate duality, are able to bring this out.},
  date = {2018},
  pages = {19},
  author = {Rockafellar, R Tyrrell},
  file = {/home/arthur/Dropbox/Zotero/Rockafellar_2018_Risk and utility in the duality framework of convex analysis.pdf}
}

@article{korba_structured_2018,
  langid = {english},
  title = {A {{Structured Prediction Approach}} for {{Label Ranking}}},
  url = {https://arxiv.org/abs/1807.02374},
  urldate = {2018-11-12},
  date = {2018-07-06},
  author = {Korba, Anna and Garcia, Alexandre and d'Alché Buc, Florence},
  file = {/home/arthur/Dropbox/Zotero/Korba et al_2018_A Structured Prediction Approach for Label Ranking.pdf;/home/arthur/Zotero/storage/IS884ZF8/1807.html}
}

@article{pillaud-vivien_exponential_2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1712.04755},
  primaryClass = {cs, stat},
  title = {Exponential Convergence of Testing Error for Stochastic Gradient Methods},
  url = {http://arxiv.org/abs/1712.04755},
  abstract = {We consider binary classification problems with positive definite kernels and square loss, and study the convergence rates of stochastic gradient methods. We show that while the excess testing loss (squared loss) converges slowly to zero as the number of observations (and thus iterations) goes to infinity, the testing error (classification error) converges exponentially fast if low-noise conditions are assumed.},
  urldate = {2018-11-12},
  date = {2017-12-13},
  keywords = {Statistics - Machine Learning,Computer Science - Machine Learning},
  author = {Pillaud-Vivien, Loucas and Rudi, Alessandro and Bach, Francis},
  file = {/home/arthur/Dropbox/Zotero/Pillaud-Vivien et al_2017_Exponential convergence of testing error for stochastic gradient methods.pdf;/home/arthur/Zotero/storage/WCIPZKFT/1712.html}
}

@article{luise_differential_2018,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1805.11897},
  primaryClass = {cs, stat},
  langid = {english},
  title = {Differential {{Properties}} of {{Sinkhorn Approximation}} for {{Learning}} with {{Wasserstein Distance}}},
  url = {http://arxiv.org/abs/1805.11897},
  abstract = {Applications of optimal transport have recently gained remarkable attention thanks to the computational advantages of entropic regularization. However, in most situations the Sinkhorn approximation of the Wasserstein distance is replaced by a regularized version that is less accurate but easy to differentiate. In this work we characterize the differential properties of the original Sinkhorn distance, proving that it enjoys the same smoothness as its regularized version and we explicitly provide an efﬁcient algorithm to compute its gradient. We show that this result beneﬁts both theory and applications: on one hand, high order smoothness confers statistical guarantees to learning with Wasserstein approximations. On the other hand, the gradient formula allows us to efﬁciently solve learning and optimization problems in practice. Promising preliminary experiments complement our analysis.},
  urldate = {2018-11-13},
  date = {2018-05-30},
  keywords = {Statistics - Machine Learning,Computer Science - Machine Learning},
  author = {Luise, Giulia and Rudi, Alessandro and Pontil, Massimiliano and Ciliberto, Carlo},
  file = {/home/arthur/Dropbox/Zotero/Luise et al_2018_Differential Properties of Sinkhorn Approximation for Learning with Wasserstein.pdf}
}

@unpublished{feydy_global_2018,
  title = {Global Divergences between Measures: From {{Hausdorff}} Distance to {{Optimal Transport}}},
  url = {https://hal.archives-ouvertes.fr/hal-01827184},
  shorttitle = {Global Divergences between Measures},
  abstract = {The data fidelity term is a key component of shape registration pipelines: computed at every step, its gradient is the vector field that drives a deformed model towards its target. Unfortunately, most classical formulas are at most semi-local: their gradients saturate and stop being informative above some given distance, with appalling consequences on the robustness of shape analysis pipelines.

In this paper, we build on recent theoretical advances on "Sinkhorn entropies and divergences" to present a unified view of three fidelities between measures that alleviate this problem: the Energy Distance from statistics; the (weighted) Hausdorff distance from computer graphics; the Wasserstein distance from Optimal Transport theory. The ε-Hausdorff and ε-Sinkhorn divergences are positive fidelities that interpolate between these three quantities, and we implement them through efficient, freely available GPU routines. They should allow the shape analyst to handle large deformations without hassle.},
  urldate = {2018-11-14},
  date = {2018-08},
  author = {Feydy, Jean and Trouvé, Alain},
  file = {/home/arthur/Dropbox/Zotero/Feydy_Trouvé_2018_Global divergences between measures.pdf}
}

@article{feydy_interpolating_2018,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1810.08278},
  primaryClass = {math, stat},
  title = {Interpolating between {{Optimal Transport}} and {{MMD}} Using {{Sinkhorn Divergences}}},
  url = {http://arxiv.org/abs/1810.08278},
  abstract = {Comparing probability distributions is a fundamental problem in data sciences. Simple norms and divergences such as the total variation and the relative entropy only compare densities in a point-wise manner and fail to capture the geometric nature of the problem. In sharp contrast, Maximum Mean Discrepancies (MMD) and Optimal Transport distances (OT) are two classes of distances between measures that take into account the geometry of the underlying space and metrize the convergence in law. This paper studies the Sinkhorn divergences, a family of geometric divergences that interpolates between MMD and OT. Relying on a new notion of geometric entropy, we provide theoretical guarantees for these divergences: positivity, convexity and metrization of the convergence in law. On the practical side, we detail a numerical scheme that enables the large scale application of these divergences for machine learning: on the GPU, gradients of the Sinkhorn loss can be computed for batches of a million samples.},
  urldate = {2018-11-14},
  date = {2018-10-18},
  keywords = {Mathematics - Statistics Theory,62},
  author = {Feydy, Jean and Séjourné, Thibault and Vialard, François-Xavier and Amari, Shun-ichi and Trouvé, Alain and Peyré, Gabriel},
  file = {/home/arthur/Dropbox/Zotero/Feydy et al_2018_Interpolating between Optimal Transport and MMD using Sinkhorn Divergences.pdf;/home/arthur/Zotero/storage/RMJ2TNQN/1810.html}
}

@article{singh_minimax_2018,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1802.08855},
  primaryClass = {cs, math, stat},
  title = {Minimax {{Distribution Estimation}} in {{Wasserstein Distance}}},
  url = {http://arxiv.org/abs/1802.08855},
  abstract = {The Wasserstein metric is an important measure of distance between probability distributions, with applications in machine learning, statistics, probability theory, and data analysis. This paper provides upper and lower bounds on statistical minimax rates for the problem of estimating a probability distribution under Wasserstein loss, using only metric properties, such as covering and packing numbers, of the sample space, and weak moment assumptions on the probability distributions.},
  urldate = {2018-11-14},
  date = {2018-02-24},
  keywords = {Mathematics - Statistics Theory,Statistics - Machine Learning,Computer Science - Information Theory,Computer Science - Machine Learning},
  author = {Singh, Shashank and Póczos, Barnabás},
  file = {/home/arthur/Zotero/storage/RHKAZMSH/1802.html}
}

@article{canas_learning_2012,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1209.1077},
  primaryClass = {cs, stat},
  title = {Learning {{Probability Measures}} with Respect to {{Optimal Transport Metrics}}},
  url = {http://arxiv.org/abs/1209.1077},
  abstract = {We study the problem of estimating, in the sense of optimal transport metrics, a measure which is assumed supported on a manifold embedded in a Hilbert space. By establishing a precise connection between optimal transport metrics, optimal quantization, and learning theory, we derive new probabilistic bounds for the performance of a classic algorithm in unsupervised learning (k-means), when used to produce a probability measure derived from the data. In the course of the analysis, we arrive at new lower bounds, as well as probabilistic upper bounds on the convergence rate of the empirical law of large numbers, which, unlike existing bounds, are applicable to a wide class of measures.},
  urldate = {2018-11-14},
  date = {2012-09-05},
  keywords = {Statistics - Machine Learning,Computer Science - Machine Learning,K.3.2},
  author = {Canas, Guillermo D. and Rosasco, Lorenzo},
  file = {/home/arthur/Dropbox/Zotero/Canas_Rosasco_2012_Learning Probability Measures with respect to Optimal Transport Metrics.pdf;/home/arthur/Zotero/storage/DF3IETKK/1209.html}
}

@article{liang_how_2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1712.08244},
  primaryClass = {cs, math, stat},
  title = {How {{Well Can Generative Adversarial Networks Learn Densities}}: {{A Nonparametric View}}},
  url = {http://arxiv.org/abs/1712.08244},
  shorttitle = {How {{Well Can Generative Adversarial Networks Learn Densities}}},
  abstract = {We study in this paper the rate of convergence for learning densities under the Generative Adversarial Networks (GAN) framework, borrowing insights from nonparametric statistics. We introduce an improved GAN estimator that achieves a faster rate, through simultaneously leveraging the level of smoothness in the target density and the evaluation metric, which in theory remedies the mode collapse problem reported in the literature. A minimax lower bound is constructed to show that when the dimension is large, the exponent in the rate for the new GAN estimator is near optimal. One can view our results as answering in a quantitative way how well GAN learns a wide range of densities with different smoothness properties, under a hierarchy of evaluation metrics. As a byproduct, we also obtain improved generalization bounds for GAN with deeper ReLU discriminator network.},
  urldate = {2018-11-14},
  date = {2017-12-21},
  keywords = {Mathematics - Statistics Theory,Statistics - Machine Learning,Computer Science - Machine Learning},
  author = {Liang, Tengyuan},
  file = {/home/arthur/Dropbox/Zotero/Liang_2017_How Well Can Generative Adversarial Networks Learn Densities.pdf;/home/arthur/Zotero/storage/8L7DRVU3/1712.html}
}

@article{frogner_learning_2015,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1506.05439},
  primaryClass = {cs, stat},
  title = {Learning with a {{Wasserstein Loss}}},
  url = {http://arxiv.org/abs/1506.05439},
  abstract = {Learning to predict multi-label outputs is challenging, but in many problems there is a natural metric on the outputs that can be used to improve predictions. In this paper we develop a loss function for multi-label learning, based on the Wasserstein distance. The Wasserstein distance provides a natural notion of dissimilarity for probability measures. Although optimizing with respect to the exact Wasserstein distance is costly, recent work has described a regularized approximation that is efficiently computed. We describe an efficient learning algorithm based on this regularization, as well as a novel extension of the Wasserstein distance from probability measures to unnormalized measures. We also describe a statistical learning bound for the loss. The Wasserstein loss can encourage smoothness of the predictions with respect to a chosen metric on the output space. We demonstrate this property on a real-data tag prediction problem, using the Yahoo Flickr Creative Commons dataset, outperforming a baseline that doesn't use the metric.},
  urldate = {2018-11-14},
  date = {2015-06-17},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  author = {Frogner, Charlie and Zhang, Chiyuan and Mobahi, Hossein and Araya-Polo, Mauricio and Poggio, Tomaso},
  file = {/home/arthur/Dropbox/Zotero/Frogner et al_2015_Learning with a Wasserstein Loss.pdf;/home/arthur/Zotero/storage/KI79XH7X/1506.html}
}

@article{huhnerbein_image_2018,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1710.01493},
  title = {Image {{Labeling Based}} on {{Graphical Models Using Wasserstein Messages}} and {{Geometric Assignment}}},
  volume = {11},
  issn = {1936-4954},
  url = {http://arxiv.org/abs/1710.01493},
  abstract = {We introduce a novel approach to Maximum A Posteriori inference based on discrete graphical models. By utilizing local Wasserstein distances for coupling assignment measures across edges of the underlying graph, a given discrete objective function is smoothly approximated and restricted to the assignment manifold. A corresponding multiplicative update scheme combines in a single process (i) geometric integration of the resulting Riemannian gradient flow and (ii) rounding to integral solutions that represent valid labelings. Throughout this process, local marginalization constraints known from the established LP relaxation are satisfied, whereas the smooth geometric setting results in rapidly converging iterations that can be carried out in parallel for every edge.},
  number = {2},
  journaltitle = {SIAM Journal on Imaging Sciences},
  urldate = {2018-11-14},
  date = {2018-01},
  pages = {1317-1362},
  keywords = {62H35; 62M40; 65K10; 68U10,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Numerical Analysis,Mathematics - Optimization and Control},
  author = {Hühnerbein, Ruben and Savarino, Fabrizio and Åström, Freddie and Schnörr, Christoph},
  file = {/home/arthur/Dropbox/Zotero/Hühnerbein et al_2018_Image Labeling Based on Graphical Models Using Wasserstein Messages and.pdf;/home/arthur/Zotero/storage/7FKGMCHX/1710.html}
}

@article{ramdas_wasserstein_2015,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1509.02237},
  primaryClass = {math, stat},
  title = {On {{Wasserstein Two Sample Testing}} and {{Related Families}} of {{Nonparametric Tests}}},
  url = {http://arxiv.org/abs/1509.02237},
  abstract = {Nonparametric two sample or homogeneity testing is a decision theoretic problem that involves identifying differences between two random variables without making parametric assumptions about their underlying distributions. The literature is old and rich, with a wide variety of statistics having being intelligently designed and analyzed, both for the unidimensional and the multivariate setting. Our contribution is to tie together many of these tests, drawing connections between seemingly very different statistics. In this work, our central object is the Wasserstein distance, as we form a chain of connections from univariate methods like the Kolmogorov-Smirnov test, PP/QQ plots and ROC/ODC curves, to multivariate tests involving energy statistics and kernel based maximum mean discrepancy. Some connections proceed through the construction of a $\backslash$textit\{smoothed\} Wasserstein distance, and others through the pursuit of a "distribution-free" Wasserstein test. Some observations in this chain are implicit in the literature, while others seem to have not been noticed thus far. Given nonparametric two sample testing's classical and continued importance, we aim to provide useful connections for theorists and practitioners familiar with one subset of methods but not others.},
  urldate = {2018-11-14},
  date = {2015-09-07},
  keywords = {Mathematics - Statistics Theory,Statistics - Machine Learning},
  author = {Ramdas, Aaditya and Garcia, Nicolas and Cuturi, Marco},
  file = {/home/arthur/Dropbox/Zotero/Ramdas et al_2015_On Wasserstein Two Sample Testing and Related Families of Nonparametric Tests.pdf;/home/arthur/Zotero/storage/A8JWRMGM/1509.html}
}

@article{alvarez_kernels_2011,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1106.6251},
  primaryClass = {cs, math, stat},
  title = {Kernels for {{Vector}}-{{Valued Functions}}: A {{Review}}},
  url = {http://arxiv.org/abs/1106.6251},
  shorttitle = {Kernels for {{Vector}}-{{Valued Functions}}},
  abstract = {Kernel methods are among the most popular techniques in machine learning. From a frequentist/discriminative perspective they play a central role in regularization theory as they provide a natural choice for the hypotheses space and the regularization functional through the notion of reproducing kernel Hilbert spaces. From a Bayesian/generative perspective they are the key in the context of Gaussian processes, where the kernel function is also known as the covariance function. Traditionally, kernel methods have been used in supervised learning problem with scalar outputs and indeed there has been a considerable amount of work devoted to designing and learning kernels. More recently there has been an increasing interest in methods that deal with multiple outputs, motivated partly by frameworks like multitask learning. In this paper, we review different methods to design or learn valid kernel functions for multiple outputs, paying particular attention to the connection between probabilistic and functional methods.},
  urldate = {2018-11-14},
  date = {2011-06-30},
  keywords = {Computer Science - Artificial Intelligence,Mathematics - Statistics Theory,Statistics - Machine Learning},
  author = {Alvarez, Mauricio A. and Rosasco, Lorenzo and Lawrence, Neil D.},
  file = {/home/arthur/Dropbox/Zotero/Alvarez et al_2011_Kernels for Vector-Valued Functions.pdf;/home/arthur/Zotero/storage/MVKKXH7S/1106.html}
}

@article{altschuler_near-linear_2017-1,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1705.09634},
  primaryClass = {cs, stat},
  title = {Near-Linear Time Approximation Algorithms for Optimal Transport via {{Sinkhorn}} Iteration},
  url = {http://arxiv.org/abs/1705.09634},
  abstract = {Computing optimal transport distances such as the earth mover's distance is a fundamental problem in machine learning, statistics, and computer vision. Despite the recent introduction of several algorithms with good empirical performance, it is unknown whether general optimal transport distances can be approximated in near-linear time. This paper demonstrates that this ambitious goal is in fact achieved by Cuturi's Sinkhorn Distances. This result relies on a new analysis of Sinkhorn iteration, which also directly suggests a new greedy coordinate descent algorithm, Greenkhorn, with the same theoretical guarantees. Numerical simulations illustrate that Greenkhorn significantly outperforms the classical Sinkhorn algorithm in practice.},
  urldate = {2018-11-14},
  date = {2017-05-26},
  keywords = {Computer Science - Data Structures and Algorithms,Statistics - Machine Learning},
  author = {Altschuler, Jason and Weed, Jonathan and Rigollet, Philippe},
  file = {/home/arthur/Dropbox/Zotero/Altschuler et al_2017_Near-linear time approximation algorithms for optimal transport via Sinkhorn2.pdf;/home/arthur/Zotero/storage/757UQPBT/1705.html}
}

@article{vinyals_matching_2016,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1606.04080},
  primaryClass = {cs, stat},
  title = {Matching {{Networks}} for {{One Shot Learning}}},
  url = {http://arxiv.org/abs/1606.04080},
  abstract = {Learning from a few examples remains a key challenge in machine learning. Despite recent advances in important domains such as vision and language, the standard supervised deep learning paradigm does not offer a satisfactory solution for learning new concepts rapidly from little data. In this work, we employ ideas from metric learning based on deep neural features and from recent advances that augment neural networks with external memories. Our framework learns a network that maps a small labelled support set and an unlabelled example to its label, obviating the need for fine-tuning to adapt to new class types. We then define one-shot learning problems on vision (using Omniglot, ImageNet) and language tasks. Our algorithm improves one-shot accuracy on ImageNet from 87.6\% to 93.2\% and from 88.0\% to 93.8\% on Omniglot compared to competing approaches. We also demonstrate the usefulness of the same model on language modeling by introducing a one-shot task on the Penn Treebank.},
  urldate = {2018-11-14},
  date = {2016-06-13},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  author = {Vinyals, Oriol and Blundell, Charles and Lillicrap, Timothy and Kavukcuoglu, Koray and Wierstra, Daan},
  file = {/home/arthur/Dropbox/Zotero/Vinyals et al_2016_Matching Networks for One Shot Learning.pdf;/home/arthur/Zotero/storage/GIKFYRNZ/1606.html}
}

@article{zhang_policy_2018,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1808.03030},
  primaryClass = {cs, stat},
  langid = {english},
  title = {Policy {{Optimization}} as {{Wasserstein Gradient Flows}}},
  url = {http://arxiv.org/abs/1808.03030},
  abstract = {Policy optimization is a core component of reinforcement learning (RL), and most existing RL methods directly optimize parameters of a policy based on maximizing the expected total reward, or its surrogate. Though often achieving encouraging empirical success, its underlying mathematical principle on policy-distribution optimization is unclear. We place policy optimization into the space of probability measures, and interpret it as Wasserstein gradient ﬂows. On the probabilitymeasure space, under speciﬁed circumstances, policy optimization becomes a convex problem in terms of distribution optimization. To make optimization feasible, we develop efﬁcient algorithms by numerically solving the corresponding discrete gradient ﬂows. Our technique is applicable to several RL settings, and is related to many state-ofthe-art policy-optimization algorithms. Empirical results verify the effectiveness of our framework, often obtaining better performance compared to related algorithms.},
  urldate = {2018-11-14},
  date = {2018-08-09},
  keywords = {Statistics - Machine Learning,Computer Science - Machine Learning},
  author = {Zhang, Ruiyi and Chen, Changyou and Li, Chunyuan and Carin, Lawrence},
  file = {/home/arthur/Zotero/storage/NBD7Z2QV/Zhang et al. - 2018 - Policy Optimization as Wasserstein Gradient Flows.pdf}
}

@article{blier_learning_2018,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1810.01322},
  primaryClass = {cs, stat},
  title = {Learning with {{Random Learning Rates}}},
  url = {http://arxiv.org/abs/1810.01322},
  abstract = {Hyperparameter tuning is a bothersome step in the training of deep learning models. One of the most sensitive hyperparameters is the learning rate of the gradient descent. We present the 'All Learning Rates At Once' (Alrao) optimization method for neural networks: each unit or feature in the network gets its own learning rate sampled from a random distribution spanning several orders of magnitude. This comes at practically no computational cost. Perhaps surprisingly, stochastic gradient descent (SGD) with Alrao performs close to SGD with an optimally tuned learning rate, for various architectures and problems. Alrao could save time when testing deep learning models: a range of models could be quickly assessed with Alrao, and the most promising models could then be trained more extensively. This text comes with a PyTorch implementation of the method, which can be plugged on an existing PyTorch model: https://github.com/leonardblier/alrao .},
  urldate = {2018-11-14},
  date = {2018-10-02},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  author = {Blier, Léonard and Wolinski, Pierre and Ollivier, Yann},
  file = {/home/arthur/Dropbox/Zotero/Blier et al_2018_Learning with Random Learning Rates.pdf;/home/arthur/Zotero/storage/8JCCJE5T/1810.html}
}

