%!TEX root = article.tex
\section{Introduction}

Optimal transport (OT) distances are fundamental in statistical learning, both as a tool for analyzing the convergence of various algorithms~\cite{canas2012learning,dalalyan2019user}, and as a data-dependent term for tasks as diverse as supervised learning~\cite{frogner2015learning}, unsupervised generative modeling~\cite{arjovsky2017wgan} or domain adaptation~\cite{courty2016optimal}.
%
OT lifts a given distance over data points living in space $\Xx$ into a distance
on the space $\Pp(\Xx)$ of probability distributions over this data space $\Xx$. We refer to the monograph~\cite{santambrogio2015optimal} for a detailed mathematical treatement.
%
This distance has many favorable geometrical properties. In particular it allows one to compare distributions having disjoint supports. 
% 
Computing OT distance is usually performed by sampling once from the input
distributions and solving a discrete linear program (LP), due to~\citet{Kantorovich42}. This approach is numerically costly and
statistically unefficient \cite{weed2019sharp}. The optimisation problem depends
on a fixed sampling of points from the data. It is therefore not adapted to
machine learning setting where data is resampled continuously (e.g. in GANs), or
accessed in an online manner. The goal of this paper is to develop an efficient
online method able to estimate OT distances between continuous distributions. We will use a stream of data to refine an approximate optimal transport solution, adapting the celebrated Sinkhorn algorithm to an online setting.
  


%%%%
To alleviate both the computational and statistical burdens of OT, it is common
to regularize the Kantorovich LP.
%
The most successful approach in this direction is to use an entropic barrier penalty. 
%
When dealing with discrete distributions, this yields a problem that can be solved
numerically using Sinkhorn-Knopp's matrix balancing
algorithm~\cite{Sinkhorn64,sinkhorn1967concerning}.
%
This approach was pushed forward for ML applications by
\citet{cuturi2013sinkhorn}. Sinkhorn distances are smooth and amenable to GPU
computations, which makes them suitable as a loss function in model training.
The Sinkhorn algorithm operates in two distinct phases: draw samples from the
distributions and evaluate a pairwise distance matrix in the first phase;
balance this distance matrix using Sinkhorn-Knopp iterations in the second
phase.

This two-step approach does not estimate the true regularized OT
distance, and cannot handle samples provided as a stream, e.g. renewed at each
training iteration of an outer algorithm. A cheap fix is to use Sinkhorn over
mini-batches (see for instance~\citet{2018-Genevay-aistats} for an application
to GANs). Yet this introduces a strong estimation bias, especially in high
dimension ---see~\citet{fatras2019learning}Â for a mathematical analysis. In
constrast, we use streams of mini-batches to progressively enrich a consistent representation of the
transport plan.

\paragraph{Contributions.} Our paper proposes a new take on estimating optimal transport distances between continuous distributions. We make the following contribution
\begin{itemize}
    \item We introduce an online variant of the Sinkhorn algorithm, that uses
    streams of samples $(x_t)_t$ and $(y_t)_y$ to enrich a non-parametric
    functional representation of the dual regularized optimal transport solution.
    \item We cast online Sinkhorn as an instance of a block-convex stochastic mirror
    descent algorithm. This shows convergence of the algorithm
    with high probability. We analyse special cases of online Sinkhorn.
    \item We demonstrate the performance of online Sinkhorn for estimating OT
    distances between continuous distributions and for accelerating the early phase of discrete Sinkhorn iterations. Comparison with other methods advocates for our original non-parametric representations of OT solutions.
\end{itemize}

\paragraph{Notations.} We denote $\Cc(\Xx)$ the set of continuous functions over
a space $\Xx$, $\Mm^+(\Xx)$ and $\Pp(\Xx)$ the set of positive and probability
measures on $\Xx$, respectively. $\frac{d \mu}{d \alpha}$ denotes the
Radon-Nikodym derivative of measure $\mu$ with respect to measure $\alpha$. We
write $(i, j]$ the sequence $[i+1, \dots, j]$.


% In this paper, we show how mingling together the sampling and the optimization phases can be beneficial
% to quickly estimate OT distances for ML problems.  
% %
% Our approach relies on three observations. First,
% Sinkhorn iterations can be rewritten as a block convex mirror descent on the
% space of positive distributions. This formulation is valid in the discrete and
% continuous setting. Second, we can modify these iterations to rely on
% realizations $\hat \alpha_t$, $\hat \beta_t$ of the input distributions $\alpha$
% and $\beta$, renewed at each iteration $t$. Finally, we can represent the
% iterates produced by such approximations in a space of mixtures of simple
% functions. Those iterates are a simple transformation of the potentials in the
% Sinkhorn optimization problem.
% %
% This corresponds to the following contributions:
% \begin{itemize}
%     \item We introduce a new \textit{online Sinkhorn} algorithm. It produces a sequence of
%     estimates $(\hat w_t)_t \in \RR$ and of transportation plans $\hat \pi_t \in
%     \Pp(\Xx \times \Xx)$ \todo{need to introduce this}, using two stream of renewed samples $\hat \alpha_t =
%     \sum_{i=1}^n \delta_{x^t_i}$, $\hat \beta_t = \sum_{i=1}^n \delta_{y^t_i}$,
%     where $x^t_i$ and $y^t_i$ are sampled from $\alpha$ and $\beta$.
%     \item We show that those estimations are consistent, in the sense that $\hat
%     w_t \to \Ww_{C,\varepsilon}(\alpha, \beta)$, and $\hat \pi_t \to \pi^\star$ \todo{weak convergence?}.
%     \item We empirically demonstrate that our algorithm permits a faster
%     estimation of optimal transportation distances for discrete distributions,
%     and a convincing estimation of OT distances between \textit{continous} distributions.
% \end{itemize}

% % We show how to estimate the true regularized optimal transport distance through repeated experiments.

\section{Related work}\label{sec:related}

We review recent work on Sinkhorn distances and on the general problem of estimating optimal transport distances. 

\paragraph{Sinkhorn properties.} Sinkhorn algorithm computes $\epsilon$-accurate
approximations of OT in $O(n^2/\epsilon^3)$ operations for a number $n$ of
samples~\cite{altschuler2017near} (in contrast to the $O(n^3)$ complexity for an
exact solution). Moreover, these Sinkhorn distances does not suffers from the
curse of dimensionality~\cite{2019-Genevay-aistats}, since the average error
using $n$ random samples decays like $O(\epsilon^{-d/2}/\sqrt{n})$ in dimension
$d$, in sharp contrast with the slow $O(1/n^{1/d})$ error decay of
OT~\cite{dudley_speed_1969,weed2019sharp}. These Sinkhorn distances can furthermore be sharpened
by entropic debiasing~\cite{2019-Feydy-aistats}. Our work is rather orthogonal to these references, as it focuses on estimating distances between continuous distributions.

\paragraph{Continuous optimal transport.} Extending OT computations to arbitrary distributions
(possibly having continuous densities) without relying on a fixed a priori
sampling is an emerging topic of interest. A special case is the semi-discrete
setting, where one of the two distributions is discrete. Without regularization,
over an Euclidean space, this can be solved efficiently using the computation of
Voronoi-like diagrams~\cite{merigot2011multiscale}. This idea can be extended to
entropic-regularized OT~\cite{cuturi2018semidual}, and can also be coupled with
stochastic optimization method~\cite{2016-genevay-nips} to tackle high
dimensional problems (see also~\citet{staib2017parallel} for an extension to Wasserstein barycenters). 

When dealing with arbitrary continuous densities, which are accessed through a
stream of random samples, the challenge is to approximate  the (continuous) dual
variables of the regularized Kantorovich LP using parametric or non-parametric
classes of functions. For application to generative model fitting, one can use
deep networks, which leads to an alternative formulation of Generative
Adversarial Networks (GANs)~\cite{arjovsky2017wgan} (see
also~\citet{seguy2018large} for an extension to the estimation of transportation
maps). There is however no theoretical guarantees for this type of dual
approximations, due to the non-convexity of the resulting optimization problem.
To our knowledge, the only mathematically rigorous algorithm uses reproducing
Hilbert space representations of potentials~\cite{2016-genevay-nips}. As this
construction is generic to all optimisation problems over functions, the convergence is slow. The representations we introduce outperform RKHS representations (\autoref{sec:compare}).