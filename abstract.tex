Optimal Transport (OT) distances are now routinely used as loss functions in ML tasks. Dealing with such losses between arbitrary (i.e. not necessarily discrete) probability distributions is however still mostly an open problem.
%
This paper introduces a new online estimator of optimal transport (OT) distances between two such arbitrary distributions. 
%
Our algorithm solves an entropic regularization of the OT problem and only uses samples streams from the distributions as inputs. 
%
This stream is used to iteratively enrich a non-parametric representation of the transportation plan. 
%
This plan is naturally represented using a mixture of simple kernel functions and has an increasing memory complexity. 
%
Compared to the classic Sinkhorn algorithm, our method leverages new samples at each iteration, which enables a consistent estimation of the true regularized OT distance. 
%
We show that our algorithm can be casted as a block-convex mirror descent in the space of positive distributions, which in turns enables a theoretical analysis of its convergence. 
%
We illustrate numerically the performance of our method in comparison with concurrent approaches.