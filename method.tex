%!TEX root = article.tex

\section{OT distances from sample streams}

We introduce an online adaptation of the Sinkhorn algorithm, the major
contribution of this paper. We construct an estimator of $f^\star$ and $g^\star$
and $\Ww(\alpha,\beta)$ using successive sets of samples $(\hat \alpha_t)_t =
\frac{1}{n} \sum_{i=n_t + 1}^{n_{t+1}} \delta_{x_i}$ and ${(\hat \beta_t)}_t$,
where we set $n_t = nt$. $(\hat \alpha_t)_t$ and $(\hat \beta_t)_t$ may be seen
as mini-batches of size $n$ within a training procedure, or as a temporal stream
of samples.
%
Our estimator progressively enriches a representation of the
solution of~\eqref{eq:dual}, that may be arbitrary complex. 

We detail an intuitive construction of our algorithm in
\autoref{sec-online-sink-iter}, formalize it in \autoref{sec:alg} before casting
it as a block-convex stochastic mirror descent in \ref{sec-mirror}.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Online Sinkhorn iterations}
\label{sec-online-sink-iter}

From \eqref{eq:dual}, along the continuous (and untractable) Sinkhorn
optimisation trajectory ${(\bar f_t, \bar g_t)}_t$, the potential $\bar f_t$ is
always the negative logarithm of an infinite mixture of kernel functions
$\kappa_y(x) \eqdef \exp(-C(\cdot, y))$:

\begin{equation*}
    \exp(-\bar f_t(\cdot)) = 
    \int_{\y \in \Xx} \exp(\bar g_t(y))  \kappa_y  \d \beta(y),
\end{equation*}
and similarly for $\bar g_t$. Our algorithm constructs a sequence of non-parametric
potentials $(f_t, g_t)_t$ that behaves as $(\bar f_t, \bar g_t)$. The strong
structural property of the continuous potentials suggests to express
$\exp(-f_t)$ as a finite mixture of kernel functions. That is, $f_t$ and $g_t$
are continuous functions constructed respectively from the weights ${(p_i,
q_i)}_{i \leq n_t}$ and positions ${(x_i, y_i)}_{i} \subset~\Xx$ as
\begin{align}\label{eq:param}
    f_t(\cdot) &= - \log \sum_{i=1}^{n_t} 
    \exp(q_i - C(\cdot, y_i)),  \\
    g_t(\cdot) &= - \log \sum_{i=1}^{n_t}
    \exp(p_i - C(x_i, \cdot)).
\end{align}

%%%%%
\paragraph{Randomized Sinkhorn.}

Provided with fresh samples $(x_i, y_i)_{n_t < i \leq n_{t+1}}$, 
corresponding to empirical measures $\hat \alpha_t$ and $\hat \beta_t$, a naive approach would 
 update the potentials using a noisy soft $C$-transform:
\begin{equation}\label{eq:updates}
     f_{t+1} = \Ctrans{g_t}{\hat \beta_t},
    \qquad g_{t+1} = \Ctrans{f_{t+1}}{\hat \alpha_t},
\end{equation}
which is equivalent to setting all $(q_i)_{i \leq n_t}$ to $0$, and assigning each weight
 $q_i$ to $g_t(y_i) - \log(n)$ for $n_t < i \leq  n_{t+1}$ and similarly for~$p_i$.
%
The variance of the updates~\eqref{eq:updates} does not decay through the
iteration, hence \textit{random Sinkhorn} algorithm does nos converge.
However, thanks to the contractance of the random operator $\Ctrans{\cdot}{\hat
\beta_t}$ and $\Ctrans{\cdot}{\hat \alpha_t}$, \autoref{prop:markov} shows that
that the Markov chain ${(f_t, g_t)}_t$ that it defines converges towards a
stationary distribution independant of initialization.

%%%%%
\paragraph{Online Sinkhorn.}

To ensure convergence towards the potentials $(f^\star, g^\star)$ (up to a
constant factor), we must therefore take more cautions steps---in other words,
we cannot afford to forget past iterates to obtain a consistent estimation of
potentials. We introduce a learning rate in Sinkhorn iterations, that averages
the past representations and the newly computed noisy $C$-transforms.
\begin{equation}\label{eq:updates}
    e^{-f_{t+1}}
    \eqdef (1 - \eta_t) e^{-f_t} 
    + \eta_t 
    e^{-\Ctrans{g_t}{\hat \beta_t}},
\end{equation}
and similarly for $g_t$. Performing the averaging over the space of inverse
scalings $(e^{-\hat f_{t}},e^{-\hat g_{t}})$ yields simple updates for the
weights ${(p_i,q_i)}_i$, and is crucial for our theoretical convergence
analysis. In essence, the weights of past samples are reduced by a constant
factor, while new weights are computed from the evaluation of $f_t(\cdot),
g_t(\cdot)$ at random new points ${(x_i, y_i)}_i$. Note that we perform
\textit{simultaneous updates} of $f_t$ and $g_t$, which is important for the
convergence analysis.

%%%%%
\paragraph{Estimating Sinkhorn distance.} 

The iterations \eqref{eq:updates} allows to estimate potential functions up to a constant. As explained in~\autoref{sec:gradient},
this estimation is sufficient for most applications aiming at minimizing a Sinkhorn loss, as it only requires the spatial derivatives of the potentials.
%
If required, it is however possible to estimate the Sinkhorn distance using our method, by performing a final soft $C$-transform, using $\Oo(n_T^2)$ operations:
\begin{equation}\label{eq-dist-est}
    \Ww_t = \frac{1}{2}\Big(\dotp{\bar \alpha_T}{f_T + \Ctrans{g_T}{\bar \alpha_T}}
     + \dotp{\bar \beta_T}{g_T + \Ctrans{f_T}{\bar \alpha_T}}\Big),
\end{equation}
where $\bar \alpha_t \eqdef \frac{1}{n_{t+1}}\sum_{i=1}^{n_{t+1}} \delta_{x_i}$
and $\bar \beta_t$ gather all seen samples.

\begin{algorithm}[t]
    \begin{algorithmic}
    \Input Distribution $\alpha$ and $\beta$, learning weights ${(\eta_t)}_t$
    \State Set $p_i = q_i = 0$ for $i \in (0, n_t]$
    \For{$t= 0, \dots, {T-1}$}
        \For{$i = j \in (0, n_t]$}
        \State $q_j \gets q + \log(1 - \eta_t)$, $p_i \gets p + \log(1 - \eta_t)$,
        \EndFor
        \State Sample $(x_i)_{(n_t, n_{t+1}]} \sim \alpha$, $(y_j)_{(n_t, n_{t+1}]} \sim \beta$
        \For{$i \in (n_t, n_{t+1}]$}
            \State $q_i \gets 
            - \log \frac{\eta_t}{n} 
            \sum_{j=1}^{n_t} \exp(\frac{p_j - C(x_j, y_i)}{\varepsilon})$
            \State $p_i \gets 
            - \log \frac{\eta_t}{n} 
            \sum_{j=1}^{n_t} \exp(\frac{q_j - C(x_i, y_j)}{\varepsilon})$
        \EndFor
        \State \textit{Optional}: refit all $q_i = g_t(y_i) - \log (n_{t+1})$
        \State\hspace{2.45cm} $p_i = f_t(x_i) - \log (n_{t+1})$
        \State Save $(q_i, p_i, x_i, y_i)_{(n_t,n_{t+1}]}$
    \EndFor
    \State Returns $f_T : (q_i, y_i)_{(0, n_T]}$ and
    $g_T : (p_i, x_i)_{(0, n_T]}$
    \end{algorithmic}
    \caption{Online Sinkhorn potentials}\label{alg:online_sinkhorn}
\end{algorithm}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Algorithm, complexity and refinements}\label{sec:alg}

The pseudo-code of online Sinkhorn is detailed in \autoref{alg:online_sinkhorn}.
Each iteration has complexity $\Oo(t\,n^2)$, due to the evaluation of the
distances $C(x_i, y_i)$ for all $(x_i)_{(0, n_t]}$, $(y_i)_{(n_t, n_{t+1}]}$
and to the computation of the
soft $C$-transforms. Online Sinkhorn progressively estimates a distance matrix
$(C(x_i,y_j))_{i,j}$ on the fly, in parallel to the update of the potentials
$f_t$ and $g_t$. In total, its computation cost after drawing $n_t$ samples is
$\Oo(n_t^2)$, and its memory cost is $\Oo(n_t)$.
%
We propose some heuristic refinements to accelerate convergence and alleviate the memory and computational cost.


%%%%%
\paragraph{Fully-corrective scheme.} 

The potentials $f_t$ and $g_t$ may be
improved by refitting the weights $(p_i)_{(0, n_t]}$, $(q_j)_{(0, n_t]}$ based
on all previously seen samples. This
amounts to replace, after iteration $t$, for all $i \in (0, n_{t+1}]$,
\begin{align}
    q_i &\gets g_t(y_i) - \log(n_{t+1}) 
    = - \log \frac{1}{n_{t+1}} 
    \sum_{j=1}^{n_{t+1}} e^{p_j - C(x_j, y_i)},
\end{align}
and similarly for each $p_i$. This corresponds to performing one step of the
 discrete Sinkhorn algorithm with distributions $\bar \alpha_t$ and $\bar
 \beta_t$. This reduces the dual cost $F_{ \bar \alpha, \bar \beta}(f_t,
 g_t)$, and on average, the energy $F_{\alpha, \beta}$. This
 reweighted scheme (akin to the fully-corrective Frank-Wolfe scheme \cite{lacoste2015global}) has
 a cost in $\Oo(n_t^2)$. In practice, it can be used only every $k$ iterations, with $k$
 increasing with $t$.

%%%%%
\paragraph{Memory compression.} 

The memory requirement in $\Oo(n_t)$ is an
avoidable limitation of the algorithm, as the optimal
potentials $(f^\star, g^\star)$ do not admit a parametric representation in
general. However, we may compress the representations $(q_j, y_j)$ and $(x_i,
p_i)_i$ using $k$-means clustering over $M$ centroids. The
sampled points $(x_i)_i$ and $(y_j)_j$ are attached to centroids ${(X_I)}_{I \in
(0,M_t]}$ and ${(Y_J)}_{J \in (0,M_t]}$. For all $I \in (0, M_t]$, we set
weights and potentials as
\begin{align}
    Q_J &\gets - \log \sum_{\substack{j,\:y_j \text{ closest}\\\text{to } \bar Y_J}}
     \exp(-q_j),\\
    f_t(\cdot) &\gets - \log\sum_{J=1}^{M_t} \exp(Q_J - C(\cdot, \bar Y_J)),
\end{align}
and similarly for $(p_I)_I$ and $g_t$. Once again, this operation should be made
once every $k$ iterations. We can force $M_t$ to stay constant after linearly increasing in a first stage.

\paragraph{Out-of-loop averaging, finite samples.} Optionally, we may also
compute out-of-loop averages of potentials
\begin{align}
    \exp(-\bar f_{t+1}) = (1 - \frac{1}{t}) \exp(-\bar f_t) + \frac{1}{t} \exp(-\hat f_t), \\
    \exp(-\bar g_{t+1}) = (1 - \frac{1}{t}) \exp(-\bar g_t) + \frac{1}{t} \exp(-\hat g_t),
\end{align}
to further reduce the estimation variance. We show in \autoref{sec:exps} that
this averaging is efficient in practice. Finally, we note that our algorithm
applies on both continuous or discrete distributions. When $\alpha$ and $\beta$
are discrete distributions of size $N$, we can store $p$ and $q$ as fixed-size
vectors of size $N$, and subsample mini-batches of size $n < N$. The resulting
algorithm is a \textit{subsampled} Sinkhorn algorithm for histograms, which is
detailed in the appendix for completeness. We show in \autoref{sec:exps} that it
is useful to accelerate the first phase of the Sinkhorn algorithm.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Stochastic mirror descent interpretation}
\label{sec-mirror}

This online Sinkhorn can be recast as a stochastic mirror descent algorithm, which enables a convergence analysis.
%
This equivalence is obtained by applying a change
of variable in \eqref{eq:wass}, defining 
\begin{equation}\label{eq-change-var}
	\mu \triangleq \alpha \exp(f)
	\qandq 
	\nu \triangleq \beta \exp(g). 
\end{equation}
The dual problem~\eqref{eq:dual} 
rewrites as a minimisation problem over positive measures on $\Xx$ and $\Yy$:
\begin{equation}\label{eq:wass_reparam}
    - \!\!\!\!\min_{(\mu,\nu) \in \Mm^+(\Xx)^2} \!\!\!\KL(\alpha | \mu)
    + \KL(\beta | \nu) + \dotp{\mu \otimes \nu}{e^{-C}} - 1,
\end{equation}
where the function $\KL: \Pp(\Xx) \times \Mm^+(\Xx) \triangleq \dotp{\alpha}{ \log \frac{\d \alpha}{\d \mu}}$ is the Kullback-Leibler divergence between
$\alpha$ and $\mu$. 
%
This objective is block convex in $\mu$, $\nu$, but not jointly convex. 
%
As we now detail, this problem can be solved using a stochastic mirror descent~\cite{beck2003mirror}, applied here over the Banach space of Radon measures on $\Xx$, equipped with the total variation norm. 

%%%
\paragraph{Mirror maps and gradient.}

For this, we define the (convex) distance generating function $\Mm^+(\Xx)^2 \to \RR$:
\begin{equation}
    \omega(\mu, \nu) \triangleq \KL(\alpha | \mu) + \KL(\beta | \nu).
\end{equation}
The gradient of this function and of its Fenchel conjugate $\omega^\star:
\Cc(\Xx)^2 \to \RR$ yields two \textit{mirror maps}. For all $(\mu, \nu) \in
\Mm^+(\Xx)^2$, $(\varrho, \varphi) \in \Cc(\Xx)^2, \varrho > 0, \varphi > 0$,
\begin{equation}
    \nabla \omega(\mu, \nu) = - \frac{\d \alpha}{\d \mu}, 
    \qquad \nabla \omega^\star(\varrho, \varphi) = (-\frac{\alpha}{\varrho}, -\frac{\beta}{\varphi}).
\end{equation}
The gradient $\nabla F(\mu, \nu)$ of the objective $F$ appearing
in~\eqref{eq:wass_reparam} is a continuous function
\begin{equation}
    \nabla_\mu F(\mu, \nu) = \frac{1}{\frac{\d\mu}{\d\alpha}} - \int_{y \in \Xx}
    \frac{\d \nu}{\d \beta}(y) \exp(- C(\cdot, y)) \d \beta(y)
\end{equation}
and similarly for $\nabla_\nu F$.

%%%
\paragraph{Stochastic mirror descent.}

To define stochastic mirror descent iterations, we may replace integration over $\beta$ is by an integration over a sampled measure $\hat \beta$. This in turn defines an \textit{unbiased gradient estimate} $\tilde \nabla F$ of $\nabla F$, which has bounded second order moments.
%
This absence of bias is crucial to prove convergence of SMD with high
probability. Using the mirror maps and the stochastic estimation of the
gradient, one has the following equivalence result, whose proofs stems from
direct computations. 

\begin{proposition}
The stochastic mirror descent iterations
\begin{equation}
    (\mu_t, \nu_t) = \nabla \omega^\star\Big( \nabla \omega(\mu_t, \nu_t) - 
    \eta_t \tilde \nabla F(\mu_t, \nu_t)\Big)
\end{equation}
are equal to the updates~\eqref{eq:updates} under the change of variable~\eqref{eq-change-var}.
\end{proposition}

%%%
\paragraph{Interpretation.} 

It is important to realize that $\mu_t$ and $\nu_t$ does not need to be stored in memory. Instead,
their associated potentials $f_t$ and $g_t$ are parametrized as
\eqref{eq:param}. In particular, $\mu_t$ and $\nu_t$ remains absolutely
continuous with respect to $\alpha$ and $\beta$ respectively, so that the
Kullbach-Leibler divergence terms are always finite. Note that the mirror descent
we consider operates in an infinite-dimensional space.

Finally, we mention that  when computing exact gradient (in the absence of noise) and when using constant step-size of
$\eta_t=1$, the algorithm matches exactly Sinkhorn iterations with simultaneous updates of the dual variable. This provides a novel interpretation on the Sinkhorn algorithm, that differs from the usual Bregman projection \citep{benamou2015iterative}, and the related understanding of Sinkhorn as a constant step-size mirror descent on the primal objective~\cite{mishchenko2019sinkhorn}. 







% Estimator $\dotp{\alpha}{-\log \EE[\exp(-\hat f)]} + \dotp{\beta}{-\log \EE[\exp(-\hat g)]}$

% - estimator properties

% \subsection{Online Sinkhorn convergence}

% - slowed down Sinkhorn convergence

% - Random iterated functions

% - Combining both

% \subsection{Non-convex mirror descent}

% \section{Experiments}

% \subsection{Offline distance averaging}

% \subsection{Online distance computations}

% \subsection{Training generative models}



