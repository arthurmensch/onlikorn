%!TEX root = article.tex

\section{OT distances from sample streams}

We now introduce an online adaptation of the Sinkhorn algorithm, the major
contribution of this paper. It constructs an estimator of
$\Ww(\alpha,\beta)$, $f^\star$ and $g^\star$ using successive sets of samples
$(\hat \alpha_t)_t = \frac{1}{n} \sum_{i=nt}^{n(t+1) - 1} \delta_{x_i}$ and
${(\hat \beta_t)}_t$. 
%
Our estimator progressively enriches a representation of the
solution of~\eqref{eq:dual}, that may be arbitrary complex. $(\hat \alpha_t)_t$
and $(\hat \beta_t)_t$ may be seen as mini-batches of size $n$ within a training procedure,
or as a temporal stream. 

We first introduce in Section~\ref{sec-online-sink-iter} the intuitions behind the construction of our algorithm,
before casting it in Section~\ref{sec-mirror} as a non-convex stochastic mirror descent to establish convergence guaranties. 
%
We set $n_t \eqdef nt$.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Online Sinkhorn iterations}
\label{sec-online-sink-iter}

From \eqref{eq:dual}, along the Sinkhorn optimisation trajectory, the potential $f_t$ is always the negative logarithm an
infinite mixture of kernel functions $\kappa_y(x) \eqdef \exp(-C(\cdot, y))$:
\begin{equation*}
    e^{-f_t(\cdot)} = 
    \int_{\Xx} e^{g_t(y)}  \kappa_y  \d \beta(y),
\end{equation*}
and similarly for $g_t$. Our algorithm constructs a sequence of
non-parametric potentials $(f_t, g_t)_t$ which approximate the continuous
potentials \todo{what are these $f_t,g_t$? do you meant $f^\star$ and $g^\star$?} $g_t$ and $f_t$ in the long run. The strong structural property of
the continuous potentials suggests to express
$\exp(-f_t)$ as a finite mixture of kernel functions.
That is, $f_t$ and $g_t$ are continuous functions constructed
respectively from the weights ${(p_i > 0, q_i > 0)}_{i \leq n_t}$ and
positions ${(x_i, y_i)}_{i} \subset~\Xx$
\begin{align}\label{eq:param}
    f_t(\cdot) &= - \log \sum_{i=1}^{n_t} 
    \exp(q_i - C(\cdot, y_i)),  \\
    g_t(\cdot) &= - \log \sum_{i=1}^{n_t}
    \exp(p_i - C(x_i, \cdot)).
\end{align}

%%%%%
\paragraph{Failure of a randomized Sinkhorn.}

Provided with fresh samples $(x_i, y_i)_{n_t < i \leq n_{t+1}}$, 
corresponding to empirical measures $\hat \alpha_t$ and $\hat \beta_t$, a naive approach would 
 update the potentials using a noisy soft $C$-transform:
\begin{equation}\label{eq:updates}
     f_{t+1} = \Ctrans{g_t}{\hat \beta_t},
    \qquad g_{t+1} = \Ctrans{f_{t+1}}{\hat \alpha_t},
\end{equation}
which is equivalent to setting all $(q_i)_{i \leq n_t}$ to $0$, and assigning each weight
 $q_i$ to $g_t(y_i) - \log(n)$ for $n_t < i \leq  n_{t+1}$ and similarly for~$p_i$.
%
The variance of the updates~\eqref{eq:updates} does not decay through the iteration, so that this
\textit{random Sinkhorn} algorithm does nos converge. However, thanks to the contractance of the random operator $\Ctrans{\cdot}{\hat \beta_t}$ and $\Ctrans{\cdot}{\hat \alpha_t}$, \autoref{prop:markov} shows that that the Markov chain ${(f_t,
g_t)}_t$ that it defines converges towards a stationary distribution independant
of initialization.

%%%%%
\paragraph{Online Sinkhorn.}

To ensure convergence towards the potentials $(f^\star, g^\star)$ (up to a
constant factor), we must therefore take more cautions steps---in other words,
we cannot afford to forget past iterates to obtain a consistent estimation of
potentials. We introduce a learning rate in Sinkhorn iterations, that averages
the past representations and the newly computed noisy $C$-transforms.
\begin{equation}\label{eq:updates}
    e^{-f_{t+1}}
    \eqdef (1 - \eta_t) e^{-f_t} 
    + \eta_t 
    e^{-\Ctrans{g_t}{\hat \beta_t}},
\end{equation}
and similarly for $g_t$. Performing the averaging over the space of inverse scalings
$(e^{-\hat f_{t}},e^{-\hat g_{t}})$ yields simple updates for the weights
${(p_i,q_i)}_i$, and is crucial for our theoretical convergence analysis. In essence, the
weights of past samples are reduced by a constant factor, while new weights are
computed from the evaluation of $g_t(\cdot)$ at random new points ${(x_j, y_j)}_j$. Note that we
perform \textit{simultaneous updates} of $f_t$ and $g_t$, which is once again important for the convergence analysis.

%%%%%
\paragraph{Estimating the distance.} 

This approach allows us to estimate potential functions up to a constant. As explained in~\autoref{sec:gradient},
this estimation is sufficient for most applications aiming at minimizing a Sinkhorn loss, as it only requires the spatial derivatives of the potentials~\eqref{eq-grad-backprop}. 
%
If required, it is however possible to estimate online the Sinkhorn distance  by performing a final soft $C$-transform, using $\Oo(n_t^2)$ operations:
\begin{equation}
    \Ww(\alpha,\beta) \approx \frac{1}{2}\Big(\dotp{\bar \alpha_t}{f_t + \Ctrans{g_t}{\bar \alpha_t}}
     + \dotp{\bar \beta_t}{g_t + \Ctrans{f_t}{\bar \alpha_t}}\Big),
\end{equation}
where $\bar \alpha_t \eqdef \frac{1}{n_{t+1}}\sum_{i=1}^{n_{t+1}} \delta_{x_i}$
gathers all previously seen samples, and similarly for $\bar \beta_t$.

\begin{algorithm}[t]
    \begin{algorithmic}
    \Input Distribution $\alpha$ and $\beta$, learning weights ${(\eta_t)}_t$
    \State Set $p_i = q_j = 0$ for $i = j \in (0, n_t]$
    \For{$t= 0, \dots, {T-1}$}
        \For{$i = j \in (0, n_t]$}
        \State $q_j \gets q + \log(1 - \eta_t)$, $p_i \gets p + \log(1 - \eta_t)$,
        \EndFor
        \State Sample $(x_i)_{(n_t, n_{t+1}]} \sim \alpha$, $(y_j)_{(n_t, n_{t+1}]} \sim \beta$
        \For{$i = j \in (n_t, n_{t+1}]$}\todo{the mixing of $i$ and $j$ is not clear}
            \State $q_j \gets 
            - \log \frac{\eta_t}{n} 
            \sum_{i=1}^{n_t} \exp(\frac{p_i - C(x_i, y_j)}{\varepsilon})$
            \State $p_i \gets 
            - \log \frac{\eta_t}{n} 
            \sum_{j=1}^{n_t} \exp(\frac{q_j - C(x_i, y_j)}{\varepsilon})$
        \EndFor
        \State \textit{Optional}: refit all $q_j = g_t(y_j) - \log (n_{t+1})$
        \State\hspace{2.45cm} $p_i = f_t(x_i) - \log (n_{t+1})$
        \State Save $(q_j, y_j)_{(n_t,n_{t+1}]}$ and $(x_i, x_i)_{(n_t,n_{t+1}]}$
    \EndFor
    \State Returns $f_T : (q_j, y_j)_{(0, n_T]}$ and
    $g_T : (p_i, x_i)_{(0, n_T]}$
    \end{algorithmic}
    \caption{Online Sinkhorn potentials}\label{alg:online_sinkhorn}
\end{algorithm}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Algorithm, complexity and refinements}

The pseudo-code of online Sinkhorn is detailed in \autoref{alg:online_sinkhorn}.
Each iteration has complexity $\Oo(t\,n^2)$, due to the evaluation of
the distances $C(x_i, y_j)$ for all ???\todo{say here which $i,j$ are concerned (pseudo code seems weird also)}, and to the computation of the $C$-transforms. This online Sinkhorn progressively estimate a distance matrix $(C(x_i,y_j))_{i,j}$ on the fly,
in parallel to the update of the potentials $f_t$ and $g_t$. In total, its computation cost
after drawing $n_t$ samples is $\Oo(n_t^2)$, and its memory cost is $\Oo(n_t)$.
%
We now propose some heuristic refinements to accelerate convergence and alleviate computational cost.


%%%%%
\paragraph{Fully-corrective scheme.} 

The potentials $f_t$ and $g_t$ may be
improved by refitting the weights $(p_i)_{(0, n_t]}$, $(q_j)_{(0, n_t]}$ based
on all previously seen samples. This
amounts to replace, after iteration $t$, for all $j \in (0, n_{t+1}]$
\begin{align}
    q_j &\gets g_t(y_j) - \log(n_{t+1}) 
    = - \log \frac{1}{n_{t+1}} 
    \sum_{i=1}^{n_{t+1}} e^{p_i - C(x_i, y_j)},
\end{align}
and similarly for each $p_i$. This corresponds exactly to performing one step of the
 discrete Sinkhorn algorithm with distributions $\bar \alpha_t$ and $\bar
 \beta_t$. This reduces the dual cost $F_{ \bar \alpha, \bar \beta}(f_t,
 g_t)$, and on average, the energy $F_{\alpha, \beta}$. This
 reweighted scheme (akin to the fully-corrective Frank-Wolfe scheme \cite{lacoste2015global}) has
 a cost in $\Oo(n_t^2)$. In practice, it can be used only every $k$ iterations, with $k$ linearly
 increasing with $t$.

%%%%%
\paragraph{Memory compression.} 

The memory requirement in $\Oo(N)$ is an
avoidable limitation of the algorithm, which cannot be avoided sas the optimal
potentials $(f^\star, g^\star)$ do not admit a parametric representation in
general. However, we may compress the representations $(q_j, y_j)$ and $(x_i,
p_i)_i$, We propose to perform $k$-means clustering over $M$ centroids. The
sampled points $(x_i)_i$ and $(y_j)_j$ are attached to centroids ${(X_I)}_{I \in
[0,M_t]}$ and ${(Y_J)}_{J \in [0,M_t]}$. For all $I \in (0, M_t]$, we set
weights and potentials as
\begin{align}
    Q_J &\gets - \log \sum_{\substack{j,\:y_j \text{ closest}\\\text{to } \bar Y_J}}
     \exp(-q_j),\\
    f_t(\cdot) &\gets - \log\sum_{J=1}^{M_t} \exp(Q_J - C(\cdot, \bar Y_J)),
\end{align}
and similarly for $(p_I)_I$ and $g_t$. Once again, this operation should be made
once every $k$ iterations. We can force $M_t$ to stay constant after linearly increase it.

\paragraph{Out-of-loop averaging, finite samples.} Optionally, we may also
compute out-of-loop averages of potentials
\begin{align}
    \exp(-\bar f_{t+1}) = (1 - \frac{1}{t}) \exp(-\bar f_t) + \frac{1}{t} \exp(-\hat f_t), \\
    \exp(-\bar g_{t+1}) = (1 - \frac{1}{t}) \exp(-\bar g_t) + \frac{1}{t} \exp(-\hat g_t),
\end{align}
to further reduce the estimation variance. We show in Section~\ref{} \todo{where?} that this averaging is
efficient in practice. Finally, we note that our algorithm of course applies on both continuous or discrete distributions. In the discrete case, one can \todo{you mean store the full vector?} tabulate the vectors $p$ and $q$. The resulting
algorithm is a \textit{subsampled} Sinkhorn algorithm for histograms, which is detailed in the appendix for completeness.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Non-convex stochastic mirror descent interpretation}
\label{sec-mirror}

This online Sinkhorn can be recast as a stochastic mirror descent algorithm, which enables a convergence analysis.
%
This equivalence is obtained by applying a change
of variable in \eqref{eq:wass}, defining 
\begin{equation}\label{eq-change-var}
	\mu \triangleq \alpha \exp(f)
	\qandq 
	\nu \triangleq \beta \exp(g). 
\end{equation}
The dual problem~\eqref{eq:dual} 
rewrites as a minimisation problem over positive measures on $\Xx$ and $\Yy$:
\begin{equation}\label{eq:wass_reparam}
    - \!\!\!\!\min_{(\mu,\nu) \in \Mm^+(\Xx)^2} \!\!\!\KL(\alpha | \mu)
    + \KL(\beta | \nu) + \dotp{\mu \otimes \nu}{e^{-C}} - 1,
\end{equation}
where the function $\KL: \Pp(\Xx) \times \Mm^+(\Xx) \triangleq \dotp{\alpha}{ \log \frac{\d \alpha}{\d \mu}}$ is the Kullback-Leibler divergence between
$\alpha$ and $\mu$. 
%
This objective is block convex in $\mu$, $\nu$, but not jointly convex. 
%
As we now detail, this problem can be solved using a stochastic mirror descent~\cite{beck2003mirror}, applied here over the Banach space of Radon measure on $\Xx$, equipped with the total variation norm. 

%%%
\paragraph{Mirror maps.}

For this, we define the (convex) distance generating function $\Mm^+(\Xx)^2 \to \RR$:
\begin{equation}
    \omega(\mu, \nu) \triangleq \KL(\alpha | \mu) + \KL(\beta | \nu).
\end{equation}
The gradient of this function and of its Fenchel conjugate $\omega^\star: \Cc(\Xx)^2 \to \RR$ yields two \textit{mirror maps}. For all $(\mu, \nu) \in
\Mm^+(\Xx)^2$, $(p, q) \in \Cc(\Xx)^2$, \todo{Shouldn't $p,q$ be $>0$?} \todo{shouldn't there be a second component in $ \nabla \omega(\mu, \nu)$?}
\begin{equation}
    \nabla \omega(\mu, \nu) = - \frac{\d \alpha}{\d \mu}, 
    \qquad \nabla \omega^\star(p, q) = (-\frac{\alpha}{p}, -\frac{\beta}{q}).
\end{equation}
The gradient $\nabla F(\mu, \nu)$ (according to the duality pairing with continuous functions) of the objective $F$ appearing in~\eqref{eq:wass_reparam} reads\todo{Wouldn't it make more sense to write the formula bellow using only $\mu,\nu$ and not $f,g$?}
\begin{equation}
    \nabla_\mu F(\mu, \nu) = e^{-f} - 
    \int_{x \in \Xx} \!\!\! \exp(g(y) - C(\cdot, y))\d \beta(y),
\end{equation}
and similarly for $\nabla_\nu F$, where \todo{did not understand what are $f,g$} $f$ and $g$ are the potentials associated
to $f$ and $g$. 

%%%
\paragraph{Stochastic mirror descent.}

To define stochastic mirror descent iterations, $\beta$ is- replaced by random samples $\hat \beta$, which in turn defines an \textit{unbiased gradient estimate} $\tilde \nabla F$ of $\nabla F$, which has bounded second order moments.
%
This absence of bias is crucial to prove convergence of the algorithm (with high
probability) \todo{put ref where this is stated/proved}. This proof relies on the fact that these gradients belong to \todo{in what sense this is a space? I feel this discussion is too difficult to follow and maybe not useful} the space of \todo{typo??} $\exp(-f),
\exp(-f)$, in which the soft $C$-transform is a simple expectation. 
%
Using these mirror maps and this stochastic estimation of the gradient, one has the following equivalence result, whose proofs follows from direct computations. 

\begin{proposition}
The stochastic mirror descent iterations
\begin{equation}
    (\mu_t, \nu_t) = \nabla \omega^\star\Big( \nabla \omega(\mu_t, \nu_t) - 
    \eta_t \tilde \nabla F(\mu_t, \nu_t)\Big)
\end{equation}
are equal to the updates~\eqref{eq:updates} under the change of variable~\eqref{eq-change-var}.
\end{proposition}

%%%
\paragraph{Interpretation.} 

It is important to realize that $\mu_t$ and $\nu_t$ does not need to be stored in memory. Instead,
their associated potentials $f_t$ and $g_t$ are parametrized as
\eqref{eq:param}. In particular, $\mu_t$ and $\nu_t$ remains absolutely
continuous with respect to $\alpha$ and $\beta$ respectively, so that the
Kullbach-Leibler divergence terms are always finite. Note that this corresponds to a mirror descent
in an infinite-dimensional spaces, which is valid as $F$ is relatively 1-smooth with respect to
$\omega$ \todo{what does this mean?}.

Finally, we mention that  when computing exact gradient (in the absence of noise) and when using constant step-size of
$\eta_t=1$, the algorithm matches exactly Sinkhorn iterations with simultaneous updates of the dual variable. This provides a novel interpretation on the Sinkhorn algorithm, which is different from the usual Bregman projection and mirror descent interpretations \citep{benamou2015iterative}, which is crucial to be adapted to an online stochastic setting~\cite{mishchenko2019sinkhorn}. 







% Estimator $\dotp{\alpha}{-\log \EE[\exp(-\hat f)]} + \dotp{\beta}{-\log \EE[\exp(-\hat g)]}$

% - estimator properties

% \subsection{Online Sinkhorn convergence}

% - slowed down Sinkhorn convergence

% - Random iterated functions

% - Combining both

% \subsection{Non-convex mirror descent}

% \section{Experiments}

% \subsection{Offline distance averaging}

% \subsection{Online distance computations}

% \subsection{Training generative models}



