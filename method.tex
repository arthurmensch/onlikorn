%!TEX root = article.tex

\section{OT distances from sample streams}

We introduce an online adapation of the Sinkhorn algorithm in this section. We
wish to construct an estimator of $\Ww(\alpha,\beta)$ from multiple sets of
samples $(\hat \alpha^t)_t = \frac{1}{n} \sum_{i=1} \delta_{x_i^t}$
 and $(\hat \beta^t)_t$. This estimator should
successively use these samples to enrich a representation of the solution of
\eqref{eq:dual}, that may be arbitrary complex. $(\hat \alpha_n^t)_t$ and $(\hat
\beta_n^t)_t$ may be seen as mini-batches within a training procedure, or as a
temporal stream. We first introduce the intuitions behind the construction of
our algorithm, before casting it as a non-convex stochastic mirror descent
problem.

\subsection{Online Sinkhorn iterations}

From \eqref{eq:dual}, along the Sinkhorn optimisation trajectory, the potential $f_t$ is always the negative logarithm an
infinite mixture of kernel functions $\kappa_y: x \to \exp(-\frac{C(\cdot, y)}{\varepsilon})$:
\begin{equation}
    \exp(-\frac{f_t(\cdot)}{\varepsilon}) = 
    \int_{y \in \Yy} \exp(g_t(y))  \exp(-\frac{C(\cdot, y)}{\epsilon}) \d \beta(y),
\end{equation}
and similarly for $g_t$. Our algorithm will construct a sequence $(\hat f_t,
\hat g_t)$ that behaves like $g_t$ and $f_t$ in the long run. The strong
structural property of the continuous potentials suggests to express $\exp(-\frac{\hat f_t(\cdot)}{\varepsilon})$ as a
finite mixture of kernel functions. That is, $\hat f_t$ and $\hat g_t$ are continuous
functions constructed from the weights ${(p_i^t)}_{i,s}, {(q_i^s)}_{i,s} > 0$ and positions
${(y_i^s)}_{i,t} \in~\Yy, {(x_i^s)}_{i,s\leq t} \in~\Xx$:
\begin{align}
    \hat f_t(\cdot) &= - \log \sum_{s=1}^t \sum_{i=1}^{n_t} 
    \exp(\frac{p_i^s - C(\cdot, y_i^s)}{\varepsilon}) \\
    \hat g_t(\cdot) &= - \log \sum_{i=1}^{n_t} 
    \exp(\frac{q_i^s - C(x_i^s, \cdot)}{\varepsilon}).
\end{align}

Provided with fresh
 samples $(x_i^t)_i$ and $(y_i^t)_i$, a naive approach updates the potentials using a noisy soft C-transform:

\begin{equation}
    \hat f_{t+1} = T(g_t, \hat \beta_t),\qquad g_{t+1} = T(\hat f_{t+1}, \hat \alpha_t),
\end{equation}
which translates into setting all $(p_i^s)_{s \leq 0}$ to 0, and to set each weight
 $p_i^{t+1} = T(g_t, \hat \beta_t)(x_i^{t+1}) / n_t$.

As the variance of the updates does not reduce, this algorithm does nos
converge. It is however possible to show, through contractance of the random
operator $T(\cdot, \hat \beta)$, that the Markov chain $(f_t, g_t)$ that it
defines follows at the limit a stationary distribution that does not depend on initialisation.

The variance introduced must therefore be reduced in some way---in other words, we cannot afford to forget past iterates to obtain a consistent estimation of potentials. We introduce a learning rate in Sinkhorn iterations, that averages the past representations and the newly computed noisy C-transforms.

\begin{align}
    \exp(-\hat f_{t+1}) = (1 - \eta_t) \exp(-\hat f_t) + \eta_t T(g_t, \hat \beta_t), \\
    \exp(-\hat g_{t+1}) = (1 - \eta_t) \exp(-\hat g_t) + \eta_t T(f_t, \hat \alpha_t).
\end{align}

Performing the averaging in the space of $\exp(-\hat f_{t})$ yields simple
updates for the weights $(p_i^s)_s$, and is crucial for our theoretical
understanding. In essence, the weights of past samples are reduced by a constant
factor, while new weights are computed from the soft c-transform of $g_t(\cdot)$ at
points $y_i^t$. Note that we perform \textit{simultenous updates} of $f_t$ and
$g_t$, once again from theoretical considerations.

\subsection{Algorithm and complexity}

We provide the pseudo code for online Sinkhorn in \autoref{alg:online_sinkhorn}.
The iteration $t$ has a complexity in $\Oo(t\,n_t^2)$, due to the evaluation of
the distances $C(x_i^t, y_i^s)$ for all $s < t$, and to the computation of
the C-transforms. Online Sinkhorn constructs the distance matrix $C$ on the
fly, parallel to updating the potentials $f$ and $g$. In total, its computation
cost after seing $n$ samples is $\Oo(n^2)$, and its memory cost is $\Oo(n)$.


\paragraph{Refinements.} Optionally, we may also
compute out-of-loop averages of potentials
\begin{align}
    \exp(-\bar f_{t+1}) = (1 - \gamma_t) \exp(-\bar f_t) + \gamma_t \exp(-\hat f_t), \\
    \exp(-\bar g_{t+1}) = (1 - \gamma_t) \exp(-\bar g_t) + \gamma_t \exp(-\hat g_t),
\end{align}
to further reduce the estimation variance for an unchanged complexity.

\begin{algorithm}[t]
    \begin{algorithmic}
    \Input Distribution $\alpha$ and $\beta$, $\varepsilon$, $\eta_t$, $\gamma_t$
    \State Initialize $\Pp = [\:]$, $\Qq = [\:]$, $\Xx = [\:]$, $\Yy = [\:]$
    \For{$t =0, \dots, T$}
        \For{$p \in \Pp$, $q \in \Qq$}
            \State $p \gets p + \varepsilon \log(1 - \eta_t)$,
             \State $q \gets q + \varepsilon \log(1 - \eta_t)$
        \EndFor
        \State Sample $(x_i^t)_{i=1,n} \sim \alpha$, $(y_i^t)_{i=1,n} \sim \beta$
        \For{$y \in y_1^t, \dots,y_n^t$, $x \in x_1^t, \dots,x_n^t$}
            \State $p_i^t \gets 
            - \varepsilon \log \frac{\eta_t}{n_t} 
            \sum_{p \in \Pp, y \in \Yy} \exp(\frac{p - C(x_i^t, y)}{\varepsilon})$
            \State $q_i^t \gets 
            - \varepsilon \log \frac{\eta_t}{n_t} 
            \sum_{p \in \Pp, x \in \Xx} \exp(\frac{q - C(x, y_i^t)}{\varepsilon})$
        \EndFor
        \State Add $(x_i^t)_i$ to $\Xx$, $(y_i^t)_i$ to $\Yy$
        \State Add $(q_i^t)_i$ to $\Qq$, $(p_i^t)_i$ to $\Pp$
    \EndFor
    \State Returns representations of $f_T: (\Yy, \Pp)$, $g_T: (\Xx, \Qq)$, 
    \end{algorithmic}
    \caption{Online Sinkhorn}\label{alg:online_sinkhorn}
\end{algorithm}


% Estimator $\dotp{\alpha}{-\log \EE[\exp(-\hat f)]} + \dotp{\beta}{-\log \EE[\exp(-\hat g)]}$

% - estimator properties

% \subsection{Online Sinkhorn convergence}

% - slowed down Sinkhorn convergence

% - Random iterated functions

% - Combining both

% \subsection{Non-convex mirror descent}

% \section{Experiments}

% \subsection{Offline distance averaging}

% \subsection{Online distance computations}

% \subsection{Training generative models}



