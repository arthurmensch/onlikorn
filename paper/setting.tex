%!TEX root = article.tex
\section{Background: optimal transport distances}

We first recall the definition of optimal transport distances between arbitrary distributions (i.e. not necessarily discrete), then review how these are estimated using a finite number of samples.

\subsection{Optimal transport distances and algorithms}

\paragraph{Wasserstein distances.} 

We consider a complete metric space $(\Xx,d)$ (assumed to be compact for simplicity), equipped
with a continuous cost function $C(x,y) \in \RR$ for any $(x,y) \in \Xx^2$ (assumed to be symmetric also for simplicity). 
%
Optimal transport lifts this \textit{ground cost} into a cost between probability
distributions over the space $\Xx$. 
%
The Wasserstein cost between two probability distributions $(\alpha, \beta) \in \Pp(\Xx)^2$ is defined as the minimal cost required to move each element of mass of $\alpha$ to each element of mass of $\beta$. It rewrites as the solution of a
linear problem (LP) over the set of transportation plans (which are probability distribution $\pi$ over $\Xx \times \Xx$):
\begin{equation}\label{eq:wass_0}
    \Ww_{C,0}(\alpha, \beta) \eqdef 
    \min_{\pi \in \Pp(\Xx^2)}
    \enscond{
    	\dotp{C}{\pi}
	}{ \pi_1=\al, \pi_2=\be},
\end{equation}
where we denote $\dotp{C}{\pi} \eqdef \int C(x,y) \d\pi(x,y)$. Here, $\pi_1 =
\int_{y\in \Xx} \d \pi(\cdot, y)$ and $\pi_2 = \int_{x\in \Xx} \d \pi(x, \cdot)$
are the first and second marginals of the transportation plan $\pi$. We refer to
\cite{santambrogio2015optimal} for a review on OT.
%
% When $C=d^p(x,y)$ is the $p^{\text{th}}$ power of the ground distance, with $p
% \geq 1$, then $\Ww_{C,0}^{1/p}$ is itself a distance over $\Pp(\Xx)$, whose
% associated topology is the one of the convergence in
% law~.

\paragraph{Entropic regularization and Sinkhorn algorithm.} 

The solutions of~\eqref{eq:wass} can be approximated by a strictly convex optimisation problem, where an entropic term is added to the linear objective to force curvature. The so-called Sinkhorn cost is then
\begin{equation}\label{eq:wass}
    \Ww_{C,\varepsilon}(\alpha, \beta) \eqdef 
    \min_{
    \substack{
        \pi \in \Pp(\Xx \times \Xx)
        \\\pi_1 = \alpha, \pi_2 = \beta
    }    
    } \dotp{C}{\pi} + \varepsilon \KL(\pi | \alpha \otimes \beta),
\end{equation}
where the Kulback-Leibler divergence is defined as $\KL(\pi | \alpha \otimes
\beta) \eqdef \int \log(\frac{\d\pi}{\d\al\d\be}) \d\pi$ (which is thus equal to
the mutual information of $\pi$).
%
$\Ww_{C,\varepsilon}$ approximates $\Ww_{C,0}(\alpha,\beta)$ up to an $\epsilon
\log(\epsilon)$ error \citep{2019-Genevay-aistats}. In the following, we set
$\varepsilon$ to $1$ without loss of generality, as $\Ww_{C, \varepsilon} =
\epsilon \Ww_{C / \varepsilon, 1}$, and simply write $\Ww$.
%
\eqref{eq:wass} admits a dual form, which is a maximization problem over the space of continuous functions:
\begin{equation}\label{eq:dual}
    F_{\alpha, \beta}(f, g) \eqdef \max_{(f, g) \in \Cc(\Xx)^2} \dotp{f}{\alpha} + 
    \dotp{g}{\beta}
    - \dotp{e^{f \oplus g - C}}{\alpha \otimes \beta} + 1, 
\end{equation}
where $\dotp{f}{\alpha} \eqdef \int f(x) \d\al(x)$ and $(f \oplus g - C)(x,y)
\eqdef f(x)+g(y)-C(x,y)$.
%  We write $F_{\alpha, \beta}(f, g)$ the dual objective. 
Problem \eqref{eq:dual} can be solved by close-form alternated maximization. At iteration $t$, the updates are simply
\begin{equation}
    f_{t+1}(\cdot) = \Ctrans{g_t}{\beta}, \quad
    g_{t+1}(\cdot) = \Ctrans{f_{t+1}}{\alpha},\quad
    \Ctrans{h}{\mu} \triangleq 
    - \log \int_{y \in \Xx}\!\! \exp(h(y) - C(\cdot, y))\d \mu(y).\label{eq:sinkhorn}
\end{equation}
The operation $h \mapsto \Ctrans{h}{\mu}$  maps a continuous function to another
continuous function, and is a smooth approximation of the celebrated
$C$-transform of OT~\citep{santambrogio2015optimal}. We thus refer to it as a
\textit{soft C-transform}. Note that we consider \textit{simultaneous} updates
of $f_t$ and $g_t$ in this paper, as it simplifies our analysis.
%
The notation $f_t(\cdot)$ emphasizes the fact that $f_t$ and $g_t$ are
\textit{functions}. 
%
% In the discrete setting, iterations 

It can be shown that ${(f_t)}_t$ and ${(g_t)}_t$ converge in $(\Cc(\Xx),
\norm{\cdot}_{\text{var}})$ to a solution $(f^\star, g^\star)$ of
\eqref{eq:dual}, where $\norm{f}_{\text{var}} \eqdef \max_x f(x) - \min_x f(x)$
is the so-called variation norm. Functions endowed with this norm are only
considered up to an additive constant.  Global convergence is due to the strict
contraction of the operators $\Ctrans{\cdot}{\beta}$ and
$\Ctrans{\cdot}{\alpha}$ in the space $(\Cc(\Xx), \norm{\cdot}_{\text{var}})$
\citep{lemmens_nonlinear_2012}.

\subsection{Estimating OT distances with realizations}

When the input distributions are discrete (or equivalently when $\Xx$ is a
finite set), i.e. $\alpha = \frac{1}{n}\sum_{i=1}^n \delta_{x_i}$ and $\beta =
\frac{1}{n} \sum_{i=1}^n \delta_{y_i}$, the function $f_t$ and $g_t$ need only
to be evaluated on $(x_i)_t$ and $(y_i)_i$, which allows a proper
implementation. The iterations~\eqref{eq:sinkhorn} then correspond to the
\citet{sinkhorn1967concerning} algorithm over the inverse scaling vectors $\u_t
\triangleq {(e^{-f_t(x_i)})}_{i=1}^n, \v_t \triangleq
{(e^{-g_t(y_i)})}_{i=1}^n$:
\begin{equation*}
	\u_{t+1} = \K \frac{1}{n \v_t}
	\qandq
	\v_{t+1} = \K^\top \frac{1}{n \u_t}
\end{equation*}
where $\K=(e^{-C(x_i,y_i)})_{i,j=1}^n \in \RR^{n \times n}$, and inversion is made pointwise. The Sinkhorn algorithm for OT thus
operates in two phases: first, the kernel matrix $\K$ is computed, with a cost in
$O(n^2 d)$, where $d$ is the dimension of $\Xx$; second, $\K$ is balanced, each
iteration costing $O(n^2)$. The online Sinkhorn algorithm that we propose mixes
these two phases to accelerate convergence (see results in \autoref{sec:accelerating}).

% The goal of this paper is to go beyond this discrete setting, and handle generic distributions (possibly having continuous densities). In particular, our numerical scheme manipulates continuous functions though an adapted parameterization which is automatically refined during the iterations.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\paragraph{Consistency and bias.}\label{sec:gradient}

The OT distance $\Ww_{C,0}(\alpha,\beta)$ and its regularized version
$\Ww_{C,\epsilon}(\alpha,\beta)$ can be approximated by the (computable)
distance between discrete realizations $\hat \alpha = \frac{1}{n} \sum_i
\delta_{x_i}$, $\hat \beta = \frac{1}{n} \sum_i \delta_{y_i}$, where ${(x_i)}_i$
and ${(y_i)}_i$ are i.i.d samples from $\alpha$ and $\beta$.  Consistency holds,
as $\Ww(\hat \alpha_n, \hat \beta_n) \to \Ww(\alpha,
\beta)$. Although this is a reassuring result, the sample complexity of
transport in high dimensions with low regularization remains high (see
\autoref{sec:related}).
%  For computational reasons, we cannot choose $n$ to be
% much more than $10^5$. 


The estimation of $\Ww(\alpha,\beta)$ may be improved using several i.i.d sets
of samples $(\hat \alpha_t)_t$ and ${(\hat \beta_t)}_t$. Those should be of
reasonable size to fit in memory and may for example come from a temporal
stream. \cite{2018-Genevay-aistats} use a Monte-Carlo estimate $\hat \Ww(\alpha,
\beta) = \frac{1}{T} \sum_{t=1}^T \Ww(\hat \alpha_t, \hat \beta_t)$. However,
this yields a biased estimation as the distance $\Ww(\alpha, \beta)$ and the
optimal potentials $f^\star=f^\star(\alpha, \beta)$ differ from their
expectation under sampling $\EE_{\hat \alpha \sim \alpha, \hat \beta \sim \beta}
[\Ww(\hat \alpha, \hat \beta)]$ and $\EE_{\hat \alpha \sim \alpha, \hat \beta
\sim \beta}[f^\star(\hat \alpha, \hat \beta)]$. In contrast, online Sinkhorn
consistently estimates the true potential functions (up to a constant) and the
Sinkhorn cost.