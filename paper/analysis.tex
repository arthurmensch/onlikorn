%!TEX root = article.tex


\section{Convergence analysis}\label{sec:analysis}

We show a
stationary distribution convergence property for the randomized Sinkhorn algorithm,
 an approximate convergence property for the online Sinkhorn algorithm with
fixed batch-size and an exact convergence result for online Sinkhorn with
increasing batch sizes, with asymptotic convergence rates. We make the following classical assumption on the cost regularity and compactness of $\alpha$ and $\beta$.

\begin{assumption}\label{ass:lip}
    The cost $C: \Xx \times \Xx \to \RR$ is $L$-Lipschitz, and $\Xx$ is  compact.
\end{assumption}

\subsection{Randomized Sinkhorn}

We first state a result concerning the randomized Sinkhorn algorithm~\eqref{eq:updates_naive}, proved in \autoref{sec:proof_markov}.

\begin{proposition}\label{prop:markov}
    Under \autoref{ass:lip}, the randomized Sinkhorn algorithm \eqref{eq:updates_naive} yields a time-homogeneous
    Markov chain ${(\hat f_t, \hat g_t)}_t$ which is $(\hat \alpha_s, \hat \beta_s)_{s \leq
    t}$ measurable, and converges in law towards a stationary distribution
    $(f_\infty, g_\infty) \in \Pp(\Cc(\Xx)^2)$ independent of the initialization
    point $(f_0, g_0)$.
\end{proposition}

This result follows from \citet{diaconis_iterated} convergence theorem on
iterated random functions which are contracting on average. We use the
fact that $\Ctrans{\cdot}{\hat \beta}$ and $\Ctrans{\cdot}{\hat \alpha}$ are
\textit{uniformly} contracting, independently of the distributions $\hat \alpha$ and
$\hat \beta$, for the variational norm $\Vert \cdot \Vert_{\text{var}}$.
Using the law of large number for Markov chains
\citep{breiman_strong_1960}, the (tractable) average $\frac{1}{t} \sum_{s=1}^t \exp(-\bar f_s)$
converges almost surely to $\EE[e^{-f_\infty}]
 \in \Cc(\Xx)$. This expectation verifies the functional equations
\begin{equation}
    \EE[e^{-f_\infty}] =
     \int_{y} \EE[e^{g_\infty(y) -C(\cdot, y)}]\d \beta(y) 
     \quad
    \EE[e^{-g_\infty}] =
    \int_{x} \EE[e^{f_\infty(x) -C(x, \cdot)}]\d \alpha(x) 
\end{equation}
These equations are close to the Sinkhorn fixed point equations, and
get closer as $\varepsilon$ increases, since $\varepsilon \EE[\exp(\pm f_\infty /
\varepsilon)] \to \EE[\pm f_\infty]$ as $\varepsilon \to \infty$. Running the random
Sinkhorn algorithm with averaging fails to provide exactly the dual solution, but solves an approximate problem.
%
% We leave the quantification of this approximation for future work.

%%%
\subsection{Online Sinkhorn}

We make the following \citet{robbins1951stochastic} assumption on the weight sequence. We then state an approximate convergence result for the online Sinkhorn algorithm with fixed batch-size $n(t) = n$.

\begin{assumption}\label{ass:weights}
    The sequence ${(\eta_t)}_t$ is such that
    $\sum \eta_t = \infty$ and $\sum \eta_t^2 < \infty$.
\end{assumption}

\begin{proposition}\label{prop:convergence_approx}
    Under \autoref{ass:lip} and \ref{ass:weights}, the online Sinkhorn algorithm (\autoref{alg:online_sinkhorn}) yields a sequence $(f_t, g_t)$ that reaches a
    ball centered around $f^\star, g^\star$ for the variational norm $\Vert
    \cdot \Vert_{\var}$.
     Namely, there exists $T > 0$, $A > 0$ such that for all $t > T$, almost surely
    \begin{equation}
        \Vert f_t - f^\star \Vert_{\var}
        + \Vert g_t - g^\star \Vert_{\var} 
        \leq \frac{A}{\sqrt{n}} .
    \end{equation}
\end{proposition}

The proof is reported in \autoref{sec:proof_convergence_approx}. It is not possible
to ensure the convergence of online Sinkhorn with constant batch-size. This is a
fundamental difference with other SA algorithms, e.g. SGD on strongly convex
objectives (see \cite{moulines_non-asymptotic_2011}). This stems from the fact
that the metric for which $\text{Id} - \Ff$ is contracting is
not a Hilbert norm. The constant $A$ depends on $L$, the diameter of $\Xx$ and the regularity of potentials $f^\star$ and $g^\star$, but not on the dimension. It behaves like $\exp(\frac{1}{\varepsilon})$ when
$\varepsilon \to 0$.
Fortunately, we can show the almost sure convergence of the online Sinkhorn algorithm
with slightly increasing batch-size $n(t)$ (that may grow arbitrarily slowly for $\eta_t = \frac{1}{t}$), as specified in the following
assumption.

\begin{assumption}\label{ass:double_weights}
    For all $t > 0$, the batch-size $n(t) = \frac{B}{w_t^2}$ is an integer. $\sum w_t \eta_t <
    \infty$ and $\sum \eta_t = \infty$.
\end{assumption}

\begin{proposition}\label{prop:convergence_true}
    Under \autoref{ass:lip} and
    \ref{ass:double_weights}, the online Sinkhorn algorithm converges almost surely:
    \begin{equation}
        \Vert \hat f_t - f^\star \Vert_{\var} + \Vert \hat g_t - g^\star \Vert_{\var} \to 0.
    \end{equation}
\end{proposition}

The proof is reported in \autoref{sec:proof_prop_convergence}. It relies on a uniform
law of large number for functions \citep[][chapter
19]{van_der_vaart_asymptotic_2000} and on the uniform contractivity of soft
$C$-transform operator \citep[e.g.][Proposition 19]{vialard2019elementary}. Consistency of the iterates is an original property---\cite{2016-genevay-nips} only show convergence of the OT value. Finally, using bounds from \cite{moulines_non-asymptotic_2011}, we derive  asymptotic rates of convergence for online Sinkhorn (see \autoref{app:proof_rate}), with
respect to the number of observed samples $N$. We write $\delta_N = \Vert \hat
f_{t(N)} - f^\star \Vert_{\var} + \Vert \hat g_{t(N)} - g^\star \Vert_{\var}$,
where $t(N)$ is the iteration number for which $n_t > N$ samples have been observed.

\begin{proposition}\label{prop:rate}
    For all $\iota \in (0, 1)$, $S > 0$ and $B \in \NN^\star$, setting $\eta_t =
    \frac{S}{t^{1 - \iota}}$, $n(t) = \lceil B t^{4\iota} \rceil$, there
    exists~$D > 0$ independant of $N$ and $N_0 > 0$ such that, for all $N >
    N_0$, $\delta_N \leq \frac{D}{N^{\frac{1 - \iota}{1 + 4 \iota}}}$.
\end{proposition}

Online Sinkhorn thus provides estimators of potentials whose asymptotic sample
complexity in variational norm is arbitrarily close to $\Oo(\frac{1}{N})$. To
the best of our knowledge, this is an original property. It also results in a
distance estimator $\hat \Ww_N$ whose complexity is arbitrarily close to
$\Oo(\frac{1}{\sqrt{N}})$, recovering existing asymptotic rates from
\cite{2019-Genevay-aistats}, for any Lipschitz cost. We derive non-asymptotic rates in \autoref{app:proof_rate}
(see \eqref{eq:non-asymptotic}), which make explicit the bias-variance trade-off
when choosing the step-sizes and batch-sizes. We also give the explicit form
of $D$; it does not depend on the dimension. For low $\epsilon$,
$D$ is proportional to $\exp(\frac{2}{\epsilon})$; the bound is therefore vacuous for
$\varepsilon \to 0$. Note that using growing
batch-sizes amounts to increase the budget of a single iteration over time: the
overall computational complexity after seeing $N$ samples is always $\Oo(N^2)$.

% Online Sinkhorn thus works for $\eta_t = \frac{1}{t^a}$ with $a \in [0, 1]$,
% provided that we use batch-sizes of size $n(t) = n\, t^{2(1-a)} \log^c t$, with $c >
% 1$. Slowing down Sinkhorn iterations thus permits to work with batches whose
% size increases more slowly. The limit case when $a = 1$ requires only $b > 0$
% can accommodate batch-sizes growing arbitrarily slowly. 