%!TEX root = article.tex


\section{Convergence analysis}\label{sec:analysis}

We show a
stationary distribution convergence property for the randomized Sinkhorn algorithm,
 an approximate convergence property for the online Sinkhorn algorithm with
fixed batch-size and an exact convergence result for online Sinkhorn with
increasing batch sizes, with asymptotic convergence rates. We make the following classical assumption on the cost regularity and compacity of $\alpha$ and $\beta$.

\begin{assumption}\label{ass:lip}
    The cost $C: \Xx \times \Xx \to \RR$ is $L$-lipschitz, and $\Xx$ is  compact.
\end{assumption}

\subsection{Randomized Sinkhorn}

We first state a result concerning the randomized Sinkhorn algorithm~\eqref{eq:updates_naive}, proved in \autoref{sec:proof_markov}.

\begin{proposition}\label{prop:markov}
    Under \autoref{ass:lip}, the randomized Sinkhorn algorithm \eqref{eq:updates_naive} yields a time-homogeneous
    Markov chain ${(\hat f_t, \hat g_t)}_t$ which is $(\hat \alpha_s, \hat \beta_s)_{s \leq
    t}$ measurable, and converges in law towards a stationary distribution
    $(f_\infty, g_\infty) \in \Pp(\Cc(\Xx)^2)$ independent of the initialization
    point $(f_0, g_0)$.
\end{proposition}

This result follows from \citet{diaconis_iterated} convergence theorem on
iterated random functions which are contracting on average. We use the
fact that $\Ctrans{\cdot}{\hat \beta}$ and $\Ctrans{\cdot}{\hat \alpha}$ are
\textit{uniformly} contracting, independently of the distributions $\hat \alpha$ and
$\hat \beta$, for the variational norm $\Vert \cdot \Vert_{\text{var}}$. 

Using the law of large number for Markov chains
\citep{breiman_strong_1960}, the out-of-loop average $\exp(-\bar f_t)$
converges almost surely to $\EE[\exp(-f_\infty)]
 \in \Cc(\Xx)$ for $\gamma_t = 
 \frac{1}{t}$. This expectation verifies the fixed point equations
\begin{equation}
    \EE[\exp(-f_\infty)] =
     \dotp{\beta}{\EE[\exp(g_\infty)] \exp(-C)},\quad
    \EE[\exp(-g_\infty)] =
     \dotp{\alpha}{\EE[\exp(f_\infty)] \exp(-C)}.
\end{equation}
These equations are close to the Sinkhorn fixed point equations, and
get closer as $\varepsilon$ increases, since $\varepsilon \EE[\exp(\pm f_\infty /
\varepsilon)] \to \EE[\pm f_\infty]$ as $\varepsilon \to \infty$. Running the random
Sinkhorn algorithm with out-of-loop averaging fails to provide exactly the dual solution, but solves an approximate problem.
%
% We leave the quantification of this approximation for future work.

%%%
\subsection{Online Sinkhorn}

We make the following \citet{robbins1951stochastic} assumption on the weight sequence. We then state an approximate convergence result for the online Sinkhorn algorithm with fixed batch-size $n(t) = n$.

\begin{assumption}\label{ass:weights}
    The sequence ${(\eta_t)}_t$ is such that
    $\sum \eta_t = \infty$ and $\sum \eta_t^2 < \infty$.
\end{assumption}

\begin{proposition}\label{prop:convergence_approx}
    Under \autoref{ass:lip} and \ref{ass:weights}, the online Sinkhorn algorithm without full-reweighting (\autoref{alg:online_sinkhorn}) yields a sequence $(f_t, g_t)$ that reaches a
    ball centered around $f^\star, g^\star$ for the variational norm $\Vert
    \cdot \Vert_{\var}$.
     Namely, there exists $T > 0$, $A > 0$ such that for all $t > T$, almost surely
    \begin{equation}
        \Vert f_t - f^\star \Vert_{\var}
        + \Vert g_t - g^\star \Vert_{\var} 
        \leq \frac{A}{\sqrt{n}} .
    \end{equation}
\end{proposition}

The proof is reported in \autoref{sec:proof_convergence_approx}. It is unfortunately not possible
to ensure the convergence of online Sinkhorn with constant batch-size. This is a
fundamental difference with other SA algorithms, e.g. SGD on strongly convex
objectives (see \cite{moulines_non-asymptotic_2011}). This stems from the fact
that the norm $\Vert \cdot \Vert_{\var}$ for which $I - \Ff$ is contracting is
non-Hilbertian. The non-explicit constant $A$ depends on $L$, the diameter of $\Xx$, and the measures $\alpha$ and
$\beta$. In particular, it behaves as $\exp(-\frac{1}{\varepsilon})$ when the
regularization $\varepsilon \to 0$.
% 
% This is comparable to the error obtained when performing Sinkhorn without resampling, as
% in~\citep{2019-Genevay-aistats}.  We show in experiments
% $(\autoref{sec:exps})$ that online Sinkhorn outperforms the regular Sinkhorn
% algorithm thanks to repeated sampling. 
% %
% This suggests that the constants appearing in the bounds of online Sinkhorn are
% better than the ones appearing in the sample complexity of regular Sinkhorn.

We therefore show the almost sure convergence of the online Sinkhorn algorithm
with slightly increasing batch-size $n(t)$ (that may grow arbitrarily slowly), as specified in the following
assumption.

\begin{assumption}\label{ass:double_weights}
    For all $t > 0$, the batch-size $n(t) = \frac{n}{w_t^2}$ is an integer. $\sum w_t \eta_t <
    \infty$ and $\sum \eta_t = \infty$.
\end{assumption}

\begin{proposition}\label{prop:convergence_true}
    Under \autoref{ass:lip} and
    \ref{ass:double_weights}, the online Sinkhorn algorithm converge almost surely:
    \begin{equation}
        \Vert \hat f_t - f^\star \Vert_{\var} + \Vert \hat g_t - g^\star \Vert_{\var} \to 0.
    \end{equation}
\end{proposition}

The proof is reported in \autoref{sec:proof_prop_convergence}. It relies on a uniform
law of large number for functions \citep[][chapter
19]{van_der_vaart_asymptotic_2000} and on the uniform contractance of soft
$C$-transform operator \citep[e.g.][Proposition 19]{vialard2019elementary}. Inspired by derivations in \cite{moulines_non-asymptotic_2011}, we derive in \autoref{app:proof_rate} asymptotic rates of convergence for online Sinkhorn, with
respect to the number of observed samples $N$. We write $\delta_N = \Vert \hat
f_{t(N)} - f^\star \Vert_{\var} + \Vert \hat g_{t(N)} - g^\star \Vert_{\var}$,
where $t(N)$ is the iteration number for which $n_t > N$ samples have been observed.

\begin{proposition}\label{prop:rate}
    For all $\iota \in (0, 1)$, $S > 0$ and $B \in \NN^\star$, setting $\eta_t =
    \frac{S}{t^{1 - \iota}}$, $n(t) = \lceil B t^{4\iota} \rceil$, there
    exists~$D > 0$ independant of $N$ and $N_0 > 0$ such that, for all $N >
    N_0$, $\delta_N \leq \frac{D}{N^{\frac{1 - \iota}{1 + 4 \iota}}}$.
\end{proposition}

Online Sinkhorn thus provides estimators of potentials whose asymptotic sample
complexity in variational norm is arbitrarily close to $\Oo(\frac{1}{N})$. To
the best of our knowledge, this is an original property. It also results in a distance estimator $\hat \Ww_N$ whose
complexity is arbitrarily close to $\Oo(\frac{1}{\sqrt{N}})$, recovering
existing asymptotic rates from \cite{2019-Genevay-aistats} beyond Wasserstein-2
distances. We derive non-asymptotic rates in \autoref{sec:proof_prop_convergence} (see \eqref{eq:non-asymptotic}), that explicits the
bias-variance trade-off when choosing the step-sizes and batch-sizes. They
unfortunately rely on the non-explicit constant~$A$ from
\autoref{prop:convergence_approx}. Note that using growing batch-sizes amounts
to increase the budget of a single iteration over time: the overall
computational complexity after seeing $N$ samples remain in $\Oo(N^2)$.

% \paragraph{Choosing batch-sizes.}

% Online Sinkhorn thus works for $\eta_t = \frac{1}{t^a}$ with $a \in [0, 1]$,
% provided that we use batch-sizes of size $n(t) = n\, t^{2(1-a)} \log^c t$, with $c >
% 1$. Slowing down Sinkhorn iterations thus permits to work with batches whose
% size increases more slowly. The limit case when $a = 1$ requires only $b > 0$
% can accommodate batch-sizes growing arbitrarily slowly. 