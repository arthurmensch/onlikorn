%!TEX root = article.tex

Optimal transport (OT) distances are fundamental in statistical learning, both
as a tool for analyzing the convergence of various
algorithms~\citep{canas2012learning,dalalyan2019user}, and as a data-dependent
term for tasks as diverse as supervised learning~\citep{frogner2015learning},
unsupervised generative modeling~\citep{arjovsky2017wgan} or domain
adaptation~\citep{courty2016optimal}.
%
OT lifts a distance over data points living in a space $\Xx$ into a distance
on the space $\Pp(\Xx)$ of probability distributions over the space $\Xx$.
% 
%  We refer to the monograph of~\citet{santambrogio2015optimal} for a detailed
%  mathematical treatment.
%
This distance has many favorable geometrical properties. In particular it allows one to compare distributions having disjoint supports. 
% 
Computing OT distances is usually performed by sampling once from the input
distributions and solving a discrete linear program (LP), due
to~\citet{Kantorovich42}. This approach is numerically costly and statistically
inefficient \citep{weed2019sharp}. The optimisation problem depends on a fixed
sampling of points from the data. It is therefore not adapted to machine
learning settings where data is resampled continuously (e.g. in GANs), or
accessed in an online manner. The goal of this paper is to develop an efficient
online method able to estimate OT distances between continuous distributions. It uses 
a stream of data to refine an approximate OT solution,
adapting the Sinkhorn algorithm to an online setting.
  


%%%%
To alleviate both the computational and statistical burdens of OT, it is common
to regularize the Kantorovich LP.
%
The most successful approach in this direction is to use an entropic barrier penalty. 
%
When dealing with discrete distributions, this yields a problem that can be solved
numerically using Sinkhorn-Knopp's matrix balancing
algorithm~\citep{Sinkhorn64,sinkhorn1967concerning}.
%
This approach was pushed forward for ML applications by
\citet{cuturi2013sinkhorn}. Sinkhorn distances are smooth and amenable to GPU
computations, which make them suitable as a loss function in model training \citep{frogner2015learning, mensch_geometric_2019}.
The Sinkhorn algorithm operates in two distinct phases: draw samples from the
distributions and evaluate a pairwise distance matrix in the first phase;
balance this distances matrix using Sinkhorn-Knopp iterations in the second
phase.

This two-step approach does not estimate the true regularized OT
distance, and cannot handle samples provided as a stream, e.g. renewed at each
training iteration of an outer algorithm. A cheap fix is to use Sinkhorn over
mini-batches (see for instance~\citet{2018-Genevay-aistats} for an application
to generative modelling). Yet this introduces a strong estimation bias, especially in high
dimension ---see~\citet{fatras2019learning}Â for a mathematical analysis. In
contrast, we use streams of mini-batches to progressively enrich a consistent representation of the
transport plan.

\paragraph{Contributions.} Our paper proposes a new take on estimating optimal transport distances between continuous distributions. We make the following contributions:
\begin{itemize}
    \item We introduce an online variant of the Sinkhorn algorithm, that relies on
    streams of samples to enrich a non-parametric
    functional representation of the dual regularized OT solution.
    \item We establish the almost sure convergence of online Sinkhorn and derive asymptotic convergence rates 
    (\autoref{prop:convergence_true} and \ref{prop:rate}). We provide
    convergence results for variants.
    \item We demonstrate the performance of online Sinkhorn for estimating OT
    distances between continuous distributions and for accelerating the early
    phase of discrete Sinkhorn iterations.
\end{itemize}

\paragraph{Notations.} We denote [$\Cc_+(\Xx)$] $\Cc(\Xx)$ the set of [strictly
positive] continuous functions over a metric space $\Xx$, $\Mm^+(\Xx)$ and
$\Pp(\Xx)$ the set of positive and probability measures on $\Xx$, respectively.
We write $(i, j]$ the sequence $[i+1, \dots,
j]$.


\section{Related work}\label{sec:related}

\paragraph{Sinkhorn properties.} Sinkhorn algorithm computes $\epsilon$-accurate
approximations of OT in $O(n^2/\epsilon^3)$ operations for a number $n$ of
samples~\citep{altschuler2017near} (in contrast to the $O(n^3)$ complexity for an
exact solution). Moreover, these Sinkhorn distances do not suffer from the
curse of dimensionality~\citep{2019-Genevay-aistats}, since the average error
using $n$ random samples decays like $O(\epsilon^{-d/2}/\sqrt{n})$ in dimension
$d$, in sharp contrast with the slow $O(1/n^{1/d})$ error decay of
OT~\citep{dudley_speed_1969,weed2019sharp}. These Sinkhorn distances can furthermore be sharpened
by entropic debiasing~\citep{2019-Feydy-aistats}. Our work is rather orthogonal to these references, as it focuses on estimating distances between continuous distributions.

\paragraph{Continuous optimal transport.} Extending OT computations to arbitrary
distributions (possibly having continuous densities) without relying on a fixed
a priori sampling is an emerging topic of interest. A special case is the
semi-discrete setting, where one of the two distributions is discrete. Without
regularization, over an Euclidean space, this can be solved efficiently using
the computation of Voronoi-like diagrams~\citep{merigot2011multiscale}. This
idea can be extended to entropic-regularized OT~\citep{cuturi2018semidual}, and
can also be coupled with stochastic optimization
method~\citep{2016-genevay-nips} to tackle high dimensional problems (see
 \cite{staib2017parallel} for an extension to Wasserstein barycenters). When dealing with arbitrary continuous densities, that are accessed through a
stream of random samples, the challenge is to approximate  the (continuous) dual
variables of the regularized Kantorovich LP using parametric or non-parametric
classes of functions. For application to generative model fitting, one can use
deep networks, which leads to an alternative formulation of Generative
Adversarial Networks (GANs)~\citep{arjovsky2017wgan} (see
also~\citet{seguy2018large} for an extension to the estimation of transportation
maps). There is however no theoretical guarantees for this type of dual
approximations, due to the non-convexity of the resulting optimization problem.
To our knowledge, the only mathematically rigorous algorithm uses reproducing
Hilbert space representations of potentials~\citep{2016-genevay-nips}. As this
construction is generic to all optimisation problems over functions, the convergence is slow. Online Sinkhorn outperform RKHS representations (\autoref{sec:compare}).

\paragraph{Stochastic approximation (SA).} Our approach may be seen as SA \cite{robbins_stochastic_1951} for finding the roots of an
operator valued in a non-Hilbertian space. \cite{alber_stochastic_2012} studies
SA for solving fixed-points in Hilbert spaces. Lack of Hilbertian structure in online Sinkhorn requires an original convergence analysis.