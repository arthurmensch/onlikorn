%!TEX root = article.tex

\section{OT distances from sample streams}

We introduce an online adaptation of the Sinkhorn algorithm. We construct an estimator of $f^\star$, $g^\star$
and $\Ww(\alpha,\beta)$ using successive sets of samples ${(\hat \alpha_t)}_t$
and ${(\hat \beta_t)}_t$, where $\hat\alpha_t = \frac{1}{n} \sum_{i=n_t +
1}^{n_{t+1}} \delta_{x_i}$, and we set  $n_0 = 0$ and $n_{t+1} = n_{t} +
n$. The size of the mini-batch $n$ may potentially depends on $t$---in
particular, $n$ must increases slightly to ensure convergence (see
\autoref{prop:convergence_true}). We write $n = n(t)$ for simplicity.
% 
${(\hat \alpha_t)}_t$ and ${(\hat \beta_t)}_t$ may be
seen as mini-batches of size $n$ within a training procedure, or as a temporal
stream of samples.
%
Our estimator progressively enriches a representation of the
solution of~\eqref{eq:dual}, that may be arbitrary complex. 
% 
% We detail an intuitive construction of our algorithm in
% \autoref{sec-online-sink-iter}, formalize it in \autoref{sec:alg} before casting
% it as a block-convex stochastic mirror descent in \autoref{sec-mirror}.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Online Sinkhorn iterations}
\label{sec-online-sink-iter}

From \eqref{eq:dual}, along the continuous (and untractable) Sinkhorn
optimisation trajectory ${(\bar f_t, \bar g_t)}_t$, the exp-potential $\exp(-\bar f_t)$ is an infinite mixture of kernel functions
$\kappa_y(x) \eqdef \exp(-C(x, y))$:
\begin{equation*}
    \exp(-\bar f_t(\cdot)) = 
    \int_{y \in \Xx} \exp(\bar g_t(y))  \kappa_y  \d \beta(y),
\end{equation*}
and similarly for $\bar g_t$. Our algorithm constructs a sequence of non-parametric
potentials $(f_t, g_t)_t$ that behaves as $(\bar f_t, \bar g_t)$. The strong
structural property of the continuous potentials suggests to express
$\exp(-f_t)$ as a finite mixture of kernel functions. That is, $f_t$ and $g_t$
are continuous functions constructed respectively from the weights ${(p_i,
q_i)}_{i \leq n_t}$ and positions ${(x_i, y_i)}_{i} \subset~\Xx$ as
\begin{align}\label{eq:param}
    f_t(\cdot) = - \log \sum_{i=1}^{n_t} 
    \exp(q_i - C(\cdot, y_i)),\quad
    g_t(\cdot) = - \log \sum_{i=1}^{n_t}
    \exp(p_i - C(x_i, \cdot)).
\end{align}
%%%%%




\paragraph{Randomized Sinkhorn.}
Provided with fresh samples $(x_i, y_i)_{n_t < i \leq n_{t+1}}$, 
corresponding to empirical measures $\hat \alpha_t$ and $\hat \beta_t$, a naive approach would 
 update the potentials using a noisy soft $C$-transform:
\begin{equation}\label{eq:updates_naive}
     f_{t+1} = \Ctrans{g_t}{\hat \beta_t},
    \qquad g_{t+1} = \Ctrans{f_{t+1}}{\hat \alpha_t},
\end{equation}
which is equivalent to setting all $(q_i)_{i \leq n_t}$ to $0$, and assigning each weight
 $q_i$ to $g_t(y_i) - \log(n)$ for $n_t < i \leq  n_{t+1}$, and similarly for~$p_i$.
%
The variance of the updates~\eqref{eq:updates_naive} does not decay through the
iteration, hence \textit{random Sinkhorn} algorithm does nos converge.
However, thanks to the contraction of the random operator $\Ctrans{\cdot}{\hat
\beta_t}$ and $\Ctrans{\cdot}{\hat \alpha_t}$, we show in \autoref{prop:markov} that
the Markov chain ${(f_t, g_t)}_t$ converges towards a
stationary distribution independent of initialization.

%%%%%
\paragraph{Online Sinkhorn.}

To ensure convergence towards the potentials $(f^\star, g^\star)$ (up to a
constant factor), we must therefore take more cautious steps---in other words,
we cannot afford to forget past iterates to obtain a consistent estimation of
potentials. 




We introduce a learning rate in Sinkhorn iterations, that averages
the past representations and the newly computed noisy $C$-transforms
\begin{equation}\label{eq:updates}
    \exp(-f_{t+1})
    \eqdef (1 - \eta_t) \exp(-f_t) 
    + \eta_t 
    \exp(-\Ctrans{g_t}{\hat \beta_t}),
\end{equation}
and similarly for $g_t$. Performing the averaging over the space of inverse
scalings $(e^{-\hat f_{t}},e^{-\hat g_{t}})$ yields simple updates for the
weights ${(p_i,q_i)}_i$, and is crucial for our theoretical convergence
analysis. In essence, the weights of past samples are reduced by a constant
factor, while new weights are computed from the evaluation of $f_t(\cdot),
g_t(\cdot)$ at random new points ${(x_i, y_i)}_i$. Note that we perform
\textit{simultaneous updates} of $f_t$ and $g_t$, which is important for the
convergence analysis.

%%%%%
\paragraph{Estimating Sinkhorn distance.} 

The iterations \eqref{eq:updates} allow us to estimate potential functions up to a constant. As explained in~\autoref{sec:gradient},
this estimation is sufficient for most applications aiming at minimizing a Sinkhorn loss, as it only requires the spatial derivatives of the potentials.
%
If required, it is however possible to estimate the Sinkhorn distance using our method, by performing a final soft $C$-transform, using $\Oo(n_T^2)$ operations:
\begin{equation}\label{eq-dist-est}
    \Ww_t = \frac{1}{2}\Big(\dotp{\bar \alpha_T}{f_T + \Ctrans{g_T}{\bar \alpha_T}}
     {+} \dotp{\bar \beta_T}{g_T {+} \Ctrans{f_T}{\bar \alpha_T}}\Big),
\end{equation}
where $\bar \alpha_t \eqdef \frac{1}{n_{t+1}}\sum_{i=1}^{n_{t+1}} \delta_{x_i}$
and $\bar \beta_t$ gather previously observed samples.

\begin{algorithm}[t]
    \begin{algorithmic}
    \State \textbf{Input:} Distribution $\alpha$ and $\beta$, learning weights ${(\eta_t)}_t$. 
    \textbf{Set} $p_i = q_i = 0$ for $i \in (0, n_t]$
    \For{$t= 0, \dots, {T-1}$}
        \For{$i \in (0, n_t]$}
        \State $q_i \gets q_i + \log(1 - \eta_t)$, $p_i \gets p_i + \log(1 - \eta_t)$,
        \EndFor
        \State Sample $(x_i)_{(n_t, n_{t+1}]} \sim \alpha$, $(y_j)_{(n_t, n_{t+1}]} \sim \beta$.
        \For{$i \in (n_t, n_{t+1}]$}
            \State $q_i {\gets} 
            \log(\eta_t n_t) {-} \log 
            \sum_{j=1}^{n_t} \exp(p_j {-} C(x_j, y_i)$,\: $p_i {\gets} 
            \log(\eta_t n_t) {-} \log 
            \sum_{j=1}^{n_t} \exp(q_j {-} C(x_i, y_j)$
        \EndFor
        \State \textit{Optional}: refit all $q_i = g_t(y_i) - \log (n_{t+1})$ \quad
         $p_i = f_t(x_i) - \log (n_{t+1})$
        \State Save $(q_i, p_i, x_i, y_i)_{(n_t,n_{t+1}]}$
    \EndFor
    \State \textbf{Returns:} $f_T : (q_i, y_i)_{(0, n_T]}$ and
    $g_T : (p_i, x_i)_{(0, n_T]}$
    \end{algorithmic}
    \caption{Online Sinkhorn potentials}\label{alg:online_sinkhorn}
\end{algorithm}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Algorithm, complexity and refinements}\label{sec:alg}

The pseudo-code of online Sinkhorn is detailed in \autoref{alg:online_sinkhorn}.
We perform the updates for $q_i$ and $p_i$ in log-space, for numerical stability reasons.
Each iteration has complexity $\Oo(n_t\,n)$, due to the evaluation of the
distances $C(x_i, y_i)$ for all $(x_i)_{(0, n_t]}$, $(y_i)_{(n_t, n_{t+1}]}$
and to the computation of the
soft $C$-transforms. Online Sinkhorn computes a distance matrix
$(C(x_i,y_j))_{i,j}$ on the fly, in parallel to the updates of the potentials
$f_t$ and $g_t$. In total, its computation cost after drawing $n_t$ samples is
$\Oo(n_t^2)$, and its memory cost is $\Oo(n_t)$.
%
We propose some heuristics to accelerate convergence and alleviate memory and computational cost.


%%%%%
\paragraph{Fully-corrective scheme.} 

The potentials $f_t$ and $g_t$ may be improved by refitting the weights
$(p_i)_{(0, n_t]}$, $(q_j)_{(0, n_t]}$ based on all previously seen samples.  For this, we 
may perform one step of the discrete Sinkhorn algorithm with
distributions $\bar \alpha_t$ and $\bar \beta_t$.
This amounts to replace, after iteration $t$, for all $i \in (0, n_{t+1}]$,
$q_i \gets g_t(y_i) - \log(n_{t+1})$. and similarly for each $p_i$. This increases the dual cost
$F_{ \bar \alpha, \bar \beta}(f_t, g_t)$, and ``on average'', the energy
$F_{\alpha, \beta}$. This reweighted scheme (akin to the fully-corrective
Frank-Wolfe scheme from \cite{lacoste2015global}) has a cost in $\Oo(n_t^2)$
per iteration. In practice, it can be used every $k$ iterations, with $k$
increasing with $t$. We study a combination of partial and full updates in
\autoref{sec:accelerating}.

%%%%%
% \paragraph{Memory compression.} 

% The memory requirement in $\Oo(n_t)$ is an
% avoidable limitation of the algorithm, as the optimal
% potentials $(f^\star, g^\star)$ do not admit a parametric representation in
% general. However, we may compress the representations $(q_j, y_j)$ and $(x_i,
% p_i)_i$ using $k$-means clustering over $M$ centroids. The
% sampled points $(x_i)_i$ and $(y_j)_j$ are attached to centroids ${(X_I)}_{I \in
% (0,M_t]}$ and ${(Y_J)}_{J \in (0,M_t]}$. For all $I \in (0, M_t]$, we set
% weights and potentials as
% \begin{align}
%     Q_J &\gets - \log \sum_{\substack{j,\:y_j \text{ closest}\\\text{to } \bar Y_J}}
%      \exp(-q_j),\\
%     f_t(\cdot) &\gets - \log\sum_{J=1}^{M_t} \exp(Q_J - C(\cdot, \bar Y_J)),
% \end{align}
% and similarly for $(p_I)_I$ and $g_t$. Once again, this operation should be made
% once every $k$ iterations. $M_t$ can for instance be set constant after linearly
% increasing in a first stage. This heuristic is important for applications but
% requires significant engineering: we leave it for future work.

% \paragraph{Out-of-loop averaging.} Optionally, we may also
% compute out-of-loop averages of potentials
% \begin{align}
%     \exp(-\bar f_{t+1}) = (1 - \gamma_t) \exp(-\bar f_t) + \gamma_t \exp(-\hat f_t), \\
%     \exp(-\bar g_{t+1}) = (1 - \gamma_t) \exp(-\bar g_t) + \gamma_t \exp(-\hat g_t),
% \end{align}
% to further reduce the estimation variance. We show in \autoref{sec:exp1} that
% this averaging is efficient in practice. 

\paragraph{Finite samples.}Finally, we note that our algorithm
applies on both continuous or discrete distributions. When $\alpha$ and $\beta$
are discrete distributions of size $N$, we can store $p$ and $q$ as fixed-size
vectors of size~$N$, and subsample mini-batches of size $n < N$. The resulting
algorithm is a \textit{subsampled} Sinkhorn algorithm for histograms, which is
detailed in \autoref{sec:sinkhorn_discrete}, \autoref{alg:discrete_online}. We show in \autoref{sec:exps} that it
is useful to accelerate the first phase of the Sinkhorn algorithm.


