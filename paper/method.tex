%!TEX root = article.tex

\section{OT distances from sample streams}

We now introduce an online adaptation of the Sinkhorn algorithm. We construct functional estimators of $f^\star$, $g^\star$
and $\Ww(\alpha,\beta)$ using successive discrete distributions of samples ${(\hat \alpha_t)}_t$
and ${(\hat \beta_t)}_t$, where $\hat\alpha_t \triangleq \frac{1}{n} \sum_{i=n_t +
1}^{n_{t+1}} \delta_{x_i}$, with  $n_0 \triangleq 0$ and $n_{t+1} \triangleq n_{t} +
n$. The size of the mini-batch $n$ may potentially depends on $t$.
% ---in
% particular, $n$ must increases slightly to ensure exact convergence (see
% \autoref{prop:convergence_true}).  
${(\hat \alpha_t)}_t$ and ${(\hat \beta_t)}_t$ may be
seen as mini-batches of size $n$ within a training procedure.
%
% Our estimator progressively enriches a representation of the
% solution of~\eqref{eq:dual}, that may be arbitrary complex. 
% 
% We detail an intuitive construction of our algorithm in
% \autoref{sec-online-sink-iter}, formalize it in \autoref{sec:alg} before casting
% it as a block-convex stochastic mirror descent in \autoref{sec-mirror}.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Online Sinkhorn iterations}
\label{sec-online-sink-iter}

The optimization trajectory ${(f_t, g_t)}_t$ of the continuous Sinkhorn
algorithm given by \eqref{eq:sinkhorn} is untractable as it cannot be
represented in memory. The exp-potentials $u_t \triangleq \exp(- f_t)$ and $v_t
\triangleq \exp(-g_t)$ are indeed infinite mixtures of kernel functions
$\kappa_y(\cdot) \eqdef \exp(-C(\cdot, y))$ and $\kappa_x(\cdot) \eqdef \exp(-C(x, \cdot))$. 

We propose to construct finite-memory consistent estimates of $u_t$ and $v_t$ using
principles from stochastic approximation (SA) \cite{robbins1951stochastic}. We cast
the regularized OT problem as a root-finding problem of a function-valued operator $\Ff: \Cc_{+}(\Xx) \times \Cc_{+}(\Xx) \to \Cc(\Xx) \times \Cc(\Xx)$,
for which we can obtained unbiased estimates. Optimal potentials are indeed
exactly the roots of
% 
\begin{equation}
    \Ff: (u, v) \to 
    \Big(u(\cdot) - \int_{y \in \Xx}
     \frac{1}{v(y)} \kappa_y(\cdot)  \d \beta(y),\quad
    v(\cdot) - \int_{x \in \Xx}
    \frac{1}{u(x)}  \kappa_x(\cdot)  \d \alpha(x)\Big).
\end{equation}
% 
In particular, the simultaneous Sinkhorn updates rewrites as $(u_{t+1}, v_{t+1}) = (u_{t},
v_{t}) - \Ff(u_t, v_t)$ for all $t$. Importantly, $\Ff$ can be
evaluated without bias using two empirical measures $\hat \alpha$ and
$\hat \beta$, defining
% 
\begin{equation}
    \hat \Ff_{\hat \alpha, \hat \beta}(u, v) \triangleq 
    \Big(u(\cdot) - \frac{1}{n} \sum_{i=1}^n
    \frac{1}{v(y_i)} \kappa_{y_i}(\cdot)\quad
    v(\cdot) - \frac{1}{n} \sum_{i=1}^n
   \frac{1}{u(x_i)}  \kappa_{x_i}(\cdot) \Big).
\end{equation}
By construction, $\EE_{\hat \alpha \sim \alpha, \hat \beta \sim \beta} [\hat \Ff_{\hat
\alpha, \hat \beta}] = \Ff$, and the images of $\hat \Ff$ admit a representation in memory.




\paragraph{Randomized Sinkhorn.}To make use of a stream of samples $(\hat \alpha_t, \hat
 \beta_t)_t$, we may simply replace $\Ff$ with $\hat \Ff$ in the Sinkhorn updates.
 This amounts to use noisy soft $C$-transforms in \eqref{eq:sinkhorn}, as we set
\begin{equation}\label{eq:updates_naive}
    (u_{t+1}, v_{t+1}) \triangleq (u_{t},
    v_{t}) - \hat \Ff_{\hat \alpha, \hat \beta}(u_t, v_t),\quad\text{i.e.}\quad
     \hat f_{t+1} = \Ctrans{\hat g_t}{\hat \beta_t},
    \qquad \hat g_{t+1} = \Ctrans{\hat f_{t+1}}{\hat \alpha_t}.
\end{equation}
% which is equivalent to setting all $(q_i)_{i \leq n_t}$ to $0$, and assigning each weight
%  $q_i$ to $g_t(y_i) - \log(n)$ for $n_t < i \leq  n_{t+1}$, and similarly for~$p_i$.
%
$\hat f_t$ and $\hat g_t$ are defined in memory by $(y_i, \hat g_{t-1}(y_i))_i$ and $(x_i,
\hat f_{t-1}(x_i))_i$. Yet the variance of the updates~\eqref{eq:updates_naive} does not
decay through time, hence this \textit{randomized Sinkhorn} algorithm does not
converge. However, we show in
\autoref{prop:markov} that the Markov chain ${(\hat f_t, \hat g_t)}_t$ converges
towards a stationary distribution that is independent of the potentials $\hat f_0$ and $\hat g_0$ used for
initialization.

%%%%%
\paragraph{Online Sinkhorn.}

To ensure convergence of $\hat f_t$, $\hat g_t$ towards some optimal pair of potentials
$(f^\star, g^\star)$, one must take more cautious steps, in particular past iterates should not be discarded. 
%
We introduce a learning
rate $\eta_t$ in Sinkhorn iterations, akin to the Robbins-Monro algorithm for finding
roots of vector-valued functions:
\begin{equation}\label{eq:updates}
    (\hat u_{t+1}, \hat v_{t+1}) \triangleq (1 - \eta_t) (\hat u_t, \hat v_t) 
    - \eta_t \hat \Ff_{\hat \alpha_t, \hat \beta_t}(\hat u_t, \hat v_t),\quad\text{i.e.}\quad
    e^{-\hat f_{t+1}} = (1 - \eta_t) e^{-\hat f_{t}} + 
    \eta_t e^{-\Ctrans{\hat g_t}{\hat \beta_t}}
\end{equation}
Each update adds new kernel functions to a non-parametric estimation of $
u_t$ and $v_t$. The estimates $\hat u_t$ and $\hat v_t$ are defined by weights ${(p_{i,t},
q_{i,t})}_{i \leq n_t}$ and positions ${(x_i, y_i)}_{i \leq n_t} \subset~\Xx^2$:
\begin{align}\label{eq:param}
    e^{-\hat f_t(\cdot)} = \hat u_t(\cdot) \triangleq \sum_{i=1}^{n_t} 
    \exp(q_{i,t} - C(\cdot, y_i)),\quad
    e^{-\hat g_t(\cdot)} = \hat v_t(\cdot) \triangleq \sum_{i=1}^{n_t}
    \exp(p_{i,t} - C(x_i, \cdot)).
\end{align}
The SA updates \eqref{eq:updates} yields simple vectorized updates for the weights
${(p_i,q_i)}_i$, leading to \autoref{alg:online_sinkhorn}. We perform
the updates for $q_i$ and $p_i$ in log-space, for numerical stability reasons.

\paragraph{Complexity.}

Each iteration of online Sinkhorn has complexity $\Oo(n_t\,n)$, due to the evaluation of the
distances $C(x_i, y_i)$ for all $(x_i)_{(0, n_t]}$ and $(y_i)_{(n_t, n_{t+1}]}$,
and the soft $C$-transforms in \eqref{eq:param}. Online Sinkhorn computes a
distance matrix $(C(x_i,y_j))_{i,j \leq n_t}$ on the fly, in parallel to
updating~$\hat f_t$ and~$\hat g_t$. In total, its computation cost after drawing
$n_t$ samples is $\Oo(n_t^2)$. Its memory cost is $\Oo(n_t)$; it increases with
iterations, which is a requirement for consistent estimation. Randomized
Sinkhorn with constant batch-sizes $n$ has a memory cost of $\Oo(n)$ and a
single-iteration computational cost of $\Oo(n^2)$.

\begin{algorithm}[t]
    \begin{algorithmic}
    \State \textbf{Input:} Dist. $\alpha$ and $\beta$, learning weights ${(\eta_t)}_t$, batch sizes ${(n(t))}_t$
    \textbf{Set} $p_i = q_i = 0$ for $i \in (0, n_1]$
    \For{$t= 0, \dots, {T-1}$}
        \State Sample $(x_i)_{(n_t, n_{t+1}]} \sim \alpha$, $(y_j)_{(n_t, n_{t+1}]} \sim \beta$.
            \State Evaluate $(\hat f_t(x_i))_{i=(n_t, n_{t+1}]}$,
             $(\hat g_t(y_i))_{i=(n_t, n_{t+1}]}$ using $(q_{i,t}, p_{i,t}, x_i, y_i)_{i=(0,n_{t}]}$ in \eqref{eq:param}.
             \State $q_{(n_t, n_{t+1}],t+1} {\gets} \log \frac{\eta_t}{n}
             + (\hat g_t(y_i))_{(n_t, n_{t+1}]}$,
             \qquad $p_{(n_t, n_{t+1}],t+1} {\gets} \log \frac{\eta_t}{n} 
             + (\hat f_t(x_i))_{(n_t, n_{t+1}]}$.
            \State $q_{(0, n_t],t+1} \gets q_{(0, n_t],t} + \log(1 - \eta_t)$, \qquad
            $p_{(0, n_t],t+1} \gets p_{(0, n_t],t} + \log(1 - \eta_t)$.
        % \State Persists $\hat f_{t+1} : (q_{i,t+1}, y_i)_{(0, n_{t+1}]}$ and
        % $\hat g_{t+1} : (p_{i,t+1}, x_i)_{(0, n_{t+1}]}$
        % \State \textit{Optional}: refit all $q_i = g_t(y_i) - \log (n_{t+1})$ \quad
        %  $p_i = f_t(x_i) - \log (n_{t+1})$
    \EndFor
    \State \textbf{Returns:} $\hat f_T : (q_{i,T}, y_i)_{(0, n_T]}$ and
    $\hat g_T : (p_{i,T}, x_i)_{(0, n_T]}$
    \end{algorithmic}
    \vspace{-.4em}
    \caption{Online Sinkhorn}\label{alg:online_sinkhorn}
\end{algorithm}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Refinements}

\paragraph{Estimating Sinkhorn distance.} 

As we will see in \autoref{sec:analysis}, the iterations \eqref{eq:updates} only estimate potential functions up to a
constant. This is sufficient for minimizing a loss function involving a Sinkhorn
distance (e.g. for model training or barycenter estimation \citep{staib2017parallel}), as backpropagating through the Sinkhorn distance
relies only on the gradients of the potentials $\nabla_x f^\star(\cdot)$,
$\nabla_y g^\star(\cdot)$ \citep[e.g.][]{cuturi2018semidual}. With extra
$\Oo(n_t^2)$ operations, $(\hat f_t, \hat g_t)$ may be used to
estimate $\Ww(\alpha,\beta)$ through a final soft $C$-transform:
\begin{equation}\label{eq-dist-est}
    \hat \Ww_t \triangleq \frac{1}{2}\Big(\dotp{\bar \alpha_t}{f_t + 
    \Ctrans{\hat g_t}{\bar \alpha_t}}
     {+} \dotp{\bar \beta_t}{\hat g_t {+} \Ctrans{f_t}{\bar \alpha_t}}\Big),
\end{equation}
where $\bar \alpha_t \eqdef \frac{1}{n_{t}}\sum_{i=1}^{n_{t}} \delta_{x_i}$
and $\bar \beta_t$ are formed of all previously observed samples.


%%%%%
\paragraph{Fully-corrective scheme.} 

The potentials $\hat f_t$ and $\hat g_t$ may be improved by refitting the
weights $(p_i)_{(0, n_t]}$, $(q_j)_{(0, n_t]}$ based on all previously seen
samples.  For this, we update $\hat f_{t+1} = \Ctrans{g_t}{\bar \beta_t}$ and
$\hat g_{t+1} = \Ctrans{f_t}{\bar \alpha_t}$. This reweighted scheme (akin to
the fully-corrective Frank-Wolfe scheme from \cite{lacoste2015global}) has a
cost of $\Oo(n_t^2)$ per iteration. It requires to keep in memory (or recompute
on-the-fly) the whole distance matrix. Fully-corrective online Sinhorn  enjoys
similar convergence properties as regular online Sinkhorn, and permits the use
of non-increasing batch-sizes---see
\autoref{app:fully_corrective}. In practice, it can be used every $k$
iterations, with $k$ increasing with $t$. Combining partial and full updates can
accelerate the estimation of Sinkhorn distances (see \autoref{sec:accelerating}).


%%%%%
% \paragraph{Memory compression.} 

% The memory requirement in $\Oo(n_t)$ is an
% avoidable limitation of the algorithm, as the optimal
% potentials $(f^\star, g^\star)$ do not admit a parametric representation in
% general. However, we may compress the representations $(q_j, y_j)$ and $(x_i,
% p_i)_i$ using $k$-means clustering over $M$ centroids. The
% sampled points $(x_i)_i$ and $(y_j)_j$ are attached to centroids ${(X_I)}_{I \in
% (0,M_t]}$ and ${(Y_J)}_{J \in (0,M_t]}$. For all $I \in (0, M_t]$, we set
% weights and potentials as
% \begin{align}
%     Q_J &\gets - \log \sum_{\substack{j,\:y_j \text{ closest}\\\text{to } \bar Y_J}}
%      \exp(-q_j),\\
%     f_t(\cdot) &\gets - \log\sum_{J=1}^{M_t} \exp(Q_J - C(\cdot, \bar Y_J)),
% \end{align}
% and similarly for $(p_I)_I$ and $g_t$. Once again, this operation should be made
% once every $k$ iterations. $M_t$ can for instance be set constant after linearly
% increasing in a first stage. This heuristic is important for applications but
% requires significant engineering: we leave it for future work.


\paragraph{Finite samples.}Finally, we note that our algorithm
can handle both continuous or discrete distributions. When $\alpha$ and $\beta$
are discrete distributions of size $N$, we can store $p$ and $q$ as fixed-size
vectors of size~$N$, and update at each iterations a set of coordinates of size $n < N$. The resulting
algorithm is a \textit{subsampled} Sinkhorn algorithm for histograms, which is
detailed in \autoref{sec:sinkhorn_discrete}, \autoref{alg:discrete_online}. We show in \autoref{sec:exps} that it
is useful to accelerate the first phase of the Sinkhorn algorithm.