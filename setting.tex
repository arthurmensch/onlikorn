%!TEX root = article.tex
\section{Background: optimal transport distances}

We recall the definition of optimal transport distances between arbitrary distributions (i.e. not necessarily discrete), then review how these are estimated using finite realizations.

\subsection{Optimal transport distances and algorithms}

\paragraph{Wasserstein distances.} 

We consider a complete metric space $(\Xx,d)$ (assumed to be compact for simplicity), equipped
with a continuous cost function $C(x,y) \in \RR$ for any $(x,y) \in \Xx^2$ (assumed to be symmetric also for simplicity). 
%
Optimal transport lifts this \textit{ground cost} into a cost between probability
distributions over the space $\Xx$. 
%

The Wasserstein cost between two probability distributions $(\alpha, \beta) \in \Pp(\Xx)^2$ is defined as the minimal cost required to move each element of mass of $\alpha$ to each element of mass of $\beta$. It rewrites as the solution of a
linear problem (LP) over the set of transportation plans (which are probability distribution $\pi$ over $\Xx \times \Xx$):
\begin{equation}\label{eq:wass_0}
    \Ww_{C,0}(\alpha, \beta) \eqdef 
    \min_{\pi \in \Pp(\Xx^2)}
    \enscond{
    	\dotp{C}{\pi}
	}{ \pi_1=\al, \pi_2=\be},
\end{equation}
where we denote $\dotp{C}{\pi} \eqdef \int C(x,y) \d\pi(x,y)$. Here,  
$\pi_1 = \int_{y\in \Xx} \d \pi(\cdot, y)$ and $\pi_2 = \int_{x\in \Xx} \d
\pi(x, \cdot)$ are the first and second marginals of the transportation plan $\pi$. 
%
When $C=d^p(x,y)$ is the $p^{\text{th}}$ power of the ground distance, with $p
\geq 1$, then $\Ww_{C,0}^{\frac{1}{p}}$ is itself a distance over $\Pp(\Xx)$, whose
associated topology is the one of the convergence in
law~\citep{santambrogio2015optimal}.

\paragraph{Entropic regularization and Sinkhorn algorithm.} 

The solutions of~\eqref{eq:wass} can be approximated by a strictly convex optimisation problem, where an entropic term is added to the linear objective to force curvature. The so-called Sinkhorn cost is then
\begin{equation}\label{eq:wass}
    \Ww_{C,\varepsilon}(\alpha, \beta) \eqdef 
    \min_{
    \substack{
        \pi \in \Pp(\Xx \times \Xx)
        \\\pi_1 = \alpha, \pi_2 = \beta
    }    
    } \dotp{C}{\pi} + \varepsilon \KL(\pi | \alpha \otimes \beta),
\end{equation}
where the Kulback-Leibler divergence is defined as $\KL(\pi | \alpha \otimes
\beta) \eqdef \int \log(\frac{\d\pi}{\d\al\d\be}) \d\pi$ (which is thus equal to
the mutual information of $\pi$).
%
$\Ww_{C,\varepsilon}$ approximate $\Ww_C(\alpha,\beta)$ up to an $\epsilon
\log(\epsilon)$ error \citep{2019-Genevay-aistats}. In the following, we set
$\varepsilon$ to $1$ without loss of generality, as $\Ww_{C, \varepsilon} =
\epsilon \Ww_{C / \varepsilon, 1}$, and simply write $\Ww$.
%
The regularized problem~\eqref{eq:wass} admits a dual form, which is a maximization problem over the space of continuous functions:
\begin{equation}\label{eq:dual}
    \max_{(f, g) \in \Cc(\Xx)^2} \dotp{f}{\alpha} + 
    \dotp{g}{\beta}
    - \dotp{e^{f \oplus g - C}}{\alpha \otimes \beta} + 1, 
\end{equation}
where $\dotp{f}{\alpha} \eqdef \int f(x) \d\al(x)$ and $(f \oplus g - C)(x,y)
\eqdef f(x)+g(y)-C(x,y)$. We write $F_{\alpha, \beta}(f, g)$ the dual objective. 
Problem \eqref{eq:dual} can be solved by alternated maximization, which is itself performed in closed form. At iteration $t$, the updates are simply
\begin{align}
    &f_{t+1}(\cdot) = - \Ctrans{g_t}{\beta}, \:
    g_{t+1}(\cdot) = - \Ctrans{f_{t+1}}{\alpha}, \\
    &
    \Ctrans{h}{\mu} \triangleq 
    - \log \int_{y \in \Xx}\!\! \exp(h(y) - C(\cdot, y))\d \mu(y).\label{eq:sinkhorn}
\end{align}
The operation $h \mapsto \Ctrans{h}{\mu}$  maps a continuous function to another
continuous function, and is a smooth approximation of the celebrated
$C$-transform of OT~\citep{santambrogio2015optimal}. We thus refer to it as a
\textit{soft C-transform}. 
%
The notation $f_t(\cdot)$ emphasizes the fact that $f_t$ and $g_t$ are
\textit{functions}. 
%
% In the discrete setting, iterations 

It can be shown that ${(f_t)}_t$ and ${(g_t)}_t$ converge in $(\Cc(\Xx),
\norm{\cdot}_{\text{var}})$ to a solution $(f^\star, g^\star)$ of
\eqref{eq:dual}, where $\norm{f}_{\text{var}} \eqdef \max_x f(x) - \min_x f(x)$
is the so-called variation norm. Functions endowed with this norm are only
considered up to an additive constant.  Global convergence is due to the strict
contraction of the operators $\Ctrans{\cdot}{\beta}$ and
$\Ctrans{\cdot}{\alpha}$ in the space $(\Cc(\Xx), \norm{\cdot}_{\text{var}})$
\citep{lemmens_nonlinear_2012}.

\subsection{Estimating OT distances with realizations}

Iterations~\eqref{eq:sinkhorn} cannot be implemented when dealing with generic distributions $(\al,\be)$, because they involve continuous functions ${(f_t,g_t)}_t$. 
%
When the input distribution are discrete (or equivalently that $\Xx$ is a finite
set) then these function can be stored on discrete vectors;
the iterations~\eqref{eq:sinkhorn} correspond to the celebrated
\citet{sinkhorn1967concerning} algorithm, which is often implemented over the scaling
variable $(e^{f_t},e^{g_t})$.
%
More precisely, with $\alpha =
\frac{1}{n}\sum_{i=1}^n \delta_{x_i}$ and $\beta = \frac{1}{n} \sum_{i=1}^n
\delta_{x_i}$, the Sinkohrn iterations~\eqref{eq:sinkhorn} update $u_t \eqdef
(e^{f_t(x_i)})_{i=1}^n, v_t \eqdef (e^{g_t(y_i)})_{i=1}^n$ as
\begin{equation*}
	u_{t+1} = \frac{n}{ K v_t }
	\qandq
	v_{t+1} = \frac{n}{ K^\top u_{t+1} }
\end{equation*}
where $K=(e^{-C(x_i,y_i)})_{i,j=1}^n \in \RR^{n \times n}$. The algorithm thus
operates in two phases: first, the kernel matrix $K$ is computed, with a cost in
$O(n^2 d)$, where $d$ is the dimension of $\Xx$; second, $K$ is balanced, each
iteration costing $O(n^2)$. The online Sinkhorn algorithm that we propose mixes
these two phases to accelerate convergence (see \autoref{sec:accelerating}).

% The goal of this paper is to go beyond this discrete setting, and handle generic distributions (possibly having continuous densities). In particular, our numerical scheme manipulates continuous functions though an adapted parameterization which is automatically refined during the iterations.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\paragraph{Consistency and bias.}\label{sec:gradient}

The OT distance $\Ww_{C,0}(\alpha,\beta)$ and its regularized version
$\Ww_{C,\epsilon}(\alpha,\beta)$ can be approximated by the (computable)
distance between discrete realizations $\hat \alpha = \frac{1}{n} \sum_i
\delta_{x_i}$, $\hat \beta = \frac{1}{n} \sum_i \delta_{y_i}$, where ${(x_i)}_i$
and ${(y_i)}_i$ are i.i.d samples from $\alpha$ and $\beta$.  Consistency holds,
as $\Ww_{C,\epsilon}(\hat \alpha_n, \hat \beta_n) \to \Ww_{C,\epsilon}(\alpha,
\beta)$. Although this is a reassuring result, the sample complexity of
transport in high dimensions with low regularization remains high (see
\autoref{sec:related}). For computational reasons, we cannot choose $n$ to be
much more than $10^5$. We may wonder wether we can improve the estimation of
$\Ww_{C,\epsilon}(\alpha,\beta)$ using several sets of samples $(\hat
\alpha_t)_t$ and ${(\hat \beta_t)}_t$. Those should be of reasonable size to
allow Sinkhorn estimation, and may for example come from a temporal stream.
\citet{2018-Genevay-aistats} propose to use a Monte-Carlo estimate $\hat
\Ww(\alpha, \beta) = \frac{1}{T} \sum_{t=1}^T \Ww(\hat \alpha_t, \beta_t)$.
However, this yields a biased estimation as the distance $\Ww(\alpha,
\beta)$ differs from its expectation under sampling $\EE_{\hat \alpha \sim
\alpha, \hat \beta \sim \beta} [\Ww(\hat \alpha, \hat \beta)]$. To address this
issue, our algorithm computes a consistent estimation of the Sinkhorn cost
using a stream of samples.

Similarly, estimating the Sinkhorn potentials through sampling is biased, as
the true optimal potential $f^\star=f^\star(\alpha, \beta)$ is not the expected
optimal potential under sampling $\EE_{\hat \alpha_n \sim \alpha, \hat \beta_n
\sim \beta}[f^\star(\hat \alpha_n, \hat \beta_n)]$. Our algorithm estimates the
true continuous potential functions (up to a constant) and overcomes the sampling
bias.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \paragraph{Gradient estimations.}\label{sec:gradient} 

% OT distances $\Ww(\alpha, \beta)$ are frequently used a loss function in
% supervised learning or unsupervised density fitting. This is the case for
% instance in generative modeling~\cite{arjovsky2017wgan}, when we parametrize
% $\alpha=\alpha_\th$ as the push-forward of some noise distribution $\mu$ through
% a neural network $g_\theta$, which means that a sample $X$ from $\al$ is
% computed as $g_\th(Y)$ where $Y$ is sampled from $\mu$. A similar situation
% occurs for instance when computing Wasserstein barycenters
% \cite{staib2017parallel}. The gradient of $\Ee(\th) \eqdef \Ww(\alpha_\th,
% \beta)$ only depends on $\nabla_x f^\star$, i.e. the spatial gradient of the
% optimal solution to~\eqref{eq:dual}~\cite{peyre2019computational}. 