%!TEX root = article.tex
\section{Background: optimal transport distances}

We recall the definition of optimal transport distances between arbitrary distributions (i.e. not necessarily discrete), then review how these are estimated using finite samples.

\subsection{Optimal transport distances and algorithms}

\paragraph{Wasserstein distances.} 

We consider a complete metric space $(\Xx,d)$ (assumed to be compact for simplicity), equipped
with a continuous cost function $C(x,y) \in \RR$ for any $(x,y) \in \Xx^2$ (assumed to be symmetric also for simplicity). 
%
Optimal transport lifts this \textit{ground cost} into a cost between probability
distributions over the space $\Xx$. 
%

The Wasserstein cost between two probability distributions $(\alpha, \beta) \in \Pp(\Xx)^2$ is defined as the minimal cost required to move each element of mass of $\alpha$ to each element of mass of $\beta$. It rewrites as the solution of a
linear problem (LP) over the set of transportation plans (which are probability distribution $\pi$ over $\Xx \times \Xx$):
\begin{equation}\label{eq:wass}
    \Ww_C(\alpha, \beta) \eqdef 
    \min_{\pi \in \Pp(\Xx^2)}
    \enscond{
    	\dotp{C}{\pi}
	}{ \pi_1=\al, \pi_2=\be},
\end{equation}
where we denote $\dotp{C}{\pi} \eqdef \int C(x,y) \d\pi(x,y)$. Here,  
$\pi_1 = \int_{y\in \Xx} \d \pi(\cdot, y)$ and $\pi_2 = \int_{x\in \Xx} \d
\pi(x, \cdot)$ are the first and second marginals of the transportation plan $\pi$. 
%
When $C=d^p(x,y)$ is the $p^{\text{th}}$ power of the ground distance, with $p \geq 1$, then $\Ww_C^{\frac{1}{p}}$ is itself a distance over $\Pp(\Xx)$, whose associated topology is the one of the convergence in law~\cite{santambrogio2015optimal}.

\paragraph{Entropic regularization and Sinkhorn algorithm.} 

The solutions of~\eqref{eq:wass} can be approximated by a strictly convex optimisation problem, where an entropic term is added to the linear objective to force curvature. The so-called Sinkhorn cost is then
\begin{equation}\label{eq:wass}
    \Ww_{C,\varepsilon}(\alpha, \beta) \eqdef 
    \min_{
    \substack{
        \pi \in \Pp(\Xx \times \Xx)
        \\\pi_1 = \alpha, \pi_2 = \beta
    }    
    } \dotp{C}{\pi} + \varepsilon \KL(\pi | \alpha \otimes \beta),
\end{equation}
where the Kulback-Leibler divergence is defined as $\KL(\pi | \alpha \otimes
\beta) \eqdef \int \log(\frac{\d\pi}{\d\al\d\be}) \d\pi$ (which is thus equal to
the mutual information of $\pi$).
%
This can be shown to approximate $\Ww_C(\alpha,\beta)$  up to an $\epsilon
\log(\epsilon)$ error \citep{2019-Genevay-aistats}, which is recovered in the
limit $\epsilon=0$. 
%
The regularized problem~\eqref{eq:wass} admits a dual form, which is a maximization problem over the space of continuous functions:
\begin{equation}\label{eq:dual}
    \max_{(f, g) \in \Cc(\Xx)^2} \dotp{f}{\alpha} + \dotp{g}{\beta}
    - \varepsilon \dotp{e^{\frac{f \oplus g - C}{\epsilon}}}{\alpha \otimes \beta} + \varepsilon, 
\end{equation}
where $\dotp{f}{\alpha} \eqdef \int f(x) \d\al(x)$ and $(f \oplus g - C)(x,y)
\eqdef f(x)+g(y)-C(x,y)$. We write $F_{\alpha, \beta}(f, g)$ the dual objective. 
The major interest of problem \eqref{eq:dual} is that it can be solved by alternated maximization, which is itself performed in closed form. At iteration $t$, the updates are simply
\begin{align}
    &f_{t+1}(\cdot) = - T_{C,\varepsilon}(g_t, \beta), \:
    g_{t+1}(\cdot) = - T_{C,\varepsilon}(f_{t+1}, \alpha), \\
    &
    T_{C,\varepsilon}(h, \mu) \triangleq 
    - \epsilon \log \int_{y \in \Xx}\!\! \exp(\frac{h(y) - C(\cdot, y)}{\varepsilon})\d \mu(y).\label{eq:sinkhorn}
\end{align}
The operation $h \mapsto T_{C,\varepsilon}(h, \mu)$  maps a continuous function to another continuous function, and is a smooth approximation of the celebrated $C$-transform of OT~\cite{santambrogio2015optimal}. We thus refer to it as a \textit{soft C-transform}. 
%
The notation $f_t(\cdot)$ emphasizes the fact that $f_t$ and $g_t$ are \textit{functions}. 
%
% In the discrete setting, iterations 

It can be shown that ${(f_t)}_t$ and ${(g_t)}_t$ converge in $(\Cc(\Xx),
\norm{\cdot}_{\text{var}})$ to a solution $(f^\star, g^\star)$ of
\eqref{eq:dual}, where $\norm{f}_{\text{var}} \eqdef \max_x f(x) - \min_x f(x)$ is
the so-called variation norm, where $\sim$ means that functions are only considered up to an additive constant. This is true for any initialisation 
$f_0, g_0$.  Convergence is due to the strict contraction of the operators $T_C(\cdot, \beta)$ and
$T_{C}(\cdot, \alpha)$ in the space $(\Cc(\Xx), \norm{\cdot}_{\text{var}})$, see~\cite{peyre2019computational}.

\subsection{Estimating OT distances with realizations}

Iterations~\eqref{eq:sinkhorn} cannot be implemented when dealing with generic distributions $(\al,\be)$, because it involves continuous functions ${(f_t,g_t)}_t$. 
%
When the input distribution are discrete (or equivalently that $\Xx$ is a finite
set) then these function can be stored using discrete sample, and
algorithm~\eqref{eq:sinkhorn} corresponds to the celebrated
\citet{sinkhorn1967concerning}, which is often implemented over the scaling
variable $(e^{f_t/\epsilon},e^{g_t/\epsilon})$.
%
More precisely, when dealing with empirical distibutions $\alpha =
\frac{1}{n}\sum_{i=1}^n \delta_{x_i}$ and $\beta = \frac{1}{n} \sum_{i=1}^n
\delta_{x_i}$, then the Sinkohrn iterations~\eqref{eq:sinkhorn} update $u_t \eqdef
(e^{f_t(x_i)/\epsilon})_{i=1}^n, v_t \eqdef (e^{g_t(y_i)/\epsilon})_{i=1}^n$ as
\begin{equation*}
	u_{t+1} = \frac{n}{ K v_t }
	\qandq
	v_{t+1} = \frac{n}{ K^\top u_{t+1} }
\end{equation*}
where $K=(e^{-\frac{C(x_i,y_i)}{\epsilon}})_{i,j=1}^n \in \RR^{n \times n}$. The algorithm thus operates in two
phases: a first one, during which the kernel matrix $K$ is computed (with a cost
in $O(n^2 d)$, where $d$ is the dimension of $\Xx$), and a second one, during
which it is balanced (each iteration having a cost in $O(n^2)$).

% The goal of this paper is to go beyond this discrete setting, and handle generic distributions (possibly having continuous densities). In particular, our numerical scheme manipulates continuous functions though an adapted parameterization which is automatically refined during the iterations.

\paragraph{Consistency and bias.} The distance $\Ww(\alpha,\beta)$ can be
approximated by the (computable) distance between discrete realizations $\hat
\alpha = \frac{1}{n} \sum_i \delta_{x_i}$, $\hat \beta = \frac{1}{n} \sum_i
\delta_{y_i}$, where $(x_i)_i$ and $(y_i)_i$ are i.i.d samples from $\alpha$ and
$\beta$.  Consistency holds, as $\Ww_{C, (\varepsilon)}(\hat \alpha_n, \hat
\beta_n) \to \Ww_{C, (\varepsilon)}(\alpha, \beta),$ with a convergence rate in
$\Oo(n^{-1/2})$ for Sinkhorn distances and $\Oo(n^{-1/d})$ for Wasserstein
distances. 

Although consistency is a reassuring result, the sample complexity of transport
in high dimensions with low regularization remains high. For computational
reasons, we cannot choose $n$ to be much more than $10^5$. We may wonder wether
we can improve the estimation of $\Ww_{C}(\alpha,\beta)$ using several sets of
samples $(\hat \alpha_t)_t$ and ${(\hat \beta_t)}_t$. Those should be of
reasonable size to allow Sinkhorn estimation, and may for example come from a
temporal stream. \citet{2018-Genevay-aistats} propose to use the Monte-Carlo
estimate $\hat \Ww(\alpha, \beta) = \frac{1}{T} \sum_{t=1}^T \Ww(\hat \alpha_t,
\beta_t)$. This yields a wrong estimation as the distance $\Ww(\hat
\alpha_t, \hat \beta_t)$ between discrete realizations is a \textit{biased}
estimator of $\Ww(\alpha, \beta)$:
\begin{equation}
    \Ww(\alpha, \beta) \neq 
    \EE_{\hat \alpha \sim \alpha, \hat \beta \sim \beta} [\Ww(\hat \alpha, \hat \beta)].
\end{equation}

\paragraph{Bias in gradients.} In several applications, the distance
$\Ww(\alpha, \beta)$ is used as a loss function. This is the case in generative
modeling, when we parametrize $\alpha$ as the push-forward of some noise
distribution $\mu$ through a neural network $g_\theta$. We are then interested
in computing the displacement gradient $\delta_\alpha \Ww(\alpha, \beta) \in
\Pp(\Xx)$, in order to train $\theta$ by backpropagation. This gradient turns
out to be the spatial derivative $\nabla_x f^\star$ of the solution of
\eqref{eq:dual}. Yet, similarly, estimating this gradient through sampling is
also biased, as the true optimal potential $f^\star(\alpha, \beta)$ is not the
expected optimal potential under distribution sampling $\EE_{\hat \alpha_n \sim
\alpha, \hat \beta_n \sim \beta}[f^\star(\hat \alpha_n, \hat \beta_n)]$.  Our
approach will help reducing this bias. Instead of averaging values over several realizations, we enrich a solution using new realizations.