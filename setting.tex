%!TEX root = article.tex
\section{Background: optimal transport distances}

We recall the definition of optimal transport distances between arbitrary distributions (i.e. not necessarily discrete), then review how these are estimated using finite samples.

\subsection{Optimal transport distances and algorithms}

\paragraph{Wasserstein distances.} We consider a complete space $\Xx$, equiped
with a distance function $C: \Xx \to \Xx \to \RR$. Optimal transport lifts this
\textit{ground metric} distance into a distance between probability
distributions over the space $\Xx$. The Wasserstein distance between $\alpha$
and $\beta$ is defined as the minimal cost required to move each element of mass
of $\alpha$ to each element of mass of $\beta$. It rewrites as the solution of a
linear problem (LP) over the set of transportation plans
\begin{equation}\label{eq:wass}
    \Ww_C(\alpha, \beta) = 
    \min_{
    \substack{
        \pi \in \PP(\Xx \times \Xx)\\\pi_1 = \alpha, \pi_2 = \beta
    }    
    } \dotp{C}{\pi},
\end{equation}
where $\pi_1 = \int_{y\in \Xx} d \pi(\cdot, y)$ and $\pi_2 = \int_{x\in \Xx} d
\pi(x, \cdot)$ are the first and second marginals of the transportation plan $\pi$. It
can shown (ref) that $\Ww_C$ is a distance over $\Pp(\Xx)$, that
measures the convergence in law.

\paragraph{Entropic regularization and Sinkhorn algorithm.} 
The solution of \eqref{eq:wass} can be approximated by a simpler optimisation problem, where an entropic term is added to the linear objective to force curvature. The so-called Sinkhorn distance
\begin{equation}\label{eq:wass}
    \Ww_{C,\varepsilon}(\alpha, \beta) = 
    \min_{
    \substack{
        \pi \in \PP(\Xx \times \Xx)
        \\\pi_1 = \alpha, \pi_2 = \beta
    }    
    } \dotp{C}{\pi} + \varepsilon \KL(\pi | \alpha \otimes \beta),
\end{equation}
is indeed an $\varepsilon$-approximation of $\Ww_C(\alpha,\beta)$. The later problem admits a dual form, which is a maximization problem in
the space of continuous \textit{potential} function:
\begin{equation}\label{eq:dual}
    \max_{f, g \in \Cc(\Xx)} \dotp{f}{\alpha} + \dotp{g}{\beta}
    + \varepsilon (\dotp{\alpha \otimes \beta}{\exp(\frac{f \oplus g - C}{\epsilon})} - 1).
\end{equation}
The dual problem
\eqref{eq:dual}, a regularized version of the dual of \eqref{eq:wass}, can be solved by alternated maximization, performing at iteration $t$
\begin{gather}\label{eq:sinkhorn}
    f_{t+1}(\cdot) = - T_{C,\varepsilon}(g_t, \beta) \qquad
    g_{t+1}(\cdot) = - T_{C^\top,\varepsilon}(f_{t+1}, \alpha), \\
    \text{where} \quad T_C(h, \mu) \triangleq \int_{y \in \Xx} \exp(\frac{h(y) - C(\cdot, y)}{\varepsilon})\d \mu,
\end{gather}
is a \textit{soft C-transform}, and the notation $f_t(\cdot)$ emphasizes the
belonging of $f_t$ and $g_t$ to the space of continuous functions. ${(f_t)}_t$
and ${(g_t)}_t$ converge in $(\Cc(\Xx), \Vert \cdot \Vert_{\text{var}})$ to a
solution $(f^\star, g^\star)$ of \eqref{eq:dual}, where $\Vert f
\Vert_{\text{var}} = \max_x f(x) - \min_x f(x)$ is the so-called variation norm. 
Convergence is due to the strict contraction of the operators $T_C(\cdot, \beta)$ and
$T_{C^\top}(\cdot, \alpha)$ in the space $(\Cc(\Xx), \Vert \cdot \Vert_{\text{var}})$.

\subsection{Estimating OT distances with realizations}

Regularized optimal transport is elegantly written in functional spaces but the
iterations \eqref{eq:sinkhorn} transfers into code only for discrete
distributions $\hat \alpha = \sum_{i=1}^n a_i \delta_{x_i}$ and $\hat \beta =
\sum_{i=1}^n a_i \delta_{x_i}$. In this case, they correspond to the well-known
Sinkhorn-Knopp algorithm for balancing the matrix
$\exp(\frac{-C}{\varepsilon})$. The algorithm is run in two phases: a first one,
during which the cost matrix is computed (with a cost in $\Oo(n^2 d)$), and a
second one, during which it is balanced (each iteration have a cost in
$\Oo(n^2)$).

\paragraph{Sample complexity results.} Fortunately, the OT and Sinkhorn distances between two arbitrary distributions
$\alpha$ and $\beta$ can be approximated by the distance between discrete
realizations $\hat \alpha_n = \frac{1}{n} \sum_i \delta_{x_i}$, 
$\hat \beta_n = \frac{1}{n} \sum_i \delta_{y_i}$, where $(x_i)_i$
and $(y_i)_i$ are i.i.d samples from $\alpha$ and $\beta$. Consistency holds, as
$\Ww_{C, (\varepsilon)}(\hat \alpha_n, \hat \beta_n) \to \Ww_{C, (\varepsilon)}(\alpha, \beta),$
with a convergence rate in $\Oo(n^{-1/2})$ for Sinkhorn distances and $\Oo(n^{-1/d})$ for
Wasserstein distances.

\paragraph{Bias in distance estimation.} Although consistency is a reassuring result, the
sample complexity of transport in high dimensions with low regularization
remains high. For computational reasons, we cannot choose $n$ to be much more
than $10^5$, which is not sufficient to ensure that $\Ww_{C,\varepsilon}(\alpha,
\beta)$ is $\varepsilon$-close to $\Ww_C$ in the typical case where $d=?$ and
$\varepsilon = ?$.

We may wonder wether we can improve the estimation of $\Ww_{C}(\alpha,\beta)$
using several sets of samples $(\hat \alpha_n^t)_t$ and $(\hat \beta_n^t)$. Those should be of 
reasonable size to allow Sinkhorn estimation, and may for example come from a temporal stream. \cite{}\cite{} have proposed to use 
the Monte-Carlo estimate $\hat \Ww(\alpha, \beta) = \frac{1}{T} \sum_{t=1}^T
\Ww(\hat \alpha_n^t, \beta \alpha_n^t)$. However, this yields a wrong estimation
as the distance $\Ww(\hat \alpha_n, \hat \beta_n)$ between discrete realizations
is a \textit{biased} estimator of $\Ww(\alpha, \beta)$:
\begin{equation}
    \Ww(\alpha, \beta) \neq 
    \EE_{\hat \alpha_n \sim \alpha, \hat \beta_n \sim \beta} [\Ww(\hat \alpha_n, \hat \beta_n)].
\end{equation}

\paragraph{Bias in gradients.} In several applications, the distance
$\Ww(\alpha, \beta)$ is used as a loss function. This is the case in generative
modeling, when we parametrize $\alpha$ as the push-forward of some noise
distribution $\mu$ through a neural network $g_\theta$. We are then interested
in computing the displacement gradient $\delta_\alpha \Ww(\alpha, \beta) \in
\Pp(\Xx)$, in order to train $\theta$ by backpropagation. This gradient turns
out to be the spatial derivative $\nabla_x f^\star$ of the solution of
\eqref{eq:dual}. Yet, similarly, estimating this gradient through sampling is
biased, as $f^\star(\alpha, \beta) \neq \EE_{\hat \alpha_n \sim \alpha, \hat
\beta_n \sim \beta}[f^\star(\hat \alpha_n, \hat \beta_n)]$.

\hfill