\documentclass[a4paper, 10pt]{article}
\input{preamble.tex}

\addbibresource{biblio.bib}

\begin{document}

\section{Online Sinkhorn}

The Sinkhorn objective rewrites
\begin{equation}
    \max_{f, g \in \Cc(\Xx)} \dotp{f}{\alpha} + \dotp{g}{\beta}
     - \epsilon \dotp{\alpha \otimes \beta}{\exp(-\frac{f \oplus g - C}{\epsilon})
     }
\end{equation}
We perform the following change of variable $\mu = \alpha e^{f / \varepsilon}$,
$\nu = \beta e^{g / \epsilon}$, to obtain the equivalent problem, in $\Mm^+(\Xx)$
\begin{equation}
    \min_{\mu,\nu \in \Mm^+(\Xx)} \KL(\alpha|\mu) + \KL(\beta|\mu) + 
    \epsilon \dotp{\mu \otimes \nu}{\exp(-\frac{C}{\epsilon})} \triangleq f(\mu, \nu).
\end{equation}

The problem is not jointly convex, but convex in $\mu$ and $\nu$.
We may approach this problem from a game point of view of finding
 a local Nash equilibrium $(\mu^\star,\nu^\star)$ such that
\begin{align}\label{eq:Nash}
    \mu^\star &= \argmin_{\mu \in V(\mu^\star)} \Ff(\mu, \nu^\star) \\
    \nu^\star &= \argmin_{\nu \in V(\nu^\star)} \Ff(\mu^\star, \nu),
\end{align}
where $V$ are open sets. Such a formalism is useful as results on mirror descent
convergence in multi-agent setting exist for this problem. To solve
\eqref{eq:Nash}, we need to define distance generating functions to move back
and forth from $\mu$ and $\nu$ and their dual form. We define
\begin{align}
    \omega_\alpha(\mu) &\triangleq \KL(\alpha | \mu) \\
    \omega_\beta(\nu) &\triangleq \KL(\beta | \nu) \\
\end{align},
associated the the mirror maps
\begin{align}
    \nabla_\mu \omega_\alpha(\mu) &= 
    - \frac{\d\alpha}{\d\mu} \qquad \big( = - \exp(-f / \epsilon) \big), \\
    \nabla_\nu \omega_\beta(\nu) &= 
    - \frac{\d\beta}{\d\nu} \qquad \big( = - \exp(-g / \epsilon) \big),
\end{align}
with inverse
\begin{align}
    \nabla_\mu \omega^\star_\alpha(p) &= - \frac{\alpha}{p}, \\
    \nabla_\nu \omega^\star_\beta(q) &= - \frac{\beta}{q}.
\end{align}
\paragraph{Algorithm.} Let us consider the simple simultaneous mirror descent setting, where we build
the sequence of iterate $(\mu_t, \nu_t)_t$. It is easy to shows that if we start
from $\mu_0 \gg \alpha$ and $\nu_0 \gg \beta$, the iterates will remain
absolutely continuous with respect to $\alpha$ and $\beta$. We will therefore
write $\mu_t = \alpha e^{f_t / \varepsilon}$, $\nu_t = \beta e^{g_t /
\epsilon}$. The mirror descent iterations rewrite (for $\mu$)
\begin{equation}
    \mu_{t+1} = \frac{\alpha}{e^{-f_t / \epsilon} + \eta \nabla_\mu \Ff(\mu_t, \nu_t)},
\end{equation}
with $\nabla_\mu \Ff(\mu_t, \nu_t) = - \exp(-\frac{f_t}{\epsilon})+ \epsilon
\int_y \exp(\frac{g(y)-C(\cdot, y)}{\epsilon})\d\beta(y)$. We therefore have the
following update rules
\begin{align}
    \exp(-\frac{f_{t+1}}{\epsilon}) &=
     (1 - \eta) \exp(-\frac{f_{t}}{\epsilon}) 
     + \eta \EE_\beta[\epsilon \exp( \frac{g_t(y)-C(\cdot, y)}{\epsilon})], \\
     \exp(-\frac{g_{t+1}}{\epsilon}) &=
     (1 - \eta) \exp(-\frac{g_{t}}{\epsilon}) 
     + \eta \EE_\alpha[\epsilon \exp( \frac{f_t(x)-C(x, \cdot)}{\epsilon})].
\end{align}
Assuming we sample $\hat \beta_t = \sum_{i=1}^n b_{i,t} \delta_{y_{i,t}}$ and
$\hat \alpha_t = \sum_{i=1}^n a_{i,t} \delta_{x_{i,t}}$, we can approximate the
expectations above, and expect, with decreasing step-sizes to achieve
convergence.

Some variants (more likely to converge better) may be considered. The alternated variant writes
\begin{align}
    \exp(-\frac{f_{t+1}}{\epsilon}) &=
     (1 - \eta) \exp(-\frac{f_{t}}{\epsilon}) 
     + \eta \EE_\beta[\epsilon \exp( \frac{g_t(y)-C(\cdot, y)}{\epsilon})], \\
     \exp(-\frac{g_{t+1}}{\epsilon}) &=
     (1 - \eta) \exp(-\frac{g_{t}}{\epsilon}) 
     + \eta \EE_\alpha[\epsilon \exp( \frac{f_{t+1}(x)-C(x, \cdot)}{\epsilon})],
\end{align}
and the extrapolated version
\begin{align}
    \exp(-\frac{f_{t+1/2}}{\epsilon}) &=
     (1 - \eta) \exp(-\frac{f_{t}}{\epsilon}) 
     + \eta \EE_\beta[\epsilon \exp( \frac{g_t(y)-C(\cdot, y)}{\epsilon})], \\
     \exp(-\frac{g_{t+1/2}}{\epsilon}) &=
     (1 - \eta) \exp(-\frac{g_{t}}{\epsilon}) 
     + \eta \EE_\alpha[\epsilon \exp( \frac{f_t(x)-C(x, \cdot)}{\epsilon})], \\
     \exp(-\frac{f_{t+1}}{\epsilon}) &=
     \exp(-\frac{f_{t}}{\epsilon}) - \eta \exp(-\frac{g_{t + 1 / 2}}{\epsilon})
     + \eta \EE_\beta[\epsilon \exp( \frac{g_{t+1/2}(y)-C(\cdot, y)}{\epsilon})], \\
     \exp(-\frac{g_{t+1}}{\epsilon}) &=
     \exp(-\frac{g_{t}}{\epsilon}) - \eta \exp(-\frac{g_{t + 1 / 2}}{\epsilon})
     + \eta \EE_\alpha[\epsilon \exp( \frac{f_{t+1/2}(x)-C(x, \cdot)}{\epsilon})].
\end{align}
\paragraph{Computations.} In the simple simultaneous case, we can track $f_t$ in memory by the following representation
\begin{align}
    f_t(\cdot) &= - \epsilon \log \sum_{s=0}^t w_{t,s} \sum_{j=1}^n b_{s, j} 
    \exp\Big(g_{s-1}(y_{s,j}) - \frac{C(\cdot, y_{s, j})}{\epsilon}\Big) \\
    g_t(\cdot) &= - \epsilon \log \sum_{s=0}^t w_{t,s} \sum_{i=1}^n a_{s, i} 
    \exp\Big(f_{s-1}(x_{s, i}) - \frac{C(x_{s,i}, \cdot)}{\epsilon}\Big),
\end{align}
with $w_{t,s} = \eta (1-\eta)^{t-s}$ for $1 \leq s \leq t$, $w_{t,0} =
(1-\eta)^{t}$, and we set $g_{-1} = f_{-1} = 0$. The weights are a bit more
complex is $\eta$ depends on time.

The alternated version sets
\begin{equation}
    g_t(\cdot) = - \epsilon \log \sum_{s=0}^t w_{t,s} \sum_{i=1}^n a_{s, i} 
    \exp\Big(f_{s}(x_{s, i}) - \frac{C(x_{s,i}, \cdot)}{\epsilon}\Big),
\end{equation}

Setting $q_{t,s,i} = w_{t,s} b_{s, j} \exp(g_{s-1}(y_{s,
j}))$ and $p_{t,s,i} = w_{t,s} b_{s, j} \exp(f_{s-1}(x_{s, j})/ \epsilon)$, we can derive
simple update rules for $p$ and $q$:
\begin{equation}
    p_{t,t, i} = \eta b_{t,j} \exp(f_{t-1}(x_{t, j}),
    \qquad \foralls s < t,\quad p_{t,s, i} = (1- \eta) p_{t-1,s, i}
\end{equation}


\section{Analysis}

Bregman divergence associated to $\phi$ (désolé j'ai changé de notation).
\begin{equation}
    d_{\phi}(f_2|f_1) = \dotp{\alpha}{\exp(\frac{f_2-f_1}{\varepsilon})
     - 1 - \frac{f_2-f_1}{\varepsilon}}
      \geq \dotp{\alpha}{\big(\frac{f_2-f_1}{2 \varepsilon}\big)^2}
\end{equation}

For convergence of MD on $\min f(x)$ with mirror map $\phi$, we need to show, according to Gabriel, Jalal and Kelvin
\begin{equation}
    \mu d_\phi(x_2|x_1 )\leq d_f(x_2|x_1) \leq L d_\phi(x_2|x_1).
\end{equation}
Can we use that here ? Beware that we are in an alternated setting


\subsection{Sketch of proof}

See proof of Th2 in Ya Ping's paper.

\subsubsection{General proof}

By using the dual iteration and the three point property (normally holds by def of $D_{\alpha}$ and $D_{\beta}$):
\begin{align*}
\ps{\mu_t-\mu}{-\nabla_{\mu} \Ff(\mu_t,\nu_t)}&=\frac{1}{\eta}\ps{\mu_t-\mu}{\nabla_{\mu}w_{\alpha}(\mu_{t+1})-\nabla_{\mu}w_{\alpha}(\mu_t)}\\
&=\frac{1}{\eta}[D_{w_{\alpha}}(\mu,\mu_{t})-D_{w_{\alpha}}(\mu,\mu_{t+1})+D_{w_{\alpha}}(\mu_t,\mu_{t+1})]
\end{align*}
Suppose we can show (TO DO): 
\begin{align}\label{eq:bound_gradient}
D_{w_{\alpha}}(\mu_t,\mu_{t+1})\le \eta^2 M^2
\end{align}
Then we have:
\begin{align*}
\frac{1}{T}\sum_{t=1}^T \ps{\mu_t-\mu}{-\nabla_{\mu} \Ff(\mu_t,\nu_t)}&= \sum_{t=1}^T \frac{1}{\eta}[D_{w_{\alpha}}(\mu,\mu_{t})-D_{w_{\alpha}}(\mu,\mu_{t+1})+D_{w_{\alpha}}(\mu_t,\mu_{t+1})]\\
\le \frac{D_{w_{\alpha}}(\mu,\mu_1)}{\eta}+\eta M^2 T
\end{align*}
Similarly:
\begin{align*}
\frac{1}{T}\sum_{t=1}^T \ps{\nu_t-\nu}{-\nabla_{\nu} \Ff(\mu_t,\nu_t)}&
\le \frac{D_{w_{\beta}}(\mu,\mu_1)}{\eta}+\eta M^2 T
\end{align*}
Summing up the two previous equations and replacing $(\mu,\nu)$ by $(\mu*,\nu*)$, we get:
\begin{equation}
\frac{1}{T}\sum_{t=1}^T \ps{\mu_t-\mu*}{-\nabla_{\mu} \Ff(\mu_t,\nu_t)} + \ps{\nu_t-\nu*}{-\nabla_{\nu} \Ff(\mu_t,\nu_t)}\le \frac{D_0}{\eta}+2\eta M^2 T
\end{equation}
where $D_0=D_{w_{\alpha}}(\mu^*,\mu_1)+D_{w_{\beta}}(\nu^*,\nu_1)$.\\

\noindent Then, by optimality of $\mu^*$ and convexity of $\Ff$:
\begin{multline*}
\Ff(\mu_t,\nu_t)-\Ff(\mu^*,\nu^*)\le \Ff(\mu_t,\nu_t)-\Ff(\mu^*,\nu_t)
\le \ps{\mu^*-\mu_t}{\nabla_{\mu}\Ff(\mu_t,\nu_t)}= \ps{\mu_t-\mu^*}{-\nabla_{\mu}\Ff(\mu_t,\nu_t)}
\end{multline*}
Hence:
\begin{equation}
\frac{1}{T}\sum_{t=1}^T  
\Ff(\mu_t,\nu_t)-\Ff(\mu^*,\nu^*) \le \frac{D_0}{\eta}+2\eta^2 M^2 T
\end{equation}

\subsubsection{Tricky part}
Now let's try to prove Equation \ref{eq:bound_gradient}. 
What we need is 
\begin{itemize}
	\item $D_{w_{\alpha}}(\mu,\nu)=D_{w_{\alpha}^*}(\nabla_{\mu}w_{\alpha}(\mu), \nabla_{\mu}w_{\alpha}(\nu))$ (eq A.13 in Ya Ping's paper)
	\item  \textit{relative smoothness of $w_{\alpha}^*$ wrt $\|.\|_{\infty}$} (eq A.11 and A.12 in Ya Ping's paper)
\end{itemize}
 If it's true:
\begin{align*}
D_{w_{\alpha}}(\mu_t,\mu_{t+1}) = D_{w_{\alpha}^*}(\nabla_{\mu}w_{\alpha}(\mu_t), \nabla_{\mu}w_{\alpha}(\mu_{t+1}))\le \| \nabla_{\mu}w_{\alpha}(\mu)- \nabla_{\mu}w_{\alpha}(\nu)\|^2_{\infty}\\
=\| \exp(-\frac{f_{t+1}}{\epsilon})- \exp(-\frac{f_t}{\epsilon})\|^2_{\infty} = \eta^2 \| \nabla \Ff_{\mu}(\mu_t,\nu_t)\|^2_{\infty}
\end{align*}
and 
\begin{align*}
\|\nabla \Ff_{\mu}(\mu_t,\nu_t)\|^2_{\infty}\le ?
\end{align*}

WIP
\begin{equation}
    D_{w_{\alpha}}(\mu_t,\mu_{t+1}) \geq
    \Vert \log \frac{\mathrm d \mu_{t+1}}{\mathrm d \mu_{t}} \Vert_\alpha^2
\end{equation}

We can show
\begin{align}
    D_{\omega_\alpha}(\mu_t | \mu_{t+1}) = \eta^2
    \dotp{\alpha}{1 - \exp(\frac{f_t - \hat f_{t+1}}{\epsilon})},
    = \eta^2 (1 - \dotp{\alpha}{\nabla_\mu \Ff(\mu_t, \nu_t)})
    \\ \hat f_{t+1}(\cdot) 
    = - \varepsilon \log \int_y \exp(\frac{g_t(y) - C(\cdot, y)}{\varepsilon}) \mathrm d \beta(y).
\end{align}
Avec Sinkhorn sans bruit $f_t - \hat f_{t+1}$ va rester tranquille.

\section{Proof of convergence}

We want to solve
\begin{equation}
    \min_{\mu \in \Mm^+(\Xx), \nu \in \Mm^+(\Xx)} 
    F(\mu, \nu) \triangleq \KL(\alpha | \mu) + \KL(\beta | \nu) + \dotp{\mu \otimes \nu}{\exp(-C)}
\end{equation}

Let's write $x = (\mu, \nu)$  and $F(\mu, \nu) = F(x)$ the objective. We define the iterates $x_{t} = (\mu_t, \nu_t)$, $x_{t+1/2} = (\mu_{t+1}, \nu_t)$, 
$x_{t} = (\mu_{t+1}, \nu_{t+1})$. First note that we have
\begin{equation}
    D_{F(\mu, \cdot)} = D_{\omega_\alpha(\cdot)}\qquad D_{F(\cdot, \nu)} = D_{w_\beta(\cdot)},
\end{equation}
so that at every iteration, we perform a mirror step with a function that is both 1-relatively smooth and 1-relatively strongly convex.

Let $\nu$ be fixed, and let us define $F_\nu(\cdot) = F(\cdot, \nu)$. From the
smoothness of $F_\nu(\cdot)$ and from its convexity we have, for all 
$\mu_x, \mu_y, \mu_z \gg \alpha$,
\begin{align}
    F(\mu_x, \nu) &\leq F(\mu_y, \nu)
     + \dotp{\nabla_\mu F(\mu_y, \nu)}{\mu_x- \mu_y} + L D_{\omega_\alpha}(\mu_x, \mu_y), \\
    F(\mu_y, \nu) &\leq F(\mu_z, \nu) + \dotp{\nabla_\mu F(\mu_y, \nu)}{\mu_y - \mu_z}
\end{align}
Combining both, we obtain
\begin{align}
    F(\mu_x, \nu) &\leq F(\mu_z, \nu) + 
    \dotp{\nabla_\mu F(\mu_y, \nu)}{\mu_x - \mu_z} + L D_{\omega_\alpha}(\mu_x, \mu_y) \\
    \dotp{\nabla F(\mu_y, \nu)}{\mu_x - \mu_z} 
    &\geq F(\mu_x, \nu) - F(\mu_z, \nu) - L D_{\omega_\alpha}(\mu_x, \mu_y).
\end{align}
We now use the three point propery:
\begin{equation}
    D_{\omega_\alpha}(\mu_z, \mu_y) - D_{\omega_\alpha}(\mu_z, \mu_x) 
    - D_{\omega_\alpha}(\mu_x, \mu_y) 
    = \dotp{\nabla \omega_\alpha(\mu_x) - \nabla \omega_\alpha(\mu_y)}{\mu_z - \mu_x},
\end{equation}
replacing $\mu_y = \mu_{k}, \mu_x = \mu_{k+1}, \nu = \nu_k$, we obtain
\begin{align}
    D_{\omega_\alpha}(\mu_z, \mu_k) - D_{\omega_\alpha}(\mu_z, \mu_{k+1}) 
    - D_{\omega_\alpha}(\mu_{k+1}, \mu_k) &=
     \eta_k \dotp{\nabla_\mu F(\mu_k, \nu_k)}{\mu_{k+1} - \mu_z} \\
    &\geq \eta_k (F(\mu_{k+1}, \nu_k) - F(\mu_z, \nu_k)))
     - L \eta_k D_{\omega_\alpha}   (\mu_{k+1},\mu_k).
\end{align}
Hence, mimicking the derivation for $\nu$
\begin{align}
    \eta_k (F(\mu_{k+1},\nu_{k+1}) - F(\mu_{k+1}, \nu_z)) &\leq 
    D_{\omega_\beta}(\nu_z, \nu_k) - D_{\omega_\beta}(\nu_z, \nu_{k+1}) 
    - (1 - \eta_k L) D_{\omega_\beta}(\nu_{k+1}, \nu_k). \\
    \eta_k (F(\mu_{k+1},\nu_k) - F(\mu_z, \nu_k)) &\leq 
    D_{\omega_\alpha}(\mu_z, \mu_k) - D_{\omega_\alpha}(\mu_z, \mu_{k+1}) 
    - (1 - \eta_k L) D_{\omega_\alpha}(\mu_{k+1}, \mu_k)
\end{align}
Setting $\mu_z = \mu_{k}$, $\nu_z = \nu_k$, we obtain a descent lemma.
\begin{equation}
    F(\mu_{k+1}, \nu_{k+1}) \leq F(\mu_{k+1}, \nu_{k}) \leq F(\mu_{k}, \nu_{k}),
\end{equation}
which ensures almost sure convergence of $F(\mu_k, \nu_k)$ to $F^\star$ using constant step-sizes (gradient is zero for $k \to \infty$).

We further have, replacing $\mu_z = \mu^\star, \nu_z = \nu^\star$
%
\begin{equation}
    F(\mu_{k+1}, \nu_{k+1}) - F^\star
    - \frac{
        \sum_{k=1}^k \eta_k (F(\mu_{k+1}, \nu^\star)
    +F(\mu^\star, \nu_k) - 2 F^\star))
    }
    {2 \sum_{k=1}^k \eta_k}
    \leq \frac{
    D_{\omega_\alpha}(\mu^\star, \mu_1)
    + D_{\omega_\beta}(\nu^\star, \nu_1)
    }{\sum_{k=1}^K \eta_k}
\end{equation}

Contractance $F(\mu_{k+1}, \nu^\star) - F^\star$.
\printbibliography

\end{document}
